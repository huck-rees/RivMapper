{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df0f685c",
   "metadata": {},
   "source": [
    "# Hydraulic Geometry Calculator\n",
    "\n",
    "The following code takes the standard RivMapper reach polygons, and clips and trims the Global Bankfull Discharge Dataset (GQBF) to each reach. Using this dataset. Using GQBF, the Open-Elevation API, and standard Python geospatial libraries, the code extracts the median wetted channel width, median bankfull discharge, channel length, and channel slope for each reach, outputting all metrics to a .csv.\n",
    "\n",
    "The user must then use the BASED depth estimator to manually calculate water depth for each reach. A planned addition to this notebook will include the BASED API and automate depth calculations. \n",
    "\n",
    "Global River BankFull Discharge (GQBF): \n",
    "Liu, Y., Wortmann, M., & Slater, L. (2024). Global River BankFull Discharge (GQBF) (0.1) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.13855371\n",
    "\n",
    "Open-Elevation-API: https://github.com/Jorl17/open-elevation\n",
    "\n",
    "Boost-Assisted Stream Estimator for Depth (BASED):https://based-estimator.streamlit.app/ (Use manually after running this code to estimate depth)\n",
    "\n",
    "Author: James (Huck) Rees; PhD Student, UCSB Geography\n",
    "\n",
    "Date: March 30, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c007e34b",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c60c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyproj\n",
    "from pyproj import CRS, Transformer\n",
    "import os\n",
    "from shapely.ops import unary_union, split, linemerge\n",
    "from shapely.geometry import LineString, Point\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a2b0c",
   "metadata": {},
   "source": [
    "## Initialize functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa2c05c",
   "metadata": {},
   "source": [
    "Define working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d873a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GQBF. Checks if it is already extracted. If not, it extracts from the QGBF database\n",
    "def get_GQBF(river_name, reach_gdf, continent_abr, working_directory):\n",
    "    \"\"\"\n",
    "    Checks if a shapefile exists for the given river name in the working directory.\n",
    "    If it exists, loads it as a GeoDataFrame.\n",
    "    If it does not exist, calls extract_GQBF to generate the data.\n",
    "    \n",
    "    Parameters:\n",
    "    - river_name (str): The name of the river.\n",
    "    - reach_gdf (GeoDataFrame): The river reach geometry.\n",
    "    - continent_abr (str): Two-letter continent abbreviation.\n",
    "    - working_directory (str): The directory where the shapefile is expected to be found.\n",
    "    \n",
    "    Returns:\n",
    "    - gdf (GeoDataFrame): The loaded, extracted, and trimmed GeoDataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the expected file path for the shapefile\n",
    "    shapefile_path = os.path.join(working_directory, 'GQBF', 'Extracted_rivers', river_name, f\"{river_name}.shp\")\n",
    "    \n",
    "    # Check if the shapefile exists and is a valid file\n",
    "    if os.path.isfile(shapefile_path):\n",
    "        print(f\"Shapefile found: {shapefile_path}. Loading data...\")\n",
    "        try:\n",
    "            gdf = gpd.read_file(shapefile_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading shapefile: {e}\")\n",
    "            print(f\"Attempting to extract data for {river_name} in {continent_abr}...\")\n",
    "            gdf = extract_GQBF(river_name, reach_gdf, continent_abr, working_directory)\n",
    "    else:\n",
    "        print(f\"Shapefile not found. Extracting data for {river_name} in {continent_abr}...\")\n",
    "        gdf = extract_GQBF(river_name, reach_gdf, continent_abr, working_directory)\n",
    "\n",
    "    # Ensure the dataset is not empty before trimming\n",
    "    if gdf is not None and not gdf.empty:\n",
    "        # Call trim_GQBF() to refine the extracted data\n",
    "        trimmed_gdf = trim_GQBF(reach_gdf, gdf)\n",
    "        return trimmed_gdf\n",
    "    else:\n",
    "        print(f\"Warning: Extracted GQBF data for {river_name} is empty.\")\n",
    "        return None\n",
    "\n",
    "# This function extracts the GQBF data from one of the main geopackages. \n",
    "# This function is called as part of the previous function if the GQBF for the specified river does not yet exist.\n",
    "def extract_GQBF(river_name, reach_gdf, continent_abr, working_directory):\n",
    "    \"\"\"\n",
    "    Extracts GQBF data for a given river by importing the corresponding GeoPackage.\n",
    "    The function filters data for the specified river using its reach geometry.\n",
    "\n",
    "    Parameters:\n",
    "    - river_name (str): The name of the river.\n",
    "    - reach_gdf (GeoDataFrame): The river reach geometry.\n",
    "    - continent_abr (str): Two-letter continent abbreviation.\n",
    "    - working_directory (str): The directory where extracted data should be stored.\n",
    "\n",
    "    Returns:\n",
    "    - gdf (GeoDataFrame): The filtered and processed GeoDataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Construct the path to the GeoPackage file\n",
    "    gpkg_filename = f\"GQBFv0.1_reaches_{continent_abr}_EPSG4326.gpkg\"\n",
    "    gpkg_path = os.path.join(working_directory, 'GQBF', gpkg_filename)\n",
    "\n",
    "    # Step 2: Check if the GeoPackage file exists\n",
    "    if not os.path.isfile(gpkg_path):\n",
    "        raise FileNotFoundError(f\"GeoPackage file not found: {gpkg_path}\")\n",
    "\n",
    "    print(f\"Loading GQBF data from: {gpkg_path}...\")\n",
    "\n",
    "    # Step 3: Load the entire GeoPackage\n",
    "    try:\n",
    "        gdf = gpd.read_file(gpkg_path)\n",
    "        print(\"GeoPackage successfully loaded.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading GeoPackage: {e}\")\n",
    "    \n",
    "    # Step 4: Ensure both layers are in the same CRS (EPSG:4326)\n",
    "    if gdf.crs.to_epsg() != 4326:\n",
    "        print(\"Reprojecting GQBF dataset to EPSG:4326...\")\n",
    "        gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "    # Step 5: Spatially filter GQBF data to the river reach\n",
    "    filtered_gdf = gdf[gdf.intersects(reach_gdf.unary_union)]\n",
    "\n",
    "    if filtered_gdf.empty:\n",
    "        print(f\"Warning: No matching features found for {river_name}.\")\n",
    "    \n",
    "    # Step 6: Save the new filtered shapefile (Optional)\n",
    "    output_path = os.path.join(working_directory, 'GQBF', 'Extracted_rivers', river_name)\n",
    "    os.makedirs(output_path, exist_ok=True)  # Ensure directory exists\n",
    "    shapefile_path = os.path.join(output_path, f\"{river_name}.shp\")\n",
    "\n",
    "    if not filtered_gdf.empty:\n",
    "        filtered_gdf.to_file(shapefile_path)\n",
    "        print(f\"Extracted shapefile saved at: {shapefile_path}\")\n",
    "        print(f\"GQBF for {river_name} loaded.\")\n",
    "        print(\"Columns in filtered GQBF:\", filtered_gdf.columns.tolist())\n",
    "\n",
    "    return filtered_gdf  # Return the filtered GeoDataFrame\n",
    "\n",
    "# This function gets the RivMapper reach polygons\n",
    "def get_reach(river_name, working_directory):\n",
    "    \"\"\"\n",
    "    Imports the reach shapefile for the given river, reprojects it to EPSG:4326, and returns it as a GeoDataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - river_name (str): The name of the river.\n",
    "    - working_directory (str): The directory where the reach shapefile is stored.\n",
    "\n",
    "    Returns:\n",
    "    - reach_gdf (GeoDataFrame): The reprojected reach GeoDataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Construct the file path\n",
    "    reach_shapefile_path = os.path.join(working_directory, 'RiverMapping', 'Reaches', river_name, f\"{river_name}.shp\")\n",
    "\n",
    "    # Step 2: Check if the file exists\n",
    "    if not os.path.isfile(reach_shapefile_path):\n",
    "        raise FileNotFoundError(f\"Reach shapefile not found: {reach_shapefile_path}\")\n",
    "\n",
    "    print(f\"Loading reach shapefile from: {reach_shapefile_path}...\")\n",
    "\n",
    "    # Step 3: Load the reach shapefile\n",
    "    try:\n",
    "        reach_gdf = gpd.read_file(reach_shapefile_path)\n",
    "        print(\"Reach shapefile successfully loaded.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading reach shapefile: {e}\")\n",
    "\n",
    "    # Step 4: Reproject to EPSG:4326 if necessary\n",
    "    if reach_gdf.crs is None:\n",
    "        raise ValueError(f\"Reach shapefile does not have a CRS: {reach_shapefile_path}\")\n",
    "    \n",
    "    if reach_gdf.crs.to_epsg() != 4326:\n",
    "        print(f\"Reprojecting reach shapefile to EPSG:4326...\")\n",
    "        reach_gdf = reach_gdf.to_crs(epsg=4326)\n",
    "\n",
    "    print(\"Reach shapefile successfully reprojected to EPSG:4326.\")\n",
    "\n",
    "    return reach_gdf\n",
    "\n",
    "# Trim the GQBF to just the main channel\n",
    "def trim_GQBF(reach_gdf, filtered_gdf):\n",
    "    \"\"\"\n",
    "    Further refines the filtered GQBF dataset to extract the river's mainstem.\n",
    "\n",
    "    Parameters:\n",
    "    - reach_gdf (GeoDataFrame): The river reach geometry.\n",
    "    - filtered_gdf (GeoDataFrame): The initially filtered GQBF dataset.\n",
    "\n",
    "    Returns:\n",
    "    - mainstem_gqbf_gdf (GeoDataFrame): The extracted mainstem of the river.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Trimming GQBF data to retain only ds_order = 1 segments...\")\n",
    "\n",
    "    # Select reaches where ds_order = 1\n",
    "    ds_order_1_reaches = reach_gdf[reach_gdf['ds_order'] == 1]\n",
    "\n",
    "    # Filter the GQBF dataset to include only segments that intersect with ds_order = 1 reaches\n",
    "    trimmed_gqbf_gdf = filtered_gdf[filtered_gdf.intersects(ds_order_1_reaches.unary_union)].copy()\n",
    "    \n",
    "    print(\"Identifying the most upstream segment...\")\n",
    "    \n",
    "    # Convert upstream_l to list of integers\n",
    "    def parse_upstream_l(value):\n",
    "        if isinstance(value, str):\n",
    "            return [int(v) for v in value.split(',')]\n",
    "        elif isinstance(value, int):\n",
    "            return [value]\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    trimmed_gqbf_gdf.loc[:, 'parsed_upstream_l'] = trimmed_gqbf_gdf['upstream_l'].apply(parse_upstream_l)\n",
    "    \n",
    "    # Find the most upstream segment\n",
    "    upstream_end_gqbf_gdf = trimmed_gqbf_gdf[trimmed_gqbf_gdf.apply(lambda row: all(up not in trimmed_gqbf_gdf['reach_id'].values for up in row['parsed_upstream_l']), axis=1)].copy()\n",
    "    \n",
    "    if not upstream_end_gqbf_gdf.empty:\n",
    "        current_segment = upstream_end_gqbf_gdf.loc[upstream_end_gqbf_gdf['qbf'].idxmax()].copy()\n",
    "    else:\n",
    "        return None  # No upstream segment found\n",
    "    \n",
    "    print(\"Upstream segement identified.\")\n",
    "    # Extract mainstem by following downstream connections\n",
    "    mainstem_segments = []\n",
    "    print(\"Mapping out mainstem.\")\n",
    "    while current_segment is not None:\n",
    "        mainstem_segments.append(current_segment)\n",
    "        \n",
    "        # Get downstream segment(s)\n",
    "        downstre_values = current_segment['downstre_1']\n",
    "        if isinstance(downstre_values, str):\n",
    "            downstream_ids = [int(v) for v in downstre_values.split(',')]\n",
    "        elif isinstance(downstre_values, int):\n",
    "            downstream_ids = [downstre_values]\n",
    "        else:\n",
    "            downstream_ids = []\n",
    "        \n",
    "        # Select the next segment\n",
    "        downstream_segments = filtered_gdf[filtered_gdf['reach_id'].isin(downstream_ids)]\n",
    "        if not downstream_segments.empty:\n",
    "            current_segment = downstream_segments.loc[downstream_segments['qbf'].idxmax()].copy()\n",
    "        else:\n",
    "            current_segment = None\n",
    "    \n",
    "    # Create final mainstem GeoDataFrame\n",
    "    mainstem_gqbf_gdf = filtered_gdf[filtered_gdf['reach_id'].isin([seg['reach_id'] for seg in mainstem_segments])].copy()\n",
    "    print(\"Mainstem mapped.\")\n",
    "    \n",
    "    return mainstem_gqbf_gdf\n",
    "\n",
    "def get_elevation(lat, lon):\n",
    "    \"\"\"Get elevation using ASTER30M via Open-Topodata\"\"\"\n",
    "    import requests\n",
    "    url = \"https://api.opentopodata.org/v1/aster30m\"\n",
    "    params = {\"locations\": f\"{lat},{lon}\"}\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        if data['status'] == 'OK':\n",
    "            return data['results'][0]['elevation']\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# Get channel slope at each reach\n",
    "def get_slope(reach_gdf, gqbf_gdf):\n",
    "    \"\"\"\n",
    "    Computes the channel slope for each reach in the reach_gdf using elevation data.\n",
    "\n",
    "    Parameters:\n",
    "    - reach_gdf (GeoDataFrame): River reach geometries.\n",
    "    - gqbf_gdf (GeoDataFrame): GQBF mainstem geometries with flow direction.\n",
    "\n",
    "    Returns:\n",
    "    - slope_dict (dict): Mapping from ds_order to estimated channel slope.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_point_elevation(point):\n",
    "        return get_elevation(point.y, point.x)\n",
    "\n",
    "    slope_dict = {}\n",
    "\n",
    "    for _, reach in reach_gdf.iterrows():\n",
    "        ds_order = reach['ds_order']\n",
    "\n",
    "        # Get intersecting GQBF segments\n",
    "        reach_segments = gqbf_gdf[gqbf_gdf.intersects(reach.geometry)]\n",
    "\n",
    "        if reach_segments.empty:\n",
    "            slope_dict[ds_order] = None\n",
    "            continue\n",
    "\n",
    "        # Merge all lines into one to form a continuous path if possible\n",
    "        merged_line = linemerge(reach_segments.geometry.values)\n",
    "\n",
    "        if merged_line.geom_type == 'MultiLineString':\n",
    "            merged_line = max(merged_line, key=lambda l: l.length)\n",
    "\n",
    "        line = LineString(merged_line)\n",
    "        total_length = reach_segments['length'].sum()  # Use actual length from attribute (in meters)\n",
    "\n",
    "        if total_length == 0:\n",
    "            slope_dict[ds_order] = None\n",
    "            continue\n",
    "\n",
    "        distances = np.linspace(0, line.length, 10)\n",
    "        points = [line.interpolate(d) for d in distances]\n",
    "\n",
    "        elevations = []\n",
    "        for pt in points:\n",
    "            try:\n",
    "                elev = get_point_elevation(pt)\n",
    "                elevations.append(elev)\n",
    "            except:\n",
    "                elevations.append(None)\n",
    "\n",
    "        valid = [(d, e) for d, e in zip(distances, elevations) if e is not None]\n",
    "        if len(valid) < 2:\n",
    "            slope_dict[ds_order] = None\n",
    "            continue\n",
    "\n",
    "        dists, elevs = zip(*valid)\n",
    "        slope, _ = np.polyfit(dists, elevs, 1)\n",
    "        slope = slope * (line.length / total_length)\n",
    "\n",
    "        slope_dict[ds_order] = slope\n",
    "\n",
    "    return slope_dict\n",
    "\n",
    "def load_based_model(working_directory):\n",
    "    \"\"\"\n",
    "    Load the BASED XGBoost model for depth prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    - working_directory (str): The base working directory.\n",
    "    \n",
    "    Returns:\n",
    "    - model (xgb.Booster): Loaded XGBoost model.\n",
    "    \"\"\"\n",
    "    model_path = os.path.join(working_directory, 'Gearon_etal_2024', 'based-api', 'based_us_sans_trampush_early_stopping_combat_overfitting.ubj')\n",
    "    \n",
    "    if not os.path.isfile(model_path):\n",
    "        raise FileNotFoundError(f\"BASED model file not found: {model_path}\")\n",
    "    \n",
    "    print(f\"Loading BASED model from: {model_path}\")\n",
    "    model = xgb.Booster()\n",
    "    model.load_model(model_path)\n",
    "    print(\"BASED model loaded successfully.\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def predict_depth_based(model, width, slope, discharge):\n",
    "    \"\"\"\n",
    "    Predict channel depth using the BASED model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (xgb.Booster): The loaded BASED XGBoost model.\n",
    "    - width (float): Channel width in meters.\n",
    "    - slope (float): Channel slope (dimensionless, e.g., -0.001 for 0.1% gradient).\n",
    "    - discharge (float): Bankfull discharge in mÂ³/s.\n",
    "    \n",
    "    Returns:\n",
    "    - depth (float): Predicted bankfull depth in meters, or None if inputs invalid.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate inputs\n",
    "    if width is None or slope is None or discharge is None:\n",
    "        return None\n",
    "    \n",
    "    if width <= 0 or discharge <= 0:\n",
    "        return None\n",
    "    \n",
    "    # BASED uses absolute value of slope (magnitude only)\n",
    "    slope_abs = abs(slope)\n",
    "    \n",
    "    if slope_abs == 0:\n",
    "        return None\n",
    "    \n",
    "    # Create DataFrame with correct feature order: width, slope, discharge\n",
    "    input_data = pd.DataFrame({\n",
    "        'width': [width],\n",
    "        'slope': [slope_abs],\n",
    "        'discharge': [discharge]\n",
    "    })\n",
    "    \n",
    "    # Convert to DMatrix\n",
    "    dmatrix = xgb.DMatrix(input_data)\n",
    "    \n",
    "    # Predict\n",
    "    try:\n",
    "        prediction = model.predict(dmatrix)\n",
    "        depth = float(prediction[0])\n",
    "        \n",
    "        # Basic sanity check\n",
    "        if depth <= 0:\n",
    "            return None\n",
    "            \n",
    "        return depth\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting depth: {e}\")\n",
    "        return None\n",
    "\n",
    "# Calculate hydraulic geometry by reach\n",
    "def calculate_hydraulic_geom(river_name, continent_abr, working_directory):\n",
    "    \"\"\"\n",
    "    Calculates hydraulic geometry parameters for each reach within the river segment.\n",
    "\n",
    "    Parameters:\n",
    "    - river_name (str): The name of the river.\n",
    "    - continent_abr (str): Two-letter continent abbreviation.\n",
    "    - working_directory (str): The directory where data is stored.\n",
    "\n",
    "    Outputs:\n",
    "    - A CSV file named \"river_name_hydraulic_geometry.csv\" containing the hydraulic geometry calculations for each reach.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call get_reach() to retrieve reach geometry\n",
    "    reach_gdf = get_reach(river_name, working_directory)\n",
    "    \n",
    "    # Step 1: Retrieve GQBF data\n",
    "    gqbf_gdf = get_GQBF(river_name, reach_gdf, continent_abr, working_directory)\n",
    "\n",
    "    # Step 1.5: Compute slope for each reach\n",
    "    slope_dict = get_slope(reach_gdf, gqbf_gdf)\n",
    "    \n",
    "    # Step 1.75: Load BASED model once for this river\n",
    "    try:\n",
    "        based_model = load_based_model(working_directory)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Warning: {e}\")\n",
    "        print(\"Continuing without BASED depth predictions.\")\n",
    "        based_model = None\n",
    "\n",
    "    print(f\"Calculating hydraulic geometry for {river_name}...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Step 2: Iterate through each reach in reach_gdf\n",
    "    for _, reach in reach_gdf.iterrows():\n",
    "        ds_order = reach[\"ds_order\"]\n",
    "\n",
    "        # Select segments from gqbf_gdf that intersect with the current reach\n",
    "        reach_segments = gqbf_gdf[gqbf_gdf.intersects(reach.geometry)]\n",
    "\n",
    "        if not reach_segments.empty:\n",
    "            median_width = reach_segments[\"grwl_width\"].median()\n",
    "            median_qbf = reach_segments[\"qbf\"].median()\n",
    "            length = reach_segments[\"length\"].sum()\n",
    "        else:\n",
    "            median_width = median_qbf = length = None\n",
    "\n",
    "        slope = slope_dict.get(ds_order, None)\n",
    "        \n",
    "        # Predict depth using BASED model\n",
    "        depth = None\n",
    "        if based_model is not None:\n",
    "            depth = predict_depth_based(based_model, median_width, slope, median_qbf)\n",
    "\n",
    "        results.append({\n",
    "            \"ds_order\": ds_order,\n",
    "            \"median_width_m\": median_width,\n",
    "            \"median_qbf_m3s\": median_qbf,\n",
    "            \"length_m\": length,\n",
    "            \"slope\": slope,\n",
    "            \"BASED_depth_m\": depth\n",
    "        })\n",
    "    \n",
    "    # Step 3: Convert results to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Step 3.5: Print summary statistics\n",
    "    print(f\"\\n=== Hydraulic Geometry Summary for {river_name} ===\")\n",
    "    print(f\"Total reaches: {len(df)}\")\n",
    "    \n",
    "    valid_widths = df['median_width_m'].notna().sum()\n",
    "    valid_discharges = df['median_qbf_m3s'].notna().sum()\n",
    "    valid_slopes = df['slope'].notna().sum()\n",
    "    valid_depths = df['BASED_depth_m'].notna().sum()\n",
    "    \n",
    "    print(f\"Reaches with valid width: {valid_widths}/{len(df)}\")\n",
    "    print(f\"Reaches with valid discharge: {valid_discharges}/{len(df)}\")\n",
    "    print(f\"Reaches with valid slope: {valid_slopes}/{len(df)}\")\n",
    "    print(f\"Reaches with valid BASED depth: {valid_depths}/{len(df)}\")\n",
    "    \n",
    "    if valid_depths > 0:\n",
    "        print(f\"\\nBASED Depth Statistics:\")\n",
    "        print(f\"  Mean: {df['BASED_depth_m'].mean():.2f} m\")\n",
    "        print(f\"  Median: {df['BASED_depth_m'].median():.2f} m\")\n",
    "        print(f\"  Min: {df['BASED_depth_m'].min():.2f} m\")\n",
    "        print(f\"  Max: {df['BASED_depth_m'].max():.2f} m\")\n",
    "    else:\n",
    "        print(\"\\nWarning: No valid BASED depth predictions generated.\")\n",
    "    \n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Step 4: Ensure output directory exists\n",
    "    output_dir = os.path.join(working_directory, \"RiverMapping\", \"HydraulicGeometry\", river_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Step 5: Save to CSV\n",
    "    output_csv_path = os.path.join(output_dir, f\"{river_name}_hydraulic_geometry.csv\")\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    print(f\"Hydraulic geometry data saved to: {output_csv_path}\")\n",
    "\n",
    "# Main function to calcualte hydraulic geometry for all rivers in the .csv\n",
    "def process_hydraulic_geom_calculator(csv_file_path):\n",
    "    \"\"\"\n",
    "    Process multiple rivers by generating river masks for each entry in the input CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    csv_file (str): The file path to the CSV containing input parameters for multiple rivers.\n",
    "                    Each row should specify parameters such as:\n",
    "                    - River name\n",
    "                    - Working directory\n",
    "                    - HydroAtlas zone\n",
    "    Workflow:\n",
    "    1. Read the CSV file to get parameters for each river.\n",
    "    2. Loop through each river in the CSV to extract hydraulic geometry metrics for every reach.\n",
    "\n",
    "    Returns:\n",
    "    None: This function processes and exports river masks for each river specified in the CSV file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Read the CSV file containing input variables for multiple rivers\n",
    "    river_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Step 2: Loop through each row (each river) in the CSV\n",
    "    for index, row in river_data.iterrows():\n",
    "        # Extract necessary input values from the current CSV row\n",
    "        river_name = row['river_name']  # Name of the river\n",
    "        working_directory = row['working_directory']  # Directory for processing\n",
    "        continent_abbr = row['hydroatlas_zone']  # Continent abbreviation to select GQBF dataset in case extraction needed\n",
    "        \n",
    "        # Step 3: Call the function to process the river mask for the current river\n",
    "        calculate_hydraulic_geom(river_name, continent_abbr, working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "122da849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reach shapefile from: E:\\Dissertation\\Data\\RiverMapping\\Reaches\\Yukon_Beaver\\Yukon_Beaver.shp...\n",
      "Reach shapefile successfully loaded.\n",
      "Reprojecting reach shapefile to EPSG:4326...\n",
      "Reach shapefile successfully reprojected to EPSG:4326.\n",
      "Shapefile found: E:\\Dissertation\\Data\\GQBF\\Extracted_rivers\\Yukon_Beaver\\Yukon_Beaver.shp. Loading data...\n",
      "Trimming GQBF data to retain only ds_order = 1 segments...\n",
      "Identifying the most upstream segment...\n",
      "Upstream segement identified.\n",
      "Mapping out mainstem.\n",
      "Mainstem mapped.\n",
      "Loading BASED model from: E:\\Dissertation\\Data\\Gearon_etal_2024\\based-api\\based_us_sans_trampush_early_stopping_combat_overfitting.ubj\n",
      "BASED model loaded successfully.\n",
      "Calculating hydraulic geometry for Yukon_Beaver...\n",
      "\n",
      "=== Hydraulic Geometry Summary for Yukon_Beaver ===\n",
      "Total reaches: 1\n",
      "Reaches with valid width: 1/1\n",
      "Reaches with valid discharge: 1/1\n",
      "Reaches with valid slope: 1/1\n",
      "Reaches with valid BASED depth: 1/1\n",
      "\n",
      "BASED Depth Statistics:\n",
      "  Mean: 4.27 m\n",
      "  Median: 4.27 m\n",
      "  Min: 4.27 m\n",
      "  Max: 4.27 m\n",
      "==================================================\n",
      "\n",
      "Hydraulic geometry data saved to: E:\\Dissertation\\Data\\RiverMapping\\HydraulicGeometry\\Yukon_Beaver\\Yukon_Beaver_hydraulic_geometry.csv\n",
      "Loading reach shapefile from: E:\\Dissertation\\Data\\RiverMapping\\Reaches\\Koyukuk_Huslia\\Koyukuk_Huslia.shp...\n",
      "Reach shapefile successfully loaded.\n",
      "Reprojecting reach shapefile to EPSG:4326...\n",
      "Reach shapefile successfully reprojected to EPSG:4326.\n",
      "Shapefile found: E:\\Dissertation\\Data\\GQBF\\Extracted_rivers\\Koyukuk_Huslia\\Koyukuk_Huslia.shp. Loading data...\n",
      "Trimming GQBF data to retain only ds_order = 1 segments...\n",
      "Identifying the most upstream segment...\n",
      "Upstream segement identified.\n",
      "Mapping out mainstem.\n",
      "Mainstem mapped.\n",
      "Loading BASED model from: E:\\Dissertation\\Data\\Gearon_etal_2024\\based-api\\based_us_sans_trampush_early_stopping_combat_overfitting.ubj\n",
      "BASED model loaded successfully.\n",
      "Calculating hydraulic geometry for Koyukuk_Huslia...\n",
      "\n",
      "=== Hydraulic Geometry Summary for Koyukuk_Huslia ===\n",
      "Total reaches: 1\n",
      "Reaches with valid width: 1/1\n",
      "Reaches with valid discharge: 1/1\n",
      "Reaches with valid slope: 1/1\n",
      "Reaches with valid BASED depth: 1/1\n",
      "\n",
      "BASED Depth Statistics:\n",
      "  Mean: 9.97 m\n",
      "  Median: 9.97 m\n",
      "  Min: 9.97 m\n",
      "  Max: 9.97 m\n",
      "==================================================\n",
      "\n",
      "Hydraulic geometry data saved to: E:\\Dissertation\\Data\\RiverMapping\\HydraulicGeometry\\Koyukuk_Huslia\\Koyukuk_Huslia_hydraulic_geometry.csv\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = r\"E:\\Dissertation\\Data\\Geyman_river_datasheet.csv\"\n",
    "process_hydraulic_geom_calculator(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52bd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rivgraph)",
   "language": "python",
   "name": "rivgraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
