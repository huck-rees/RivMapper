{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "097967ed",
   "metadata": {},
   "source": [
    "# Hydraulic Geometry Calculator\n",
    "\n",
    "The following code takes the standard RivMapper reach polygons, and clips and trims the Global Bankfull Discharge Dataset (GQBF) to each reach. Using this dataset. Using GQBF, the ArcticDEM, NASADEM, the BASED stream depth estimator API, and standard Python geospatial libraries, the code extracts the median wetted channel width, median bankfull discharge, channel length, channel slope, and estimated bankfull channel depth for each reach, outputting all metrics to a .csv and mapping elevation sampling points and exporting the channel map and slope regression to PNGs.\n",
    "\n",
    "Global River BankFull Discharge (GQBF): \n",
    "Liu, Y., Wortmann, M., & Slater, L. (2024). Global River BankFull Discharge (GQBF) (0.1) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.13855371\n",
    "\n",
    "NASADEM: https://developers.google.com/earth-engine/datasets/catalog/NASA_NASADEM_HGT_001\n",
    "\n",
    "ArcticDEM:https://www.pgc.umn.edu/data/arcticdem/\n",
    "\n",
    "Boost-Assisted Stream Estimator for Depth (BASED):https://github.com/jameshgrn/based_api\n",
    "\n",
    "Author: James (Huck) Rees; PhD Student, UCSB Geography\n",
    "\n",
    "Date: January 21, 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916fabd",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4a3b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyproj\n",
    "from pyproj import CRS, Transformer\n",
    "import os\n",
    "from shapely.ops import unary_union, split, linemerge\n",
    "from shapely.geometry import LineString, Point\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import ee\n",
    "\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dfa812",
   "metadata": {},
   "source": [
    "## Initialize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9e2e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GQBF(river_name, reach_gdf, continent_abr, working_directory):\n",
    "    shapefile_path = os.path.join(working_directory, 'GQBF', 'Extracted_rivers', river_name, f\"{river_name}.shp\")\n",
    "    \n",
    "    if os.path.isfile(shapefile_path):\n",
    "        gdf = gpd.read_file(shapefile_path)\n",
    "    else:\n",
    "        gdf = extract_GQBF(river_name, reach_gdf, continent_abr, working_directory)\n",
    "\n",
    "    if gdf is not None and not gdf.empty:\n",
    "        trimmed_gdf = trim_GQBF(reach_gdf, gdf)\n",
    "        return trimmed_gdf\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_GQBF(river_name, reach_gdf, continent_abr, working_directory):\n",
    "    gpkg_filename = f\"GQBFv0.1_reaches_{continent_abr}_EPSG4326.gpkg\"\n",
    "    gpkg_path = os.path.join(working_directory, 'GQBF', gpkg_filename)\n",
    "\n",
    "    if not os.path.isfile(gpkg_path):\n",
    "        raise FileNotFoundError(f\"GeoPackage file not found: {gpkg_path}\")\n",
    "\n",
    "    gdf = gpd.read_file(gpkg_path)\n",
    "    \n",
    "    if gdf.crs.to_epsg() != 4326:\n",
    "        gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "    filtered_gdf = gdf[gdf.intersects(reach_gdf.unary_union)]\n",
    "    \n",
    "    output_path = os.path.join(working_directory, 'GQBF', 'Extracted_rivers', river_name)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    shapefile_path = os.path.join(output_path, f\"{river_name}.shp\")\n",
    "\n",
    "    if not filtered_gdf.empty:\n",
    "        filtered_gdf.to_file(shapefile_path)\n",
    "\n",
    "    return filtered_gdf\n",
    "\n",
    "def get_reach(river_name, working_directory):\n",
    "    reach_shapefile_path = os.path.join(working_directory, 'RiverMapping', 'Reaches', river_name, f\"{river_name}.shp\")\n",
    "\n",
    "    if not os.path.isfile(reach_shapefile_path):\n",
    "        raise FileNotFoundError(f\"Reach shapefile not found: {reach_shapefile_path}\")\n",
    "\n",
    "    reach_gdf = gpd.read_file(reach_shapefile_path)\n",
    "\n",
    "    if reach_gdf.crs is None:\n",
    "        raise ValueError(f\"Reach shapefile does not have a CRS: {reach_shapefile_path}\")\n",
    "    \n",
    "    if reach_gdf.crs.to_epsg() != 4326:\n",
    "        reach_gdf = reach_gdf.to_crs(epsg=4326)\n",
    "\n",
    "    return reach_gdf\n",
    "\n",
    "def trim_GQBF(reach_gdf, filtered_gdf):\n",
    "    ds_order_1_reaches = reach_gdf[reach_gdf['ds_order'] == 1]\n",
    "    trimmed_gqbf_gdf = filtered_gdf[filtered_gdf.intersects(ds_order_1_reaches.unary_union)].copy()\n",
    "    \n",
    "    def parse_upstream_l(value):\n",
    "        if isinstance(value, str):\n",
    "            return [int(v) for v in value.split(',')]\n",
    "        elif isinstance(value, int):\n",
    "            return [value]\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    trimmed_gqbf_gdf.loc[:, 'parsed_upstream_l'] = trimmed_gqbf_gdf['upstream_l'].apply(parse_upstream_l)\n",
    "    \n",
    "    upstream_end_gqbf_gdf = trimmed_gqbf_gdf[trimmed_gqbf_gdf.apply(lambda row: all(up not in trimmed_gqbf_gdf['reach_id'].values for up in row['parsed_upstream_l']), axis=1)].copy()\n",
    "    \n",
    "    if not upstream_end_gqbf_gdf.empty:\n",
    "        current_segment = upstream_end_gqbf_gdf.loc[upstream_end_gqbf_gdf['qbf'].idxmax()].copy()\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    mainstem_segments = []\n",
    "    while current_segment is not None:\n",
    "        mainstem_segments.append(current_segment)\n",
    "        \n",
    "        downstre_values = current_segment['downstre_1']\n",
    "        if isinstance(downstre_values, str):\n",
    "            downstream_ids = [int(v) for v in downstre_values.split(',')]\n",
    "        elif isinstance(downstre_values, int):\n",
    "            downstream_ids = [downstre_values]\n",
    "        else:\n",
    "            downstream_ids = []\n",
    "        \n",
    "        downstream_segments = filtered_gdf[filtered_gdf['reach_id'].isin(downstream_ids)]\n",
    "        if not downstream_segments.empty:\n",
    "            current_segment = downstream_segments.loc[downstream_segments['qbf'].idxmax()].copy()\n",
    "        else:\n",
    "            current_segment = None\n",
    "    \n",
    "    ordered_reach_ids = [seg['reach_id'] for seg in mainstem_segments]\n",
    "    segment_dict = {seg['reach_id']: seg for seg in mainstem_segments}\n",
    "    mainstem_gqbf_gdf = gpd.GeoDataFrame([segment_dict[rid] for rid in ordered_reach_ids])\n",
    "    mainstem_gqbf_gdf.crs = filtered_gdf.crs\n",
    "    \n",
    "    return mainstem_gqbf_gdf\n",
    "\n",
    "def get_elevation(lat, lon, max_retries=3):\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            point = ee.Geometry.Point([lon, lat])\n",
    "            \n",
    "            if lat >= 60:\n",
    "                dem = ee.Image('UMN/PGC/ArcticDEM/V3/2m_mosaic')\n",
    "                band = 'elevation'\n",
    "                scale = 32\n",
    "            else:\n",
    "                dem = ee.Image('NASA/NASADEM_HGT/001')\n",
    "                band = 'elevation'\n",
    "                scale = 30\n",
    "            \n",
    "            sample = dem.select(band).sample(point, scale).first()\n",
    "            \n",
    "            if sample:\n",
    "                elevation = sample.get(band).getInfo()\n",
    "                if elevation is not None and -500 < elevation < 9000:\n",
    "                    return elevation, None\n",
    "                else:\n",
    "                    return None, f\"Invalid elevation: {elevation}\"\n",
    "            \n",
    "            return None, \"No data at location\"\n",
    "            \n",
    "        except ee.EEException as e:\n",
    "            error_msg = str(e)\n",
    "            if 'User memory limit exceeded' in error_msg or 'Computation timed out' in error_msg:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(2 ** attempt)\n",
    "                    continue\n",
    "            return None, f\"GEE Error: {error_msg[:100]}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            return None, f\"Error: {str(e)}\"\n",
    "    \n",
    "    return None, \"Max retries exceeded\"\n",
    "\n",
    "def get_slope(reach_gdf, gqbf_gdf, river_name, working_directory):\n",
    "    def get_point_elevation(point):\n",
    "        return get_elevation(point.y, point.x)\n",
    "    \n",
    "    slope_dict = {}\n",
    "    \n",
    "    for idx, reach in reach_gdf.iterrows():\n",
    "        ds_order = reach['ds_order']\n",
    "        \n",
    "        reach_segments = gqbf_gdf[gqbf_gdf.intersects(reach.geometry)].copy()\n",
    "        \n",
    "        if reach_segments.empty:\n",
    "            slope_dict[ds_order] = None\n",
    "            continue\n",
    "        \n",
    "        total_length = reach_segments['length'].sum()\n",
    "        \n",
    "        if total_length == 0:\n",
    "            slope_dict[ds_order] = None\n",
    "            continue\n",
    "        \n",
    "        # Sample elevations\n",
    "        n_samples = 25\n",
    "        target_distances = np.linspace(0, total_length, n_samples)\n",
    "        \n",
    "        segment_cumulative_distances = [0]\n",
    "        for _, seg in reach_segments.iterrows():\n",
    "            segment_cumulative_distances.append(segment_cumulative_distances[-1] + seg['length'])\n",
    "        \n",
    "        elevations = []\n",
    "        distances = []\n",
    "        sample_points = []\n",
    "        sample_numbers = []\n",
    "        \n",
    "        for i, target_dist in enumerate(target_distances):\n",
    "            seg_idx = 0\n",
    "            for j in range(len(segment_cumulative_distances) - 1):\n",
    "                if segment_cumulative_distances[j] <= target_dist < segment_cumulative_distances[j + 1]:\n",
    "                    seg_idx = j\n",
    "                    break\n",
    "            else:\n",
    "                seg_idx = len(reach_segments) - 1\n",
    "            \n",
    "            seg = reach_segments.iloc[seg_idx]\n",
    "            dist_from_seg_start = target_dist - segment_cumulative_distances[seg_idx]\n",
    "            fraction_in_seg = dist_from_seg_start / seg['length'] if seg['length'] > 0 else 0\n",
    "            fraction_in_seg = np.clip(fraction_in_seg, 0, 1)\n",
    "            point = seg.geometry.interpolate(fraction_in_seg, normalized=True)\n",
    "            \n",
    "            try:\n",
    "                elev, error = get_point_elevation(point)\n",
    "                if elev is not None:\n",
    "                    elevations.append(elev)\n",
    "                    distances.append(target_dist)\n",
    "                    sample_points.append(point)\n",
    "                    sample_numbers.append(i + 1)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if len(elevations) < 2:\n",
    "            slope_dict[ds_order] = None\n",
    "            continue\n",
    "        \n",
    "        # Outlier detection\n",
    "        elevations_array = np.array(elevations)\n",
    "        distances_array = np.array(distances)\n",
    "        sample_numbers_array = np.array(sample_numbers)\n",
    "        \n",
    "        median_elev = np.median(elevations_array)\n",
    "        q1 = np.percentile(elevations_array, 25)\n",
    "        q3 = np.percentile(elevations_array, 75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 2.5 * iqr\n",
    "        upper_bound = q3 + 2.5 * iqr\n",
    "        \n",
    "        is_outlier = (elevations_array < lower_bound) | (elevations_array > upper_bound)\n",
    "        \n",
    "        elevations_clean = elevations_array[~is_outlier]\n",
    "        distances_clean = distances_array[~is_outlier]\n",
    "        sample_numbers_clean = sample_numbers_array[~is_outlier]\n",
    "        \n",
    "        if len(elevations_clean) < 2:\n",
    "            slope_dict[ds_order] = None\n",
    "            continue\n",
    "        \n",
    "        # Sort by distance\n",
    "        sort_indices = np.argsort(distances_clean)\n",
    "        distances_clean = distances_clean[sort_indices]\n",
    "        elevations_clean = elevations_clean[sort_indices]\n",
    "        sample_numbers_clean = sample_numbers_clean[sort_indices]\n",
    "        \n",
    "        # Calculate slope\n",
    "        slope_channel, intercept = np.polyfit(distances_clean, elevations_clean, 1)\n",
    "        gradient_magnitude = abs(slope_channel)\n",
    "        r_squared = np.corrcoef(distances_clean, elevations_clean)[0,1]**2\n",
    "        \n",
    "        slope_dict[ds_order] = gradient_magnitude\n",
    "        \n",
    "        # Create output directory\n",
    "        output_dir = os.path.join(working_directory, \"RiverMapping\", \"HydraulicGeometry\", river_name, \"Slope_regressions\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Plot 1: Channel map\n",
    "        fig1, ax1 = plt.subplots(figsize=(10, 8))\n",
    "        for i, (_, seg) in enumerate(reach_segments.iterrows()):\n",
    "            ax1.plot(*seg.geometry.xy, 'b-', linewidth=2, alpha=0.5)\n",
    "        \n",
    "        clean_indices = np.where(~is_outlier)[0]\n",
    "        for idx_val in clean_indices:\n",
    "            pt = sample_points[idx_val]\n",
    "            pt_num = sample_numbers_array[idx_val]\n",
    "            ax1.plot(pt.x, pt.y, 'go', markersize=6, zorder=5)\n",
    "            ax1.text(pt.x, pt.y, f' {pt_num}', fontsize=7, ha='left', va='bottom',\n",
    "                    bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.7))\n",
    "        \n",
    "        ax1.set_title(f'{river_name} - Reach {ds_order}: Sample Points', fontsize=12, fontweight='bold')\n",
    "        ax1.set_xlabel('Longitude')\n",
    "        ax1.set_ylabel('Latitude')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'{river_name}_reach{ds_order}_map.png'), dpi=200, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot 2: Elevation profile\n",
    "        fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        for i, (dist, elev, num) in enumerate(zip(distances_array, elevations_array, sample_numbers_array)):\n",
    "            if is_outlier[i]:\n",
    "                ax2.plot(dist, elev, 'rx', markersize=10, markeredgewidth=2, zorder=6)\n",
    "            else:\n",
    "                ax2.plot(dist, elev, 'bo', markersize=8, zorder=5)\n",
    "                ax2.text(dist, elev, f' {num}', fontsize=8, ha='left', va='bottom')\n",
    "        \n",
    "        fit_line = slope_channel * distances_clean + intercept\n",
    "        ax2.plot(distances_clean, fit_line, 'r-', linewidth=2.5, \n",
    "                label=f'Slope={slope_channel:.6f} (R²={r_squared:.3f})', zorder=4)\n",
    "        \n",
    "        ax2.set_title(f'{river_name} - Reach {ds_order}: Elevation Profile', fontsize=12, fontweight='bold')\n",
    "        ax2.set_xlabel('Distance along channel (m)')\n",
    "        ax2.set_ylabel('Elevation (m)')\n",
    "        ax2.legend(loc='best')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'{river_name}_reach{ds_order}_profile.png'), dpi=200, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    return slope_dict\n",
    "\n",
    "def load_based_model(working_directory):\n",
    "    model_path = os.path.join(working_directory, 'Gearon_etal_2024', 'based-api', 'based_us_sans_trampush_early_stopping_combat_overfitting.ubj')\n",
    "    \n",
    "    if not os.path.isfile(model_path):\n",
    "        raise FileNotFoundError(f\"BASED model file not found: {model_path}\")\n",
    "    \n",
    "    model = xgb.Booster()\n",
    "    model.load_model(model_path)\n",
    "    return model\n",
    "\n",
    "def predict_depth_based(model, width, slope, discharge):\n",
    "    if width is None or slope is None or discharge is None:\n",
    "        return None\n",
    "    \n",
    "    if width <= 0 or discharge <= 0:\n",
    "        return None\n",
    "    \n",
    "    slope_abs = abs(slope)\n",
    "    if slope_abs == 0:\n",
    "        return None\n",
    "    \n",
    "    input_data = pd.DataFrame({\n",
    "        'width': [width],\n",
    "        'slope': [slope_abs],\n",
    "        'discharge': [discharge]\n",
    "    })\n",
    "    \n",
    "    dmatrix = xgb.DMatrix(input_data)\n",
    "    \n",
    "    try:\n",
    "        prediction = model.predict(dmatrix)\n",
    "        depth = float(prediction[0])\n",
    "        if depth <= 0:\n",
    "            return None\n",
    "        return depth\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def calculate_hydraulic_geom(river_name, continent_abr, working_directory):\n",
    "    print(f\"\\nProcessing {river_name}...\")\n",
    "    \n",
    "    reach_gdf = get_reach(river_name, working_directory)\n",
    "    gqbf_gdf = get_GQBF(river_name, reach_gdf, continent_abr, working_directory)\n",
    "    slope_dict = get_slope(reach_gdf, gqbf_gdf, river_name, working_directory)\n",
    "    \n",
    "    try:\n",
    "        based_model = load_based_model(working_directory)\n",
    "    except:\n",
    "        based_model = None\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for _, reach in reach_gdf.iterrows():\n",
    "        ds_order = reach[\"ds_order\"]\n",
    "        reach_segments = gqbf_gdf[gqbf_gdf.intersects(reach.geometry)]\n",
    "\n",
    "        if not reach_segments.empty:\n",
    "            median_width = reach_segments[\"grwl_width\"].median()\n",
    "            median_qbf = reach_segments[\"qbf\"].median()\n",
    "            length = reach_segments[\"length\"].sum()\n",
    "        else:\n",
    "            median_width = median_qbf = length = None\n",
    "\n",
    "        slope = slope_dict.get(ds_order, None)\n",
    "        \n",
    "        depth = None\n",
    "        if based_model is not None:\n",
    "            depth = predict_depth_based(based_model, median_width, slope, median_qbf)\n",
    "\n",
    "        results.append({\n",
    "            \"ds_order\": ds_order,\n",
    "            \"median_width_m\": median_width,\n",
    "            \"median_qbf_m3s\": median_qbf,\n",
    "            \"length_m\": length,\n",
    "            \"slope\": slope,\n",
    "            \"BASED_depth_m\": depth\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    output_dir = os.path.join(working_directory, \"RiverMapping\", \"HydraulicGeometry\", river_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    output_csv_path = os.path.join(output_dir, f\"{river_name}_hydraulic_geometry.csv\")\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    print(f\"  ✓ Completed: {output_csv_path}\")\n",
    "\n",
    "def process_hydraulic_geom_calculator(csv_file_path):\n",
    "    river_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "    for index, row in river_data.iterrows():\n",
    "        river_name = row['river_name']\n",
    "        working_directory = row['working_directory']\n",
    "        continent_abbr = row['hydroatlas_zone']\n",
    "        \n",
    "        calculate_hydraulic_geom(river_name, continent_abbr, working_directory)\n",
    "    \n",
    "    print(\"\\n✓ All rivers processed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9102d1e3",
   "metadata": {},
   "source": [
    "## Input RivMapper .csv path and run hydraulic geometry calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ed58fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Bermejo...\n",
      "  ✓ Completed: E:\\Dissertation\\Data\\RiverMapping\\HydraulicGeometry\\Bermejo\\Bermejo_hydraulic_geometry.csv\n",
      "\n",
      "✓ All rivers processed successfully!\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = r\"E:\\Dissertation\\Data\\Bermejo_river_datasheet.csv\"\n",
    "process_hydraulic_geom_calculator(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1181c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
