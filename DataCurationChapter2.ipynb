{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ab56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_river_folders(base_directory):\n",
    "    \"\"\"\n",
    "    Create folders for river monitoring stations in the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    base_directory (str): The path where folders should be created\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the base directory if it doesn't exist\n",
    "    if not os.path.exists(base_directory):\n",
    "        os.makedirs(base_directory)\n",
    "        print(f\"Created base directory: {base_directory}\")\n",
    "    \n",
    "    # Create each folder\n",
    "    created_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for folder_name in folder_names:\n",
    "        folder_path = os.path.join(base_directory, folder_name)\n",
    "        \n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "            created_count += 1\n",
    "            print(f\"Created: {folder_name}\")\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "            print(f\"Already exists: {folder_name}\")\n",
    "    \n",
    "    print(f\"\\n✓ Summary: {created_count} folders created, {skipped_count} already existed\")\n",
    "    print(f\"Total folders: {len(folder_names)}\")\n",
    "\n",
    "# Run it\n",
    "target_directory = r\"E:\\Dissertation\\Data\\RiverMapping\\Reaches\"\n",
    "create_river_folders(target_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70bdeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "def convert_gpkg_to_shapefile():\n",
    "    \"\"\"\n",
    "    Convert .gpkg files to shapefiles with specific attributes and reprojection.\n",
    "    Searches through all planform types and river names, converts each .gpkg to a shapefile\n",
    "    with a single 'ds_order' attribute set to 1, reprojects to EPSG:3395, and saves to\n",
    "    the corresponding reach folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define paths\n",
    "    source_base = r\"E:\\Dissertation\\Data\\Zhao_etal_2025\"\n",
    "    target_base = r\"E:\\Dissertation\\Data\\RiverMapping\\Reaches\"\n",
    "    \n",
    "    # Define planform types\n",
    "    planform_types = [\"B\", \"HSW\", \"LSW\", \"Me\"]\n",
    "    \n",
    "    # Counters for summary\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    not_found_count = 0\n",
    "    errors = []\n",
    "    \n",
    "    # Get list of all river folders from the target directory\n",
    "    river_folders = [f for f in os.listdir(target_base) \n",
    "                    if os.path.isdir(os.path.join(target_base, f))]\n",
    "    \n",
    "    print(f\"Found {len(river_folders)} river reach folders\")\n",
    "    print(f\"Searching through {len(planform_types)} planform types\\n\")\n",
    "    \n",
    "    # Process each river and planform combination\n",
    "    for river_name in river_folders:\n",
    "        found_gpkg = False\n",
    "        \n",
    "        for planform in planform_types:\n",
    "            # Construct source path\n",
    "            source_dir = os.path.join(source_base, planform, river_name)\n",
    "            source_file = os.path.join(source_dir, f\"{river_name}.gpkg\")\n",
    "            \n",
    "            # Check if the .gpkg file exists\n",
    "            if os.path.exists(source_file):\n",
    "                found_gpkg = True\n",
    "                \n",
    "                try:\n",
    "                    # Read the geopackage\n",
    "                    gdf = gpd.read_file(source_file)\n",
    "                    \n",
    "                    # Verify it has exactly one feature\n",
    "                    if len(gdf) != 1:\n",
    "                        print(f\"⚠ Warning: {river_name} ({planform}) has {len(gdf)} features, expected 1\")\n",
    "                    \n",
    "                    # Create new GeoDataFrame with only ds_order attribute\n",
    "                    gdf_new = gpd.GeoDataFrame(\n",
    "                        {'ds_order': [1]},\n",
    "                        geometry=gdf.geometry,\n",
    "                        crs=gdf.crs\n",
    "                    )\n",
    "                    \n",
    "                    # Reproject to EPSG:3395 (WGS 84 / World Mercator)\n",
    "                    gdf_reprojected = gdf_new.to_crs('EPSG:3395')\n",
    "                    \n",
    "                    # Construct output path\n",
    "                    output_dir = os.path.join(target_base, river_name)\n",
    "                    output_file = os.path.join(output_dir, f\"{river_name}.shp\")\n",
    "                    \n",
    "                    # Save as shapefile\n",
    "                    gdf_reprojected.to_file(output_file)\n",
    "                    \n",
    "                    success_count += 1\n",
    "                    print(f\"✓ Converted: {river_name} (from {planform})\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_count += 1\n",
    "                    error_msg = f\"{river_name} ({planform})\"\n",
    "                    errors.append(error_msg)\n",
    "                    print(f\"✗ Error: {river_name} ({planform}) - {str(e)}\")\n",
    "                    print(f\"   Skipping and continuing with remaining rivers...\")\n",
    "                \n",
    "                # Break after finding first matching planform (whether success or error)\n",
    "                break\n",
    "        \n",
    "        if not found_gpkg:\n",
    "            not_found_count += 1\n",
    "            print(f\"○ Not found: {river_name} (checked all planform types)\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"✓ Successfully converted: {success_count}\")\n",
    "    print(f\"○ GPKG files not found: {not_found_count}\")\n",
    "    print(f\"✗ Errors encountered: {error_count}\")\n",
    "    print(f\"Total river folders processed: {len(river_folders)}\")\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"FAILED RIVERS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for error in errors:\n",
    "            print(f\"  - {error}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting GPKG to Shapefile conversion...\\n\")\n",
    "    convert_gpkg_to_shapefile()\n",
    "    print(\"\\nConversion process complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefb7897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "target_base = r\"E:\\Dissertation\\Data\\RiverMapping\\Reaches\"\n",
    "\n",
    "print(f\"Does directory exist? {os.path.exists(target_base)}\")\n",
    "if os.path.exists(target_base):\n",
    "    folders = [f for f in os.listdir(target_base) if os.path.isdir(os.path.join(target_base, f))]\n",
    "    print(f\"Number of folders found: {len(folders)}\")\n",
    "    if folders:\n",
    "        print(f\"First few folders: {folders[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b81b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_folder_names_from_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Generate a list of folder names from a given directory.\n",
    "    \n",
    "    Parameters:\n",
    "    directory_path (str): Path to the directory containing folders\n",
    "    \n",
    "    Returns:\n",
    "    list: Sorted list of folder names\n",
    "    \"\"\"\n",
    "    # Get all items in the directory\n",
    "    all_items = os.listdir(directory_path)\n",
    "    \n",
    "    # Filter to only keep directories (not files)\n",
    "    folder_names = [item for item in all_items \n",
    "                   if os.path.isdir(os.path.join(directory_path, item))]\n",
    "    \n",
    "    # Sort alphabetically for consistency\n",
    "    folder_names.sort()\n",
    "    \n",
    "    return folder_names\n",
    "\n",
    "# Use it\n",
    "directory = r\"E:\\Dissertation\\Data\\Zhao_etal_2025\\B\"\n",
    "folder_names = get_folder_names_from_directory(directory)\n",
    "\n",
    "# Print as a Python list that you can copy-paste\n",
    "print(f\"Found {len(folder_names)} folders:\\n\")\n",
    "print(\"folder_names = [\")\n",
    "for i, name in enumerate(folder_names):\n",
    "    if i < len(folder_names) - 1:\n",
    "        print(f\"    '{name}',\")\n",
    "    else:\n",
    "        print(f\"    '{name}'\")\n",
    "print(\"]\")\n",
    "\n",
    "# Also print just the count and first few examples\n",
    "print(f\"\\nTotal: {len(folder_names)} folders\")\n",
    "if folder_names:\n",
    "    print(f\"First 5: {folder_names[:5]}\")\n",
    "    print(f\"Last 5: {folder_names[-5:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9808c430",
   "metadata": {},
   "source": [
    "## This folder creates folders with river names at the given directory\n",
    "It will optionally generate an additional subfolder, such as \"cleaned\" below the river_name folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5395c08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_unique_river_folders(base_directory, subfolder_name=None):\n",
    "    \"\"\"\n",
    "    Create folders for river monitoring stations, avoiding duplicates.\n",
    "    \n",
    "    Parameters:\n",
    "    base_directory (str): The path where folders should be created\n",
    "    subfolder_name (str, optional): If provided, creates a subfolder with this name\n",
    "                                    inside each river folder\n",
    "    \"\"\"\n",
    "    \n",
    "    # Raw list of folder names with duplicates\n",
    "    raw_folder_names = [\n",
    "        'Amazonas_Tamshiyacu', 'AmuDarya_Kerki', 'Amur_Khabarovsk',\n",
    "        'Amyl_Kachulka', 'Apalachicola_NearBlountstown', 'Araguaia_Aruana',\n",
    "        'Araguaia_LuizAlves', 'Araguaia_SaoFelixDoAraguaia', 'Beas_MandiPlain',\n",
    "        'Beni_Rurrenabaque', 'Benue_Ibi', 'Bhareli_NTRoadCrossing',\n",
    "        'Brahmaputra_Bahadurabad', 'Brahmaputra_Pandu', 'Brahmaputra_Yangcun',\n",
    "        'Chenab_Akhnoor', 'Cuiaba_PortoDoAlegre', 'Demini_PostoAjuricaba',\n",
    "        'Gandak_Dumariaghat', 'Gandak_Triveni', 'Ganges_Paksey',\n",
    "        'Guapore_Pimenteiras', 'HuangHe_Huayuankou', 'HuangHe_TanglaiQu',\n",
    "        'Ica_IpirangaVelho', 'Indus_Kotri', 'Irrawaddy_Katha',\n",
    "        'Irrawaddy_Sagaing', 'Itacuai_LadarioJusante', 'Jurua_EirunepeMontante',\n",
    "        'Jurua_SantosDumont', 'Katun_Srostki', 'Kokcha_Khojaghar',\n",
    "        'Krishna_Vijayawada', 'Lena_Tabaga', 'Liard_UpperCrossing',\n",
    "        'Logone_Bongor', 'Logone_Lai', 'Madeira_Humaita',\n",
    "        'Mamore_Guajara-Mirim', 'Mamore_PuertoSiles', 'Maranon_Borja',\n",
    "        'Maranon_SanRegis', 'Mortes_SantoAntonioDoLeverger', 'Napo_Bellavista',\n",
    "        'Napo_NvoRocafuerte', 'Naryn_UstKekirim', 'Ob_Prokhorkino',\n",
    "        'Panj_NizPyandzh', 'Parana_Corrientes', 'Peace_FifthMeridian',\n",
    "        'Pilcomayo_VillaMontes', 'Purus_Canutama', 'Purus_Labrea',\n",
    "        'Purus_SeringalDaCaridade', 'Purus_ValparaisoMontante', 'Red_Index',\n",
    "        'Rufiji_Stigler', 'Salinas_SanAugustin', 'SaoFrancisco_BomJesusDaLapa',\n",
    "        'SaoFrancisco_Morpara', 'SaptKosi_Baltara', 'SaptKosi_Chatara-Kothu',\n",
    "        'Selenga_Naushki', 'Solimoes_SantoAntonioDoIca', 'Solimoes_SaoPauloDeOlivenca',\n",
    "        'Solimoes_Tabatinga', 'Taku_NearTulsequa', 'Tanana_NearHardingLake',\n",
    "        'Tarauaca_Envira', 'Tista_AndersonBr', 'Tista_Kaunia',\n",
    "        'Tisza_Vylok', 'Trinity_Romayor', 'Ucayali_Atalaya',\n",
    "        'Ucayali_Pucallpa', 'Ucayali_Requena', 'Ural_Kushum',\n",
    "        'White_DevallsBluff', 'White_Petersburg', 'Yukon_NearStevensVillage',\n",
    "        'Zambezi_Sesheke', 'Beni', 'Beni', 'Beni', 'Beni', 'Beni',\n",
    "        'Beni', 'Beni', 'Beni', 'Beni', 'Beni', 'Bermejo', 'Bermejo',\n",
    "        'Bermejo', 'Bermejo', 'Bermejo', 'Bermejo', 'Bermejo', 'Bermejo',\n",
    "        'Bermejo', 'Bermejo', 'Bermejo', 'Bermejo', 'Bermejo', 'Bermejo',\n",
    "        'Bermejo', 'Bermejo', 'Bermejo', 'Bermejo', 'Bermejo', 'Bermejo',\n",
    "        'Koyukuk_Huslia', 'Yukon_Beaver', 'Aladan_VerkhoyanskiyPerevoz',\n",
    "        'Amazonas_Jatuarana', 'Amur_Komsomolsk', 'Benue_Umaisha',\n",
    "        'BolshayaKet_Rodyonovka', 'Brahmaputra_Pasighat', 'Chari_Bousso',\n",
    "        'Chari_Guelengdeng', 'Chari_Ndjamena', 'Chari_Sahr', 'Fraser_Hope',\n",
    "        'Gandak_Devghat', 'Helmand_Kajaki', 'Helmand_Malakhan', 'Indus_Attock',\n",
    "        'Irtysh_Bobrovsky', 'Irtysh_Hanti-Mansisk', 'Irtysh_Pavlodar',\n",
    "        'Irtysh_Semiyarskoje', 'Jutai_PortoSeguro', 'Kamchatka_Kozyrevsk',\n",
    "        'Kan_Kansk', 'MadreDeDios_CachuelaEsperanza', 'Magdalena_Calamar',\n",
    "        'Magdalena_PuertoBerrio', 'Manas_Mathanguri', 'Mbam_Goura',\n",
    "        'Mekong_Kratie', 'Niger_Tossaye', 'Ob_Barnaul', 'Ob_Kolpashevo',\n",
    "        'Ob_Mogochin', 'Ob_Phominskoje', 'Orinoco_CiudadBolivar',\n",
    "        'Orinoco_Musinacio', 'Paraguay_Asuncion', 'Paraguay_PortoMurtinho',\n",
    "        'Parana_Chapeton', 'Porcupine_NearFortYukon', 'Sangha_Ouesso',\n",
    "        'Solimoes_Itapeua', 'Solimoes_Manacapuru', 'SonghuaJiang_Haerbin',\n",
    "        'Vilyuy_KhatyrykKhoma', 'Yangtze_Datong', 'Yellowstone_NearSidney',\n",
    "        'Yukon_Eagle', 'Zambezi_LukuluMission', 'Zambezi_Matundo-Cais'\n",
    "    ]\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    folder_names = []\n",
    "    duplicates_removed = []\n",
    "    \n",
    "    for name in raw_folder_names:\n",
    "        if name not in seen:\n",
    "            seen.add(name)\n",
    "            folder_names.append(name)\n",
    "        else:\n",
    "            duplicates_removed.append(name)\n",
    "    \n",
    "    # Report on duplicates\n",
    "    if duplicates_removed:\n",
    "        from collections import Counter\n",
    "        duplicate_counts = Counter(raw_folder_names)\n",
    "        print(\"Duplicate rivers found and removed:\")\n",
    "        for name, count in duplicate_counts.items():\n",
    "            if count > 1:\n",
    "                print(f\"  - {name}: appeared {count} times, keeping 1\")\n",
    "        print()\n",
    "    \n",
    "    # Create the base directory if it doesn't exist\n",
    "    if not os.path.exists(base_directory):\n",
    "        os.makedirs(base_directory)\n",
    "        print(f\"Created base directory: {base_directory}\\n\")\n",
    "    \n",
    "    # Create each folder\n",
    "    created_count = 0\n",
    "    skipped_count = 0\n",
    "    subfolders_created = 0\n",
    "    subfolders_skipped = 0\n",
    "    \n",
    "    for folder_name in folder_names:\n",
    "        folder_path = os.path.join(base_directory, folder_name)\n",
    "        \n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "            created_count += 1\n",
    "            print(f\"Created: {folder_name}\")\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "            print(f\"Already exists: {folder_name}\")\n",
    "        \n",
    "        # Create subfolder if specified\n",
    "        if subfolder_name:\n",
    "            subfolder_path = os.path.join(folder_path, subfolder_name)\n",
    "            if not os.path.exists(subfolder_path):\n",
    "                os.makedirs(subfolder_path)\n",
    "                subfolders_created += 1\n",
    "            else:\n",
    "                subfolders_skipped += 1\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Original list: {len(raw_folder_names)} entries\")\n",
    "    print(f\"Duplicates removed: {len(duplicates_removed)}\")\n",
    "    print(f\"Unique folders: {len(folder_names)}\")\n",
    "    print(f\"✓ Folders created: {created_count}\")\n",
    "    print(f\"○ Already existed: {skipped_count}\")\n",
    "    \n",
    "    if subfolder_name:\n",
    "        print(f\"\\nSubfolder '{subfolder_name}':\")\n",
    "        print(f\"✓ Created: {subfolders_created}\")\n",
    "        print(f\"○ Already existed: {subfolders_skipped}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify your target directory here\n",
    "    target_directory = r\"E:\\Dissertation\\Data\\RiverMapping\\RiverMasks\"  # Change this to your desired path\n",
    "    \n",
    "    # Optional: specify a subfolder name to create inside each river folder\n",
    "    # Set to None if you don't want subfolders\n",
    "    subfolder = \"Cleaned\"  # Example: \"masks\" or \"data\" or None\n",
    "    \n",
    "    create_unique_river_folders(target_directory, subfolder_name=subfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baa5445",
   "metadata": {},
   "source": [
    "This moves the Greenberg et al 2024 masks from his folder structure to the new folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3ac0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def move_greenberg_masks():\n",
    "    \"\"\"\n",
    "    Move .tif mask files from Greenberg et al. directory structure to RiverMapping directory.\n",
    "    Searches through all planform types and river names, finds mask folders, and moves\n",
    "    all .tif files to the new directory structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define paths\n",
    "    source_base = r\"E:\\Dissertation\\Data\\Greenberg_etal_2024\\RiverData\"\n",
    "    target_base = r\"E:\\Dissertation\\Data\\RiverMapping\\RiverMasks\"\n",
    "    \n",
    "    # Define planform types\n",
    "    planform_types = [\"B\", \"HSW\", \"LSW\", \"Me\"]\n",
    "    \n",
    "    # Counters for summary\n",
    "    rivers_processed = 0\n",
    "    files_moved = 0\n",
    "    rivers_not_found = 0\n",
    "    rivers_with_multiple_folders = 0\n",
    "    errors = []\n",
    "    rivers_with_masks = []\n",
    "    rivers_without_masks = []\n",
    "    rivers_with_ambiguous_structure = []\n",
    "    \n",
    "    # Get list of all potential river names by scanning all planform directories\n",
    "    all_river_names = set()\n",
    "    for planform in planform_types:\n",
    "        planform_dir = os.path.join(source_base, planform)\n",
    "        if os.path.exists(planform_dir):\n",
    "            river_dirs = [d for d in os.listdir(planform_dir) \n",
    "                         if os.path.isdir(os.path.join(planform_dir, d))]\n",
    "            all_river_names.update(river_dirs)\n",
    "    \n",
    "    all_river_names = sorted(all_river_names)\n",
    "    print(f\"Found {len(all_river_names)} unique river names across all planform types\")\n",
    "    print(f\"Searching through {len(planform_types)} planform types\\n\")\n",
    "    \n",
    "    # Process each river\n",
    "    for river_name in all_river_names:\n",
    "        found_masks = False\n",
    "        \n",
    "        # Search through planform types for this river\n",
    "        for planform in planform_types:\n",
    "            # Construct source path to river directory\n",
    "            river_dir = os.path.join(source_base, planform, river_name)\n",
    "            \n",
    "            if not os.path.exists(river_dir):\n",
    "                continue\n",
    "            \n",
    "            # Check what's inside the river directory\n",
    "            items_in_river_dir = [d for d in os.listdir(river_dir) \n",
    "                                 if os.path.isdir(os.path.join(river_dir, d))]\n",
    "            \n",
    "            # Look for potential search locations\n",
    "            waterlevel_folders = [f for f in items_in_river_dir if f.startswith('WaterLevel')]\n",
    "            has_mask_direct = 'mask' in items_in_river_dir\n",
    "            has_river_subfolder = river_name in items_in_river_dir\n",
    "            \n",
    "            # Determine the correct path to mask folder\n",
    "            mask_dir = None\n",
    "            all_waterlevel_folders = []\n",
    "            \n",
    "            # Collect all WaterLevel folders from both levels\n",
    "            if waterlevel_folders:\n",
    "                all_waterlevel_folders.extend(waterlevel_folders)\n",
    "            \n",
    "            # Also check inside river_name subfolder if it exists\n",
    "            if has_river_subfolder:\n",
    "                river_subfolder = os.path.join(river_dir, river_name)\n",
    "                subfolders = [d for d in os.listdir(river_subfolder) \n",
    "                             if os.path.isdir(os.path.join(river_subfolder, d))]\n",
    "                subfolder_waterlevels = [f for f in subfolders if f.startswith('WaterLevel')]\n",
    "                all_waterlevel_folders.extend(subfolder_waterlevels)\n",
    "            \n",
    "            # Case 1: Multiple WaterLevel folders across both levels - ambiguous\n",
    "            if len(all_waterlevel_folders) > 1:\n",
    "                found_masks = True  # Mark as found to avoid \"not found\" message\n",
    "                rivers_with_ambiguous_structure.append(river_name)\n",
    "                rivers_with_multiple_folders += 1\n",
    "                print(f\"⚠ Warning: {river_name} ({planform}) has multiple WaterLevel folders: {all_waterlevel_folders}\")\n",
    "                print(f\"   Skipping due to ambiguous structure...\")\n",
    "                break\n",
    "            \n",
    "            # Case 2: WaterLevel folder directly in river_dir\n",
    "            elif len(waterlevel_folders) == 1:\n",
    "                potential_mask_dir = os.path.join(river_dir, waterlevel_folders[0], \"mask\")\n",
    "                if os.path.exists(potential_mask_dir) and os.path.isdir(potential_mask_dir):\n",
    "                    mask_dir = potential_mask_dir\n",
    "            \n",
    "            # Case 3: WaterLevel folder inside river_name subfolder\n",
    "            elif has_river_subfolder:\n",
    "                river_subfolder = os.path.join(river_dir, river_name)\n",
    "                subfolders = [d for d in os.listdir(river_subfolder) \n",
    "                             if os.path.isdir(os.path.join(river_subfolder, d))]\n",
    "                subfolder_waterlevels = [f for f in subfolders if f.startswith('WaterLevel')]\n",
    "                \n",
    "                if len(subfolder_waterlevels) == 1:\n",
    "                    potential_mask_dir = os.path.join(river_subfolder, subfolder_waterlevels[0], \"mask\")\n",
    "                    if os.path.exists(potential_mask_dir) and os.path.isdir(potential_mask_dir):\n",
    "                        mask_dir = potential_mask_dir\n",
    "                elif 'mask' in subfolders:\n",
    "                    # No WaterLevel, but mask exists directly in river_name subfolder\n",
    "                    mask_dir = os.path.join(river_subfolder, \"mask\")\n",
    "            \n",
    "            # Case 4: mask folder directly in river_dir (no river_name subfolder, no WaterLevel)\n",
    "            elif has_mask_direct:\n",
    "                mask_dir = os.path.join(river_dir, \"mask\")\n",
    "            \n",
    "            # If we found a mask directory, process it\n",
    "            if mask_dir:\n",
    "                # Get all .tif files in the mask directory\n",
    "                tif_files = [f for f in os.listdir(mask_dir) if f.lower().endswith('.tif')]\n",
    "                \n",
    "                if tif_files:\n",
    "                    found_masks = True\n",
    "                    \n",
    "                    try:\n",
    "                        # Create target directory\n",
    "                        target_dir = os.path.join(target_base, river_name, \"Cleaned\")\n",
    "                        os.makedirs(target_dir, exist_ok=True)\n",
    "                        \n",
    "                        # Move each .tif file\n",
    "                        files_moved_for_river = 0\n",
    "                        for tif_file in tif_files:\n",
    "                            source_file = os.path.join(mask_dir, tif_file)\n",
    "                            target_file = os.path.join(target_dir, tif_file)\n",
    "                            \n",
    "                            shutil.move(source_file, target_file)\n",
    "                            files_moved_for_river += 1\n",
    "                        \n",
    "                        files_moved += files_moved_for_river\n",
    "                        rivers_processed += 1\n",
    "                        rivers_with_masks.append(river_name)\n",
    "                        print(f\"✓ Moved {files_moved_for_river} file(s): {river_name} (from {planform})\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        error_msg = f\"{river_name} ({planform})\"\n",
    "                        errors.append(error_msg)\n",
    "                        print(f\"✗ Error: {river_name} ({planform}) - {str(e)}\")\n",
    "                        print(f\"   Skipping and continuing with remaining rivers...\")\n",
    "                    \n",
    "                    # Break after finding first matching planform with masks\n",
    "                    break\n",
    "        \n",
    "        if not found_masks:\n",
    "            rivers_without_masks.append(river_name)\n",
    "            rivers_not_found += 1\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"✓ Rivers with masks processed: {rivers_processed}\")\n",
    "    print(f\"✓ Total .tif files moved: {files_moved}\")\n",
    "    print(f\"○ Rivers without mask folders: {rivers_not_found}\")\n",
    "    print(f\"⚠ Rivers with ambiguous structure (skipped): {rivers_with_multiple_folders}\")\n",
    "    print(f\"✗ Errors encountered: {len(errors)}\")\n",
    "    print(f\"Total unique rivers scanned: {len(all_river_names)}\")\n",
    "    \n",
    "    if rivers_with_ambiguous_structure:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"RIVERS WITH AMBIGUOUS STRUCTURE (Multiple WaterLevel Folders)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for river in rivers_with_ambiguous_structure:\n",
    "            print(f\"  - {river}\")\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"FAILED RIVERS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for error in errors:\n",
    "            print(f\"  - {error}\")\n",
    "    \n",
    "    # Optional: Print rivers without masks (useful for debugging)\n",
    "    if rivers_without_masks and len(rivers_without_masks) <= 20:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"RIVERS WITHOUT MASK FOLDERS (showing up to 20)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for river in rivers_without_masks[:20]:\n",
    "            print(f\"  - {river}\")\n",
    "        if len(rivers_without_masks) > 20:\n",
    "            print(f\"  ... and {len(rivers_without_masks) - 20} more\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting mask file migration...\\n\")\n",
    "    move_greenberg_masks()\n",
    "    print(\"\\nMask migration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16394dc0",
   "metadata": {},
   "source": [
    "This moves the Zhao et al 2025 masks from his folder structure to the new folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47af1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_zhao_masks():\n",
    "    \"\"\"\n",
    "    Move .tif mask files from Zhao et al. directory structure to RiverMapping directory.\n",
    "    Searches through all planform types and river names, finds PreparedImagery_annual folders,\n",
    "    and moves all .tif files to the new directory structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define paths\n",
    "    source_base = r\"E:\\Dissertation\\Data\\Zhao_etal_2025\"\n",
    "    target_base = r\"E:\\Dissertation\\Data\\RiverMapping\\RiverMasks\"\n",
    "    \n",
    "    # Define planform types\n",
    "    planform_types = [\"B\", \"HSW\", \"LSW\", \"Me\"]\n",
    "    \n",
    "    # Counters for summary\n",
    "    rivers_processed = 0\n",
    "    files_moved = 0\n",
    "    rivers_not_found = 0\n",
    "    errors = []\n",
    "    rivers_with_masks = []\n",
    "    rivers_without_masks = []\n",
    "    \n",
    "    # Get list of all potential river names by scanning all planform directories\n",
    "    all_river_names = set()\n",
    "    for planform in planform_types:\n",
    "        planform_dir = os.path.join(source_base, planform)\n",
    "        if os.path.exists(planform_dir):\n",
    "            river_dirs = [d for d in os.listdir(planform_dir) \n",
    "                         if os.path.isdir(os.path.join(planform_dir, d))]\n",
    "            all_river_names.update(river_dirs)\n",
    "    \n",
    "    all_river_names = sorted(all_river_names)\n",
    "    print(f\"Found {len(all_river_names)} unique river names across all planform types\")\n",
    "    print(f\"Searching through {len(planform_types)} planform types\\n\")\n",
    "    \n",
    "    # Process each river\n",
    "    for river_name in all_river_names:\n",
    "        found_masks = False\n",
    "        \n",
    "        # Search through planform types for this river\n",
    "        for planform in planform_types:\n",
    "            # Construct source path: planform/river_name/PreparedImagery_annual/river_name\n",
    "            river_dir = os.path.join(source_base, planform, river_name)\n",
    "            imagery_dir = os.path.join(river_dir, \"PreparedImagery_annual\")\n",
    "            mask_dir = os.path.join(imagery_dir, river_name)\n",
    "            \n",
    "            # Check if mask directory exists\n",
    "            if os.path.exists(mask_dir) and os.path.isdir(mask_dir):\n",
    "                # Get all .tif files in the directory\n",
    "                tif_files = [f for f in os.listdir(mask_dir) if f.lower().endswith('.tif')]\n",
    "                \n",
    "                if tif_files:\n",
    "                    found_masks = True\n",
    "                    \n",
    "                    try:\n",
    "                        # Create target directory\n",
    "                        target_dir = os.path.join(target_base, river_name, \"Cleaned\")\n",
    "                        os.makedirs(target_dir, exist_ok=True)\n",
    "                        \n",
    "                        # Move each .tif file\n",
    "                        files_moved_for_river = 0\n",
    "                        for tif_file in tif_files:\n",
    "                            source_file = os.path.join(mask_dir, tif_file)\n",
    "                            target_file = os.path.join(target_dir, tif_file)\n",
    "                            \n",
    "                            shutil.move(source_file, target_file)\n",
    "                            files_moved_for_river += 1\n",
    "                        \n",
    "                        files_moved += files_moved_for_river\n",
    "                        rivers_processed += 1\n",
    "                        rivers_with_masks.append(river_name)\n",
    "                        print(f\"✓ Moved {files_moved_for_river} file(s): {river_name} (from {planform})\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        error_msg = f\"{river_name} ({planform})\"\n",
    "                        errors.append(error_msg)\n",
    "                        print(f\"✗ Error: {river_name} ({planform}) - {str(e)}\")\n",
    "                        print(f\"   Skipping and continuing with remaining rivers...\")\n",
    "                    \n",
    "                    # Break after finding first matching planform with masks\n",
    "                    break\n",
    "        \n",
    "        if not found_masks:\n",
    "            rivers_without_masks.append(river_name)\n",
    "            rivers_not_found += 1\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"✓ Rivers with masks processed: {rivers_processed}\")\n",
    "    print(f\"✓ Total .tif files moved: {files_moved}\")\n",
    "    print(f\"○ Rivers without mask folders: {rivers_not_found}\")\n",
    "    print(f\"✗ Errors encountered: {len(errors)}\")\n",
    "    print(f\"Total unique rivers scanned: {len(all_river_names)}\")\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"FAILED RIVERS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for error in errors:\n",
    "            print(f\"  - {error}\")\n",
    "    \n",
    "    # Optional: Print rivers without masks (useful for debugging)\n",
    "    if rivers_without_masks and len(rivers_without_masks) <= 20:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"RIVERS WITHOUT MASK FOLDERS (showing up to 20)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for river in rivers_without_masks[:20]:\n",
    "            print(f\"  - {river}\")\n",
    "        if len(rivers_without_masks) > 20:\n",
    "            print(f\"  ... and {len(rivers_without_masks) - 20} more\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Greenberg mask file migration...\\n\")\n",
    "    move_greenberg_masks()\n",
    "    print(\"\\nGreenberg mask migration complete!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\\nStarting Zhao mask file migration...\\n\")\n",
    "    move_zhao_masks()\n",
    "    print(\"\\nZhao mask migration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b732b31",
   "metadata": {},
   "source": [
    "Preliminary QC of river masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c61ad6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 135 river directories\n",
      "\n",
      "[1/135] Processing Aladan_VerkhoyanskiyPerevoz... Analyzing... Flagging... Saving CSV... ✓ 20 masks, 0 flagged\n",
      "[2/135] Processing Amazonas_Jatuarana... Analyzing... Flagging... Saving CSV... ✓ 38 masks, 0 flagged\n",
      "[3/135] Processing Amazonas_Tamshiyacu... Analyzing... Flagging... Saving CSV... ✓ 31 masks, 1 flagged\n",
      "[4/135] Processing AmuDarya_Kerki... Analyzing... Flagging... Saving CSV... ✓ 29 masks, 0 flagged\n",
      "[5/135] Processing Amur_Khabarovsk... Analyzing... Flagging... Saving CSV... ✓ 28 masks, 0 flagged\n",
      "[6/135] Processing Amur_Komsomolsk... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 2 flagged\n",
      "[7/135] Processing Amyl_Kachulka... Analyzing... Flagging... Saving CSV... ✓ 24 masks, 1 flagged\n",
      "[8/135] Processing Apalachicola_NearBlountstown... Analyzing... Flagging... Saving CSV... ✓ 37 masks, 1 flagged\n",
      "[9/135] Processing Araguaia_Aruana... Analyzing... Flagging... Saving CSV... ✓ 36 masks, 0 flagged\n",
      "[10/135] Processing Araguaia_LuizAlves... Analyzing... Flagging... Saving CSV... ✓ 36 masks, 0 flagged\n",
      "[11/135] Processing Araguaia_SaoFelixDoAraguaia... Analyzing... Flagging... Saving CSV... ✓ 37 masks, 1 flagged\n",
      "[12/135] Processing Beas_MandiPlain... Analyzing... Flagging... Saving CSV... ✓ 29 masks, 1 flagged\n",
      "[13/135] Processing Beni... Analyzing... ⚠ No .tif files found, skipping\n",
      "[14/135] Processing Beni_Rurrenabaque... Analyzing... Flagging... Saving CSV... ✓ 36 masks, 1 flagged\n",
      "[15/135] Processing Benue_Ibi... Analyzing... Flagging... Saving CSV... ✓ 12 masks, 0 flagged\n",
      "[16/135] Processing Benue_Umaisha... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 20 flagged\n",
      "[17/135] Processing Bermejo... Analyzing... ⚠ No .tif files found, skipping\n",
      "[18/135] Processing Bhareli_NTRoadCrossing... Analyzing... Flagging... Saving CSV... ✓ 31 masks, 3 flagged\n",
      "[19/135] Processing BolshayaKet_Rodyonovka... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 17 flagged\n",
      "[20/135] Processing Brahmaputra_Bahadurabad... Analyzing... Flagging... Saving CSV... ✓ 34 masks, 1 flagged\n",
      "[21/135] Processing Brahmaputra_Pandu... Analyzing... Flagging... Saving CSV... ✓ 32 masks, 0 flagged\n",
      "[22/135] Processing Brahmaputra_Pasighat... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 2 flagged\n",
      "[23/135] Processing Brahmaputra_Yangcun... Analyzing... Flagging... Saving CSV... ✓ 33 masks, 0 flagged\n",
      "[24/135] Processing Chari_Bousso... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 19 flagged\n",
      "[25/135] Processing Chari_Guelengdeng... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 21 flagged\n",
      "[26/135] Processing Chari_Ndjamena... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 21 flagged\n",
      "[27/135] Processing Chari_Sahr... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 20 flagged\n",
      "[28/135] Processing Chenab_Akhnoor... Analyzing... Flagging... Saving CSV... ✓ 20 masks, 1 flagged\n",
      "[29/135] Processing Cuiaba_PortoDoAlegre... Analyzing... Flagging... Saving CSV... ✓ 36 masks, 0 flagged\n",
      "[30/135] Processing Demini_PostoAjuricaba... Analyzing... Flagging... Saving CSV... ✓ 35 masks, 1 flagged\n",
      "[31/135] Processing Fraser_Hope... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 1 flagged\n",
      "[32/135] Processing Gandak_Devghat... Analyzing... Flagging... Saving CSV... ✓ 35 masks, 0 flagged\n",
      "[33/135] Processing Gandak_Dumariaghat... Analyzing... Flagging... Saving CSV... ✓ 33 masks, 0 flagged\n",
      "[34/135] Processing Gandak_Triveni... Analyzing... Flagging... Saving CSV... ✓ 25 masks, 0 flagged\n",
      "[35/135] Processing Ganges_Paksey... Analyzing... Flagging... Saving CSV... ✓ 37 masks, 3 flagged\n",
      "[36/135] Processing Guapore_Pimenteiras... Analyzing... Flagging... Saving CSV... ✓ 37 masks, 1 flagged\n",
      "[37/135] Processing Helmand_Kajaki... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 5 flagged\n",
      "[38/135] Processing Helmand_Malakhan... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 9 flagged\n",
      "[39/135] Processing HuangHe_Huayuankou... Analyzing... Flagging... Saving CSV... ✓ 25 masks, 1 flagged\n",
      "[40/135] Processing HuangHe_TanglaiQu... Analyzing... Flagging... Saving CSV... ✓ 36 masks, 1 flagged\n",
      "[41/135] Processing Ica_IpirangaVelho... Analyzing... Flagging... Saving CSV... ✓ 25 masks, 0 flagged\n",
      "[42/135] Processing Indus_Attock... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 6 flagged\n",
      "[43/135] Processing Indus_Kotri... Analyzing... Flagging... Saving CSV... ✓ 20 masks, 0 flagged\n",
      "[44/135] Processing Irrawaddy_Katha... Analyzing... Flagging... Saving CSV... ✓ 37 masks, 2 flagged\n",
      "[45/135] Processing Irrawaddy_Sagaing... Analyzing... Flagging... Saving CSV... ✓ 33 masks, 0 flagged\n",
      "[46/135] Processing Irtysh_Bobrovsky... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 7 flagged\n",
      "[47/135] Processing Irtysh_Hanti-Mansisk... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 5 flagged\n",
      "[48/135] Processing Irtysh_Pavlodar... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 8 flagged\n",
      "[49/135] Processing Irtysh_Semiyarskoje... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 8 flagged\n",
      "[50/135] Processing Itacuai_LadarioJusante... Analyzing... Flagging... Saving CSV... ✓ 29 masks, 0 flagged\n",
      "[51/135] Processing Jurua_EirunepeMontante... Analyzing... Flagging... Saving CSV... ✓ 35 masks, 0 flagged\n",
      "[52/135] Processing Jurua_SantosDumont... Analyzing... Flagging... Saving CSV... ✓ 36 masks, 1 flagged\n",
      "[53/135] Processing Jutai_PortoSeguro... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 1 flagged\n",
      "[54/135] Processing Kamchatka_Kozyrevsk... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 19 flagged\n",
      "[55/135] Processing Kan_Kansk... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 8 flagged\n",
      "[56/135] Processing Katun_Srostki... Analyzing... Flagging... Saving CSV... ✓ 15 masks, 0 flagged\n",
      "[57/135] Processing Kokcha_Khojaghar... Analyzing... Flagging... Saving CSV... ✓ 22 masks, 0 flagged\n",
      "[58/135] Processing Koyukuk_Huslia... Analyzing... ⚠ No .tif files found, skipping\n",
      "[59/135] Processing Krishna_Vijayawada... Analyzing... Flagging... Saving CSV... ✓ 15 masks, 0 flagged\n",
      "[60/135] Processing Lena_Tabaga... Analyzing... Flagging... Saving CSV... ✓ 18 masks, 0 flagged\n",
      "[61/135] Processing Liard_UpperCrossing... Analyzing... Flagging... Saving CSV... ✓ 28 masks, 0 flagged\n",
      "[62/135] Processing Logone_Bongor... Analyzing... Flagging... Saving CSV... ✓ 37 masks, 21 flagged\n",
      "[63/135] Processing Logone_Lai... Analyzing... Flagging... Saving CSV... ✓ 16 masks, 0 flagged\n",
      "[64/135] Processing Madeira_Humaita... Analyzing... Flagging... Saving CSV... ✓ 36 masks, 0 flagged\n",
      "[65/135] Processing MadreDeDios_CachuelaEsperanza... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 1 flagged\n",
      "[66/135] Processing Magdalena_Calamar... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 6 flagged\n",
      "[67/135] Processing Magdalena_PuertoBerrio... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 8 flagged\n",
      "[68/135] Processing Mamore_Guajara-Mirim... Analyzing... Flagging... Saving CSV... ✓ 37 masks, 1 flagged\n",
      "[69/135] Processing Mamore_PuertoSiles... Analyzing... Flagging... Saving CSV... ✓ 37 masks, 1 flagged\n",
      "[70/135] Processing Manas_Mathanguri... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 6 flagged\n",
      "[71/135] Processing Maranon_Borja... Analyzing... Flagging... Saving CSV... ✓ 28 masks, 0 flagged\n",
      "[72/135] Processing Maranon_SanRegis... Analyzing... Flagging... Saving CSV... ✓ 33 masks, 1 flagged\n",
      "[73/135] Processing Mbam_Goura... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 20 flagged\n",
      "[74/135] Processing Mekong_Kratie... Analyzing... Flagging... Saving CSV... ✓ 36 masks, 1 flagged\n",
      "[75/135] Processing Mortes_SantoAntonioDoLeverger... Analyzing... Flagging... Saving CSV... ✓ 37 masks, 1 flagged\n",
      "[76/135] Processing Napo_Bellavista... Analyzing... Flagging... Saving CSV... ✓ 30 masks, 0 flagged\n",
      "[77/135] Processing Napo_NvoRocafuerte... Analyzing... Flagging... Saving CSV... ✓ 24 masks, 0 flagged\n",
      "[78/135] Processing Naryn_UstKekirim... Analyzing... Flagging... Saving CSV... ✓ 19 masks, 0 flagged\n",
      "[79/135] Processing Niger_Tossaye... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 10 flagged\n",
      "[80/135] Processing Ob_Barnaul... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 8 flagged\n",
      "[81/135] Processing Ob_Kolpashevo... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 16 flagged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82/135] Processing Ob_Mogochin... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 15 flagged\n",
      "[83/135] Processing Ob_Phominskoje... Analyzing... Flagging... Saving CSV... ✓ 32 masks, 1 flagged\n",
      "[84/135] Processing Ob_Prokhorkino... Analyzing... Flagging... Saving CSV... ✓ 18 masks, 0 flagged\n",
      "[85/135] Processing Orinoco_CiudadBolivar... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 12 flagged\n",
      "[86/135] Processing Orinoco_Musinacio... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 5 flagged\n",
      "[87/135] Processing Panj_NizPyandzh... Analyzing... Flagging... Saving CSV... ✓ 31 masks, 1 flagged\n",
      "[88/135] Processing Paraguay_Asuncion... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 2 flagged\n",
      "[89/135] Processing Paraguay_PortoMurtinho... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 2 flagged\n",
      "[90/135] Processing Parana_Chapeton... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 1 flagged\n",
      "[91/135] Processing Parana_Corrientes... Analyzing... Flagging... Saving CSV... ✓ 29 masks, 0 flagged\n",
      "[92/135] Processing Peace_FifthMeridian... Analyzing... Flagging... Saving CSV... ✓ 37 masks, 1 flagged\n",
      "[93/135] Processing Pilcomayo_VillaMontes... Analyzing... Flagging... Saving CSV... ✓ 36 masks, 2 flagged\n",
      "[94/135] Processing Porcupine_NearFortYukon... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 5 flagged\n",
      "[95/135] Processing Purus_Canutama... Analyzing... Flagging... Saving CSV... ✓ 37 masks, 1 flagged\n",
      "[96/135] Processing Purus_Labrea... Analyzing... Flagging... Saving CSV... ✓ 37 masks, 1 flagged\n",
      "[97/135] Processing Purus_SeringalDaCaridade... Analyzing... Flagging... Saving CSV... ✓ 37 masks, 1 flagged\n",
      "[98/135] Processing Purus_ValparaisoMontante... Analyzing... Flagging... Saving CSV... ✓ 36 masks, 0 flagged\n",
      "[99/135] Processing Red_Index... Analyzing... Flagging... Saving CSV... ✓ 37 masks, 1 flagged\n",
      "[100/135] Processing Rufiji_Stigler... Analyzing... Flagging... Saving CSV... ✓ 31 masks, 1 flagged\n",
      "[101/135] Processing Salinas_SanAugustin... Analyzing... Flagging... Saving CSV... ✓ 23 masks, 0 flagged\n",
      "[102/135] Processing Sangha_Ouesso... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 18 flagged\n",
      "[103/135] Processing SaoFrancisco_BomJesusDaLapa... Analyzing... Flagging... Saving CSV... ✓ 36 masks, 0 flagged\n",
      "[104/135] Processing SaoFrancisco_Morpara... Analyzing... Flagging... Saving CSV... ✓ 37 masks, 1 flagged\n",
      "[105/135] Processing SaptKosi_Baltara... Analyzing... Flagging... Saving CSV... ✓ 32 masks, 0 flagged\n",
      "[106/135] Processing SaptKosi_Chatara-Kothu... Analyzing... ⚠ No .tif files found, skipping\n",
      "[107/135] Processing Selenga_Naushki... Analyzing... Flagging... Saving CSV... ✓ 30 masks, 1 flagged\n",
      "[108/135] Processing Solimoes_Itapeua... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 1 flagged\n",
      "[109/135] Processing Solimoes_Manacapuru... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 1 flagged\n",
      "[110/135] Processing Solimoes_SantoAntonioDoIca... Analyzing... Flagging... Saving CSV... ✓ 34 masks, 1 flagged\n",
      "[111/135] Processing Solimoes_SaoPauloDeOlivenca... Analyzing... Flagging... Saving CSV... ✓ 32 masks, 0 flagged\n",
      "[112/135] Processing Solimoes_Tabatinga... Analyzing... Flagging... Saving CSV... ✓ 26 masks, 0 flagged\n",
      "[113/135] Processing SonghuaJiang_Haerbin... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 2 flagged\n",
      "[114/135] Processing Taku_NearTulsequa... Analyzing... Flagging... Saving CSV... ✓ 27 masks, 0 flagged\n",
      "[115/135] Processing Tanana_NearHardingLake... Analyzing... Flagging... Saving CSV... ✓ 21 masks, 0 flagged\n",
      "[116/135] Processing Tarauaca_Envira... Analyzing... Flagging... Saving CSV... ✓ 33 masks, 0 flagged\n",
      "[117/135] Processing Tista_AndersonBr... Analyzing... Flagging... Saving CSV... ✓ 14 masks, 0 flagged\n",
      "[118/135] Processing Tista_Kaunia... Analyzing... Flagging... Saving CSV... ✓ 18 masks, 0 flagged\n",
      "[119/135] Processing Tisza_Vylok... Analyzing... Flagging... Saving CSV... ✓ 35 masks, 0 flagged\n",
      "[120/135] Processing Trinity_Romayor... Analyzing... Flagging... Saving CSV... ✓ 36 masks, 0 flagged\n",
      "[121/135] Processing Ucayali_Atalaya... Analyzing... Flagging... Saving CSV... ✓ 36 masks, 0 flagged\n",
      "[122/135] Processing Ucayali_Pucallpa... Analyzing... Flagging... Saving CSV... ✓ 36 masks, 1 flagged\n",
      "[123/135] Processing Ucayali_Requena... Analyzing... Flagging... Saving CSV... ✓ 35 masks, 0 flagged\n",
      "[124/135] Processing Ural_Kushum... Analyzing... Flagging... Saving CSV... ✓ 30 masks, 0 flagged\n",
      "[125/135] Processing Vilyuy_KhatyrykKhoma... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 18 flagged\n",
      "[126/135] Processing White_DevallsBluff... Analyzing... Flagging... Saving CSV... ✓ 37 masks, 1 flagged\n",
      "[127/135] Processing White_Petersburg... Analyzing... Flagging... Saving CSV... ✓ 37 masks, 1 flagged\n",
      "[128/135] Processing Yangtze_Datong... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 1 flagged\n",
      "[129/135] Processing Yellowstone_NearSidney... Analyzing... Flagging... Saving CSV... ✓ 38 masks, 0 flagged\n",
      "[130/135] Processing Yukon_Beaver... Analyzing... ⚠ No .tif files found, skipping\n",
      "[131/135] Processing Yukon_Eagle... Analyzing... Flagging... Saving CSV... ✓ 34 masks, 1 flagged\n",
      "[132/135] Processing Yukon_NearStevensVillage... Analyzing... Flagging... Saving CSV... ✓ 20 masks, 0 flagged\n",
      "[133/135] Processing Zambezi_LukuluMission... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 5 flagged\n",
      "[134/135] Processing Zambezi_Matundo-Cais... Analyzing... Flagging... Saving CSV... ✓ 39 masks, 2 flagged\n",
      "[135/135] Processing Zambezi_Sesheke... Analyzing... Flagging... Saving CSV... ✓ 35 masks, 1 flagged\n",
      "\n",
      "============================================================\n",
      "SCREENING COMPLETE\n",
      "============================================================\n",
      "Total rivers screened: 135\n",
      "QC tracking file saved to: E:\\Dissertation\\Data\\RiverMapping\\MaskQC\\QC_tracking.csv\n",
      "Individual river reports saved to: E:\\Dissertation\\Data\\RiverMapping\\MaskQC\n",
      "\n",
      "Rivers with flagged masks:\n",
      "  - Amazonas_Tamshiyacu: 1/31 masks flagged\n",
      "  - Amur_Komsomolsk: 2/39 masks flagged\n",
      "  - Amyl_Kachulka: 1/24 masks flagged\n",
      "  - Apalachicola_NearBlountstown: 1/37 masks flagged\n",
      "  - Araguaia_SaoFelixDoAraguaia: 1/37 masks flagged\n",
      "  - Beas_MandiPlain: 1/29 masks flagged\n",
      "  - Beni_Rurrenabaque: 1/36 masks flagged\n",
      "  - Benue_Umaisha: 20/39 masks flagged\n",
      "  - Bhareli_NTRoadCrossing: 3/31 masks flagged\n",
      "  - BolshayaKet_Rodyonovka: 17/39 masks flagged\n",
      "  - Brahmaputra_Bahadurabad: 1/34 masks flagged\n",
      "  - Brahmaputra_Pasighat: 2/39 masks flagged\n",
      "  - Chari_Bousso: 19/39 masks flagged\n",
      "  - Chari_Guelengdeng: 21/39 masks flagged\n",
      "  - Chari_Ndjamena: 21/39 masks flagged\n",
      "  - Chari_Sahr: 20/39 masks flagged\n",
      "  - Chenab_Akhnoor: 1/20 masks flagged\n",
      "  - Demini_PostoAjuricaba: 1/35 masks flagged\n",
      "  - Fraser_Hope: 1/39 masks flagged\n",
      "  - Ganges_Paksey: 3/37 masks flagged\n",
      "  - Guapore_Pimenteiras: 1/37 masks flagged\n",
      "  - Helmand_Kajaki: 5/39 masks flagged\n",
      "  - Helmand_Malakhan: 9/39 masks flagged\n",
      "  - HuangHe_Huayuankou: 1/25 masks flagged\n",
      "  - HuangHe_TanglaiQu: 1/36 masks flagged\n",
      "  - Indus_Attock: 6/39 masks flagged\n",
      "  - Irrawaddy_Katha: 2/37 masks flagged\n",
      "  - Irtysh_Bobrovsky: 7/39 masks flagged\n",
      "  - Irtysh_Hanti-Mansisk: 5/39 masks flagged\n",
      "  - Irtysh_Pavlodar: 8/39 masks flagged\n",
      "  - Irtysh_Semiyarskoje: 8/39 masks flagged\n",
      "  - Jurua_SantosDumont: 1/36 masks flagged\n",
      "  - Jutai_PortoSeguro: 1/39 masks flagged\n",
      "  - Kamchatka_Kozyrevsk: 19/39 masks flagged\n",
      "  - Kan_Kansk: 8/39 masks flagged\n",
      "  - Logone_Bongor: 21/37 masks flagged\n",
      "  - MadreDeDios_CachuelaEsperanza: 1/39 masks flagged\n",
      "  - Magdalena_Calamar: 6/39 masks flagged\n",
      "  - Magdalena_PuertoBerrio: 8/39 masks flagged\n",
      "  - Mamore_Guajara-Mirim: 1/37 masks flagged\n",
      "  - Mamore_PuertoSiles: 1/37 masks flagged\n",
      "  - Manas_Mathanguri: 6/39 masks flagged\n",
      "  - Maranon_SanRegis: 1/33 masks flagged\n",
      "  - Mbam_Goura: 20/39 masks flagged\n",
      "  - Mekong_Kratie: 1/36 masks flagged\n",
      "  - Mortes_SantoAntonioDoLeverger: 1/37 masks flagged\n",
      "  - Niger_Tossaye: 10/39 masks flagged\n",
      "  - Ob_Barnaul: 8/39 masks flagged\n",
      "  - Ob_Kolpashevo: 16/39 masks flagged\n",
      "  - Ob_Mogochin: 15/39 masks flagged\n",
      "  - Ob_Phominskoje: 1/32 masks flagged\n",
      "  - Orinoco_CiudadBolivar: 12/39 masks flagged\n",
      "  - Orinoco_Musinacio: 5/39 masks flagged\n",
      "  - Panj_NizPyandzh: 1/31 masks flagged\n",
      "  - Paraguay_Asuncion: 2/39 masks flagged\n",
      "  - Paraguay_PortoMurtinho: 2/39 masks flagged\n",
      "  - Parana_Chapeton: 1/39 masks flagged\n",
      "  - Peace_FifthMeridian: 1/37 masks flagged\n",
      "  - Pilcomayo_VillaMontes: 2/36 masks flagged\n",
      "  - Porcupine_NearFortYukon: 5/39 masks flagged\n",
      "  - Purus_Canutama: 1/37 masks flagged\n",
      "  - Purus_Labrea: 1/37 masks flagged\n",
      "  - Purus_SeringalDaCaridade: 1/37 masks flagged\n",
      "  - Red_Index: 1/37 masks flagged\n",
      "  - Rufiji_Stigler: 1/31 masks flagged\n",
      "  - Sangha_Ouesso: 18/39 masks flagged\n",
      "  - SaoFrancisco_Morpara: 1/37 masks flagged\n",
      "  - Selenga_Naushki: 1/30 masks flagged\n",
      "  - Solimoes_Itapeua: 1/39 masks flagged\n",
      "  - Solimoes_Manacapuru: 1/39 masks flagged\n",
      "  - Solimoes_SantoAntonioDoIca: 1/34 masks flagged\n",
      "  - SonghuaJiang_Haerbin: 2/39 masks flagged\n",
      "  - Ucayali_Pucallpa: 1/36 masks flagged\n",
      "  - Vilyuy_KhatyrykKhoma: 18/39 masks flagged\n",
      "  - White_DevallsBluff: 1/37 masks flagged\n",
      "  - White_Petersburg: 1/37 masks flagged\n",
      "  - Yangtze_Datong: 1/39 masks flagged\n",
      "  - Yukon_Eagle: 1/34 masks flagged\n",
      "  - Zambezi_LukuluMission: 5/39 masks flagged\n",
      "  - Zambezi_Matundo-Cais: 2/39 masks flagged\n",
      "  - Zambezi_Sesheke: 1/35 masks flagged\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from datetime import datetime\n",
    "\n",
    "def analyze_mask_statistics(mask_dir):\n",
    "    \"\"\"\n",
    "    Analyze all .tif masks in a directory and compute statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mask_dir : str\n",
    "        Path to directory containing .tif mask files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with statistics for each mask\n",
    "    \"\"\"\n",
    "    tif_files = sorted([f for f in os.listdir(mask_dir) if f.lower().endswith('.tif')])\n",
    "    \n",
    "    if not tif_files:\n",
    "        return None\n",
    "    \n",
    "    stats_list = []\n",
    "    \n",
    "    for tif_file in tif_files:\n",
    "        file_path = os.path.join(mask_dir, tif_file)\n",
    "        \n",
    "        try:\n",
    "            # Suppress the NotGeoreferencedWarning\n",
    "            import warnings\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n",
    "                with rasterio.open(file_path) as src:\n",
    "                    data = src.read(1)  # Read first band\n",
    "            \n",
    "            # Get unique values to check what's actually in the data\n",
    "            unique_vals = np.unique(data)\n",
    "            \n",
    "            # Convert data to float for consistent handling\n",
    "            data = data.astype(np.float64)\n",
    "            \n",
    "            # Count pixels\n",
    "            total_pixels = data.size\n",
    "            \n",
    "            # Detect if this is a 0/1 mask or 0/255 mask\n",
    "            # Check if any values are close to 255\n",
    "            has_255 = np.any(np.abs(data - 255) < 0.01)\n",
    "            \n",
    "            if has_255:\n",
    "                # This is a 0/255 mask (Zhao study)\n",
    "                is_zero = np.abs(data - 0) < 0.01\n",
    "                is_wet = np.abs(data - 255) < 0.01\n",
    "                wet_pixels = np.sum(is_wet)\n",
    "                dry_pixels = np.sum(is_zero)\n",
    "                other_pixels = total_pixels - wet_pixels - dry_pixels\n",
    "            else:\n",
    "                # This is a 0/1 mask (Greenberg study)\n",
    "                is_zero = np.abs(data - 0) < 0.01\n",
    "                is_one = np.abs(data - 1) < 0.01\n",
    "                wet_pixels = np.sum(is_one)\n",
    "                dry_pixels = np.sum(is_zero)\n",
    "                other_pixels = total_pixels - wet_pixels - dry_pixels\n",
    "            \n",
    "            # Calculate percentages\n",
    "            pct_wet = (wet_pixels / total_pixels) * 100 if total_pixels > 0 else 0\n",
    "            pct_dry = (dry_pixels / total_pixels) * 100 if total_pixels > 0 else 0\n",
    "            \n",
    "            stats_list.append({\n",
    "                'filename': tif_file,\n",
    "                'total_pixels': total_pixels,\n",
    "                'wet_pixels': wet_pixels,\n",
    "                'dry_pixels': dry_pixels,\n",
    "                'other_pixels': other_pixels,\n",
    "                'pct_wet': pct_wet,\n",
    "                'pct_dry': pct_dry,\n",
    "                'unique_values': str(unique_vals.tolist())\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading {tif_file}: {e}\")\n",
    "            stats_list.append({\n",
    "                'filename': tif_file,\n",
    "                'total_pixels': 0,\n",
    "                'wet_pixels': 0,\n",
    "                'dry_pixels': 0,\n",
    "                'other_pixels': 0,\n",
    "                'pct_wet': 0,\n",
    "                'pct_dry': 0,\n",
    "                'unique_values': '',\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(stats_list)\n",
    "\n",
    "def flag_problematic_masks(df, z_threshold=3):\n",
    "    \"\"\"\n",
    "    Flag masks that deviate significantly from the mean or have other issues.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with mask statistics\n",
    "    z_threshold : float\n",
    "        Z-score threshold for flagging outliers\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with added flag columns\n",
    "    \"\"\"\n",
    "    # Calculate z-scores for wet pixels\n",
    "    mean_wet = df['wet_pixels'].mean()\n",
    "    std_wet = df['wet_pixels'].std()\n",
    "    \n",
    "    if std_wet > 0:\n",
    "        df['z_score'] = (df['wet_pixels'] - mean_wet) / std_wet\n",
    "    else:\n",
    "        df['z_score'] = 0\n",
    "    \n",
    "    # Flag different types of problems\n",
    "    df['flag_all_dry'] = df['wet_pixels'] == 0\n",
    "    df['flag_all_wet'] = df['pct_wet'] > 99.9\n",
    "    df['flag_extreme_outlier'] = np.abs(df['z_score']) > z_threshold\n",
    "    df['flag_non_binary'] = df['other_pixels'] > 0\n",
    "    df['flag_tiny_wet'] = (df['pct_wet'] < 0.1) & (df['pct_wet'] > 0)\n",
    "    \n",
    "    # Overall flag\n",
    "    df['flagged'] = (df['flag_all_dry'] | df['flag_all_wet'] | \n",
    "                     df['flag_extreme_outlier'] | df['flag_non_binary'] | \n",
    "                     df['flag_tiny_wet'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_contact_sheet(mask_dir, df, output_path, max_images=40):\n",
    "    \"\"\"\n",
    "    Create a contact sheet showing all masks with flagged ones highlighted.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mask_dir : str\n",
    "        Path to directory containing masks\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with mask statistics and flags\n",
    "    output_path : str\n",
    "        Path to save the contact sheet image\n",
    "    max_images : int\n",
    "        Maximum number of images to show\n",
    "    \"\"\"\n",
    "    tif_files = df['filename'].tolist()[:max_images]\n",
    "    n_images = len(tif_files)\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    n_cols = min(8, n_images)\n",
    "    n_rows = int(np.ceil(n_images / n_cols))\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, n_rows * 2.5))\n",
    "    \n",
    "    for idx, tif_file in enumerate(tif_files):\n",
    "        file_path = os.path.join(mask_dir, tif_file)\n",
    "        \n",
    "        try:\n",
    "            # Suppress the NotGeoreferencedWarning\n",
    "            import warnings\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n",
    "                with rasterio.open(file_path) as src:\n",
    "                    data = src.read(1)\n",
    "            \n",
    "            ax = plt.subplot(n_rows, n_cols, idx + 1)\n",
    "            ax.imshow(data, cmap='Blues', vmin=0, vmax=1)\n",
    "            \n",
    "            # Highlight flagged images with red border\n",
    "            row = df[df['filename'] == tif_file].iloc[0]\n",
    "            if row['flagged']:\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_edgecolor('red')\n",
    "                    spine.set_linewidth(3)\n",
    "            \n",
    "            # Add title with basic info\n",
    "            title = f\"{tif_file[:15]}...\\n\"\n",
    "            title += f\"Wet: {row['pct_wet']:.1f}%\"\n",
    "            if row['flagged']:\n",
    "                title += \"\\n⚠ FLAGGED\"\n",
    "            \n",
    "            ax.set_title(title, fontsize=8, color='red' if row['flagged'] else 'black')\n",
    "            ax.axis('off')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error visualizing {tif_file}: {e}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_time_series_plot(df, output_path, river_name):\n",
    "    \"\"\"\n",
    "    Create a time series plot of wet pixels over time.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with mask statistics\n",
    "    output_path : str\n",
    "        Path to save the plot\n",
    "    river_name : str\n",
    "        Name of the river for the title\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    x = np.arange(len(df))\n",
    "    \n",
    "    # Plot all points\n",
    "    ax.plot(x, df['wet_pixels'], 'o-', color='steelblue', markersize=4, label='Normal')\n",
    "    \n",
    "    # Highlight flagged points\n",
    "    flagged_df = df[df['flagged']]\n",
    "    if not flagged_df.empty:\n",
    "        flagged_indices = flagged_df.index\n",
    "        ax.plot(flagged_indices, flagged_df['wet_pixels'], 'ro', \n",
    "                markersize=8, label='Flagged', zorder=5)\n",
    "    \n",
    "    # Add mean line\n",
    "    mean_wet = df['wet_pixels'].mean()\n",
    "    ax.axhline(mean_wet, color='green', linestyle='--', alpha=0.7, label=f'Mean: {mean_wet:.0f}')\n",
    "    \n",
    "    # Add ±3 std lines\n",
    "    std_wet = df['wet_pixels'].std()\n",
    "    ax.axhline(mean_wet + 3*std_wet, color='red', linestyle=':', alpha=0.5, label='±3 σ')\n",
    "    ax.axhline(mean_wet - 3*std_wet, color='red', linestyle=':', alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Mask Index', fontsize=12)\n",
    "    ax.set_ylabel('Wet Pixels', fontsize=12)\n",
    "    ax.set_title(f'{river_name} - Wet Pixel Count Over Time', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def screen_all_rivers(base_dir, output_dir, z_threshold=3, create_visualizations=True):\n",
    "    \"\"\"\n",
    "    Screen all river mask directories and generate reports.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_dir : str\n",
    "        Base directory containing river folders\n",
    "    output_dir : str\n",
    "        Directory to save QC reports\n",
    "    z_threshold : float\n",
    "        Z-score threshold for flagging outliers\n",
    "    create_visualizations : bool\n",
    "        Whether to create contact sheets and plots (slower)\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all river directories\n",
    "    river_dirs = [d for d in os.listdir(base_dir) \n",
    "                  if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    river_dirs = sorted(river_dirs)\n",
    "    \n",
    "    print(f\"Found {len(river_dirs)} river directories\\n\")\n",
    "    \n",
    "    # Track overall QC status\n",
    "    qc_summary = []\n",
    "    \n",
    "    for river_idx, river_name in enumerate(river_dirs, 1):\n",
    "        print(f\"[{river_idx}/{len(river_dirs)}] Processing {river_name}...\", end='', flush=True)\n",
    "        \n",
    "        # Path to masks\n",
    "        mask_dir = os.path.join(base_dir, river_name, \"Cleaned\")\n",
    "        \n",
    "        if not os.path.exists(mask_dir):\n",
    "            print(f\" ⚠ No 'Cleaned' folder found, skipping\")\n",
    "            qc_summary.append({\n",
    "                'river_name': river_name,\n",
    "                'status': 'No masks found',\n",
    "                'total_masks': 0,\n",
    "                'flagged_masks': 0,\n",
    "                'qc_complete': False\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Analyze masks\n",
    "        print(f\" Analyzing...\", end='', flush=True)\n",
    "        df = analyze_mask_statistics(mask_dir)\n",
    "        \n",
    "        if df is None or len(df) == 0:\n",
    "            print(f\" ⚠ No .tif files found, skipping\")\n",
    "            qc_summary.append({\n",
    "                'river_name': river_name,\n",
    "                'status': 'No .tif files',\n",
    "                'total_masks': 0,\n",
    "                'flagged_masks': 0,\n",
    "                'qc_complete': False\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Flag problematic masks\n",
    "        print(f\" Flagging...\", end='', flush=True)\n",
    "        df = flag_problematic_masks(df, z_threshold)\n",
    "        \n",
    "        # Create river-specific output directory\n",
    "        river_output_dir = os.path.join(output_dir, river_name)\n",
    "        os.makedirs(river_output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save statistics CSV\n",
    "        print(f\" Saving CSV...\", end='', flush=True)\n",
    "        csv_path = os.path.join(river_output_dir, f\"{river_name}_statistics.csv\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        if create_visualizations:\n",
    "            # Create contact sheet\n",
    "            print(f\" Contact sheet...\", end='', flush=True)\n",
    "            contact_sheet_path = os.path.join(river_output_dir, f\"{river_name}_contact_sheet.png\")\n",
    "            try:\n",
    "                create_contact_sheet(mask_dir, df, contact_sheet_path)\n",
    "            except Exception as e:\n",
    "                print(f\" [Contact sheet error: {e}]\", end='', flush=True)\n",
    "            \n",
    "            # Create time series plot\n",
    "            print(f\" Time series...\", end='', flush=True)\n",
    "            timeseries_path = os.path.join(river_output_dir, f\"{river_name}_timeseries.png\")\n",
    "            try:\n",
    "                create_time_series_plot(df, timeseries_path, river_name)\n",
    "            except Exception as e:\n",
    "                print(f\" [Time series error: {e}]\", end='', flush=True)\n",
    "        \n",
    "        # Summary stats\n",
    "        total_masks = len(df)\n",
    "        flagged_count = df['flagged'].sum()\n",
    "        \n",
    "        print(f\" ✓ {total_masks} masks, {flagged_count} flagged\")\n",
    "        \n",
    "        qc_summary.append({\n",
    "            'river_name': river_name,\n",
    "            'status': 'Processed',\n",
    "            'total_masks': total_masks,\n",
    "            'flagged_masks': flagged_count,\n",
    "            'pct_flagged': (flagged_count / total_masks * 100) if total_masks > 0 else 0,\n",
    "            'qc_complete': False,\n",
    "            'date_screened': datetime.now().strftime('%Y-%m-%d'),\n",
    "            'notes': ''\n",
    "        })\n",
    "    \n",
    "    # Save overall QC tracking file\n",
    "    qc_df = pd.DataFrame(qc_summary)\n",
    "    qc_tracking_path = os.path.join(output_dir, 'QC_tracking.csv')\n",
    "    qc_df.to_csv(qc_tracking_path, index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SCREENING COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total rivers screened: {len(river_dirs)}\")\n",
    "    print(f\"QC tracking file saved to: {qc_tracking_path}\")\n",
    "    print(f\"Individual river reports saved to: {output_dir}\")\n",
    "    print(f\"\\nRivers with flagged masks:\")\n",
    "    for _, row in qc_df[qc_df['flagged_masks'] > 0].iterrows():\n",
    "        print(f\"  - {row['river_name']}: {row['flagged_masks']}/{row['total_masks']} masks flagged\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your paths here\n",
    "    base_directory = r\"E:\\Dissertation\\Data\\RiverMapping\\RiverMasks\"\n",
    "    output_directory = r\"E:\\Dissertation\\Data\\RiverMapping\\MaskQC\"\n",
    "    \n",
    "    # Run the screening\n",
    "    # Set create_visualizations=False for faster CSV-only mode\n",
    "    screen_all_rivers(base_directory, output_directory, z_threshold=3, create_visualizations=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5360574",
   "metadata": {},
   "source": [
    "## Interactive QC viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56e3fd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 135 rivers needing QC\n",
      "Rivers with flagged masks: 81\n",
      "\n",
      "============================================================\n",
      "INTERACTIVE MASK QC VIEWER\n",
      "============================================================\n",
      "\n",
      "Keyboard shortcuts:\n",
      "  D - Mark for deletion and advance\n",
      "  K - Keep mask and advance\n",
      "  N - Next mask (without marking)\n",
      "  P - Previous mask\n",
      "  F - Finish current river and move to next\n",
      "  Q - Quit (progress is saved)\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "River 1/135: Logone_Bongor\n",
      "Total masks: 37, Flagged: 21\n",
      "============================================================\n",
      "Key pressed: shift\n",
      "Key pressed: N\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n",
      "Key pressed: n\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # Use interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Button\n",
    "import warnings\n",
    "\n",
    "# Enable interactive mode\n",
    "plt.ion()\n",
    "\n",
    "class MaskQCViewer:\n",
    "    \"\"\"\n",
    "    Interactive viewer for QC'ing river masks.\n",
    "    Shows flagged masks first, allows quick deletion marking and QC completion.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, qc_dir, base_mask_dir):\n",
    "        self.qc_dir = qc_dir\n",
    "        self.base_mask_dir = base_mask_dir\n",
    "        self.current_river_idx = 0\n",
    "        self.current_mask_idx = 0\n",
    "        self.masks_to_delete = []\n",
    "        \n",
    "        # Load QC tracking\n",
    "        self.qc_tracking = pd.read_csv(os.path.join(qc_dir, 'QC_tracking.csv'))\n",
    "        \n",
    "        # Filter to rivers that need QC (have flagged masks or not yet QC'd)\n",
    "        self.rivers_to_qc = self.qc_tracking[\n",
    "            (self.qc_tracking['flagged_masks'] > 0) | \n",
    "            (self.qc_tracking['qc_complete'] == False)\n",
    "        ].copy()\n",
    "        \n",
    "        self.rivers_to_qc = self.rivers_to_qc.sort_values('pct_flagged', ascending=False)\n",
    "        \n",
    "        print(f\"Found {len(self.rivers_to_qc)} rivers needing QC\")\n",
    "        print(f\"Rivers with flagged masks: {(self.rivers_to_qc['flagged_masks'] > 0).sum()}\")\n",
    "        \n",
    "    def load_river_data(self, river_name):\n",
    "        \"\"\"Load statistics CSV for a river.\"\"\"\n",
    "        csv_path = os.path.join(self.qc_dir, river_name, f\"{river_name}_statistics.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            df = pd.read_csv(csv_path)\n",
    "            # Sort by flagged first, then by z_score magnitude\n",
    "            df['abs_z'] = df['z_score'].abs()\n",
    "            df = df.sort_values(['flagged', 'abs_z'], ascending=[False, False])\n",
    "            return df\n",
    "        return None\n",
    "    \n",
    "    def view_river(self, river_idx):\n",
    "        \"\"\"Start QC for a specific river.\"\"\"\n",
    "        if river_idx >= len(self.rivers_to_qc):\n",
    "            print(\"All rivers QC'd!\")\n",
    "            return\n",
    "        \n",
    "        self.current_river_idx = river_idx\n",
    "        river_row = self.rivers_to_qc.iloc[river_idx]\n",
    "        river_name = river_row['river_name']\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"River {river_idx + 1}/{len(self.rivers_to_qc)}: {river_name}\")\n",
    "        print(f\"Total masks: {river_row['total_masks']}, Flagged: {river_row['flagged_masks']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Load river data\n",
    "        self.current_df = self.load_river_data(river_name)\n",
    "        if self.current_df is None:\n",
    "            print(f\"No statistics found for {river_name}, skipping...\")\n",
    "            self.view_river(river_idx + 1)\n",
    "            return\n",
    "        \n",
    "        self.current_river_name = river_name\n",
    "        self.current_mask_idx = 0\n",
    "        self.masks_to_delete = []\n",
    "        \n",
    "        # Show first mask\n",
    "        self.show_mask()\n",
    "    \n",
    "    def show_mask(self):\n",
    "        \"\"\"Display current mask with statistics and controls.\"\"\"\n",
    "        if self.current_mask_idx >= len(self.current_df):\n",
    "            self.finish_river()\n",
    "            return\n",
    "        \n",
    "        row = self.current_df.iloc[self.current_mask_idx]\n",
    "        mask_path = os.path.join(self.base_mask_dir, self.current_river_name, \n",
    "                                 \"Cleaned\", row['filename'])\n",
    "        \n",
    "        # Create figure\n",
    "        plt.close('all')\n",
    "        self.fig = plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Load and display mask\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n",
    "            with rasterio.open(mask_path) as src:\n",
    "                data = src.read(1)\n",
    "        \n",
    "        # Normalize for display\n",
    "        data_normalized = data / data.max() if data.max() > 0 else data\n",
    "        \n",
    "        ax = plt.subplot(1, 1, 1)\n",
    "        ax.imshow(data_normalized, cmap='Blues', vmin=0, vmax=1)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Build title with statistics\n",
    "        title = f\"{self.current_river_name} - Mask {self.current_mask_idx + 1}/{len(self.current_df)}\\n\"\n",
    "        title += f\"{row['filename']}\\n\"\n",
    "        title += f\"Wet: {row['pct_wet']:.2f}% | Z-score: {row['z_score']:.2f}\\n\"\n",
    "        \n",
    "        if row['flagged']:\n",
    "            title += \"⚠ FLAGGED: \"\n",
    "            flags = []\n",
    "            if row['flag_all_dry']: flags.append(\"All Dry\")\n",
    "            if row['flag_all_wet']: flags.append(\"All Wet\")\n",
    "            if row['flag_extreme_outlier']: flags.append(\"Extreme Outlier\")\n",
    "            if row['flag_non_binary']: flags.append(\"Non-Binary\")\n",
    "            if row['flag_tiny_wet']: flags.append(\"Tiny Wet Area\")\n",
    "            title += \", \".join(flags)\n",
    "        \n",
    "        ax.set_title(title, fontsize=11, fontweight='bold', \n",
    "                    color='red' if row['flagged'] else 'black')\n",
    "        \n",
    "        # Add navigation info\n",
    "        info_text = f\"Press: [D]elete | [K]eep | [N]ext | [P]rev | [F]inish River | [Q]uit\"\n",
    "        self.fig.text(0.5, 0.02, info_text, ha='center', fontsize=10, \n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        # Connect keyboard events\n",
    "        self.cid = self.fig.canvas.mpl_connect('key_press_event', self.on_key)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.draw()\n",
    "        plt.pause(0.001)  # Allow GUI to update\n",
    "    \n",
    "    def on_key(self, event):\n",
    "        \"\"\"Handle keyboard shortcuts.\"\"\"\n",
    "        print(f\"Key pressed: {event.key}\")  # Debug output\n",
    "        \n",
    "        if event.key == 'd':\n",
    "            # Mark for deletion\n",
    "            row = self.current_df.iloc[self.current_mask_idx]\n",
    "            if row['filename'] not in self.masks_to_delete:\n",
    "                self.masks_to_delete.append(row['filename'])\n",
    "                print(f\"  Marked for deletion: {row['filename']}\")\n",
    "            self.current_mask_idx += 1\n",
    "            plt.close(self.fig)\n",
    "            self.show_mask()\n",
    "        \n",
    "        elif event.key == 'k':\n",
    "            # Keep and move to next\n",
    "            print(f\"  Kept: {self.current_df.iloc[self.current_mask_idx]['filename']}\")\n",
    "            self.current_mask_idx += 1\n",
    "            plt.close(self.fig)\n",
    "            self.show_mask()\n",
    "        \n",
    "        elif event.key == 'n':\n",
    "            # Next mask\n",
    "            self.current_mask_idx = min(self.current_mask_idx + 1, len(self.current_df) - 1)\n",
    "            plt.close(self.fig)\n",
    "            self.show_mask()\n",
    "        \n",
    "        elif event.key == 'p':\n",
    "            # Previous mask\n",
    "            self.current_mask_idx = max(self.current_mask_idx - 1, 0)\n",
    "            plt.close(self.fig)\n",
    "            self.show_mask()\n",
    "        \n",
    "        elif event.key == 'f':\n",
    "            # Finish this river\n",
    "            plt.close(self.fig)\n",
    "            self.finish_river()\n",
    "        \n",
    "        elif event.key == 'q':\n",
    "            # Quit\n",
    "            print(\"\\nQuitting QC session...\")\n",
    "            self.save_progress()\n",
    "            plt.close('all')\n",
    "    \n",
    "    def finish_river(self):\n",
    "        \"\"\"Complete QC for current river and move to next.\"\"\"\n",
    "        # Save deletion list\n",
    "        if self.masks_to_delete:\n",
    "            delete_list_path = os.path.join(self.qc_dir, self.current_river_name, \n",
    "                                           \"masks_to_delete.txt\")\n",
    "            with open(delete_list_path, 'w') as f:\n",
    "                f.write(\"\\n\".join(self.masks_to_delete))\n",
    "            print(f\"\\n  Saved {len(self.masks_to_delete)} masks to delete\")\n",
    "        \n",
    "        # Mark as QC complete\n",
    "        self.qc_tracking.loc[\n",
    "            self.qc_tracking['river_name'] == self.current_river_name, \n",
    "            'qc_complete'\n",
    "        ] = True\n",
    "        \n",
    "        self.qc_tracking.loc[\n",
    "            self.qc_tracking['river_name'] == self.current_river_name, \n",
    "            'notes'\n",
    "        ] = f\"{len(self.masks_to_delete)} marked for deletion\"\n",
    "        \n",
    "        # Save tracking file\n",
    "        self.save_progress()\n",
    "        \n",
    "        plt.close('all')\n",
    "        \n",
    "        # Move to next river\n",
    "        self.view_river(self.current_river_idx + 1)\n",
    "    \n",
    "    def save_progress(self):\n",
    "        \"\"\"Save QC tracking file.\"\"\"\n",
    "        self.qc_tracking.to_csv(os.path.join(self.qc_dir, 'QC_tracking.csv'), index=False)\n",
    "        print(\"  Progress saved\")\n",
    "    \n",
    "    def start_qc(self):\n",
    "        \"\"\"Start the QC process.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"INTERACTIVE MASK QC VIEWER\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(\"\\nKeyboard shortcuts:\")\n",
    "        print(\"  D - Mark for deletion and advance\")\n",
    "        print(\"  K - Keep mask and advance\")\n",
    "        print(\"  N - Next mask (without marking)\")\n",
    "        print(\"  P - Previous mask\")\n",
    "        print(\"  F - Finish current river and move to next\")\n",
    "        print(\"  Q - Quit (progress is saved)\")\n",
    "        print(f\"\\n{'='*60}\\n\")\n",
    "        \n",
    "        # Start with first river\n",
    "        self.view_river(0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your paths\n",
    "    qc_directory = r\"E:\\Dissertation\\Data\\RiverMapping\\MaskQC\"\n",
    "    mask_directory = r\"E:\\Dissertation\\Data\\RiverMapping\\RiverMasks\"\n",
    "    \n",
    "    # Create viewer and start\n",
    "    viewer = MaskQCViewer(qc_directory, mask_directory)\n",
    "    viewer.start_qc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7547a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
