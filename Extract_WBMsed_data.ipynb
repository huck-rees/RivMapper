{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9739be7c",
   "metadata": {},
   "source": [
    "# Extract WBMsed data\n",
    "\n",
    "The following code is meant to take the reach shapefile for a given river, and extract sediment flux metrics from the global WBMsed database. The code looks for the maximum valued pixel within the reach, then creates a best fit line for all of the reaches for each metric. This allows for the ID of outlier pixels (such as those that belong to an adjacent larger river system). The outlier pixels are then removed. Code developed using the Rio Bermejo in Argentina and compared to field data from Repsasch et al. (2020) and WBMsed data is based on model developed and validated in Cohen et al. (2013). Outputs include mean flux of bedload, suspended bedload, washload, and total sediment flux (all in kg per s), as well as mean particle size (in m).\n",
    "\n",
    "WBMsed model: \n",
    "Cohen, S., Kettner, A.J., Syvitski, J.P.M., Fekete, B.M., 2013. WBMsed, a distributed global-scale riverine sediment flux model: Model description and validation. Computers & Geosciences, Modeling for Environmental Change 53, 80â€“93. https://doi.org/10.1016/j.cageo.2011.08.011\n",
    "\n",
    "Rio Bermejo case study with sediment flux estimates:\n",
    "Repasch, M., Wittmann, H., Scheingross, J.S., Sachse, D., Szupiany, R., Orfeo, O., Fuchs, M., Hovius, N., 2020. Sediment Transit Time and Floodplain Storage Dynamics in Alluvial Rivers Revealed by Meteoric 10Be. Journal of Geophysical Research: Earth Surface 125, e2019JF005419. https://doi.org/10.1029/2019JF005419\n",
    "\n",
    "Based on code developed by Evan Greenberg, PhD:\n",
    "https://github.com/evan-greenbrg/MeanderMigration/blob/main/GetWBMdata_new_wbm.py\n",
    "\n",
    "Code Author: James (Huck) Rees; PhD Student, UCSB Geography\n",
    "\n",
    "Date: October 17, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e06098",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ed50c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.features import geometry_mask\n",
    "from shapely.geometry import mapping\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa762fc",
   "metadata": {},
   "source": [
    "## Initialize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0784dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_asc_rasters(directory_path):\n",
    "    \"\"\"\n",
    "    Load all .asc raster files from a specified directory and assign EPSG:4326 CRS if undefined.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the .asc files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with filenames as keys and a tuple (raster data, metadata) as values.\n",
    "    \"\"\"\n",
    "    asc_files = glob.glob(os.path.join(directory_path, '*.asc'))\n",
    "    \n",
    "    if not asc_files:\n",
    "        raise FileNotFoundError(f\"No .asc files found in {directory_path}\")\n",
    "    \n",
    "    rasters = {}\n",
    "    \n",
    "    epsg_4326_wkt = 'GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433]]'\n",
    "    \n",
    "    for file_path in asc_files:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            raster_data = src.read(1)\n",
    "            metadata = src.meta\n",
    "\n",
    "            if not metadata.get('crs'):\n",
    "                print(f\"No CRS found for {file_path}. Manually assigning EPSG:4326 (WGS 84).\")\n",
    "                metadata['crs'] = epsg_4326_wkt\n",
    "            \n",
    "            file_key = os.path.basename(file_path).replace('.asc', '')\n",
    "            rasters[file_key] = (raster_data, metadata)\n",
    "    \n",
    "    return rasters\n",
    "\n",
    "def extract_raster_values(rasters, lat, lon):\n",
    "    \"\"\"\n",
    "    Extract values from all rasters in the dictionary at the given latitude and longitude.\n",
    "\n",
    "    Args:\n",
    "        rasters (dict): Dictionary of raster data and metadata.\n",
    "        lat (float): Latitude of the point.\n",
    "        lon (float): Longitude of the point.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are the raster names and values are the extracted values at the given lat/lon.\n",
    "    \"\"\"\n",
    "    extracted_values = {}\n",
    "\n",
    "    for raster_name, (raster_data, metadata) in rasters.items():\n",
    "        transform = metadata['transform']\n",
    "        row, col = rasterio.transform.rowcol(transform, lon, lat)\n",
    "\n",
    "        try:\n",
    "            value = raster_data[row, col]\n",
    "            extracted_values[raster_name] = value\n",
    "        except IndexError:\n",
    "            print(f\"Coordinates (lat: {lat}, lon: {lon}) are out of bounds for {raster_name}.\")\n",
    "            extracted_values[raster_name] = None\n",
    "\n",
    "    return extracted_values\n",
    "\n",
    "def extract_reach_raster_values(root_dir, river_name, rasters):\n",
    "    \"\"\"\n",
    "    Extract maximum raster values within each polygon reach for a given river.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root directory where the river folders are stored.\n",
    "        river_name (str): Name of the river.\n",
    "        rasters (dict): Dictionary of raster data and metadata.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the maximum raster values for each polygon reach.\n",
    "    \"\"\"\n",
    "    shapefile_path = os.path.join(root_dir, river_name, f\"{river_name}.shp\")\n",
    "\n",
    "    if not os.path.exists(shapefile_path):\n",
    "        raise FileNotFoundError(f\"Shapefile not found: {shapefile_path}\")\n",
    "    \n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "    if 'ds_order' not in gdf.columns:\n",
    "        raise ValueError(f\"'ds_order' column not found in {shapefile_path}\")\n",
    "\n",
    "    target_crs = \"EPSG:4326\"\n",
    "    if gdf.crs != target_crs:\n",
    "        print(f\"Reprojecting {river_name} shapefile from {gdf.crs} to {target_crs}.\")\n",
    "        gdf = gdf.to_crs(target_crs)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for idx, reach in gdf.iterrows():\n",
    "        reach_id = reach['ds_order']\n",
    "        geometry = [mapping(reach['geometry'])]\n",
    "\n",
    "        reach_data = {'ds_order': reach_id}\n",
    "\n",
    "        for raster_name, (raster_data, metadata) in rasters.items():\n",
    "            try:\n",
    "                mask_array = geometry_mask(\n",
    "                    geometries=geometry,\n",
    "                    transform=metadata['transform'],\n",
    "                    invert=True,\n",
    "                    out_shape=raster_data.shape\n",
    "                )\n",
    "\n",
    "                masked_raster = np.where(mask_array, raster_data, np.nan)\n",
    "\n",
    "                max_value = np.nanmax(masked_raster) if np.any(~np.isnan(masked_raster)) else None\n",
    "                reach_data[f'max_{raster_name}'] = max_value\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting raster values for {raster_name} in reach {reach_id}: {e}\")\n",
    "                reach_data[f'max_{raster_name}'] = None\n",
    "\n",
    "        data.append(reach_data)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def remove_outliers_and_interpolate(df, raster_name, degree=2, std_dev_threshold=2):\n",
    "    \"\"\"\n",
    "    Fit a polynomial curve to the raster data, identify outliers, remove them, and replace the outliers\n",
    "    using linear interpolation between adjacent upstream and downstream reaches.\n",
    "    \"\"\"\n",
    "    x = df['ds_order'].values\n",
    "    y = df[raster_name].values\n",
    "    \n",
    "    mask = ~np.isnan(y)\n",
    "    x_valid = x[mask].reshape(-1, 1)\n",
    "    y_valid = y[mask]\n",
    "    \n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    x_poly = poly.fit_transform(x_valid)\n",
    "    model = LinearRegression()\n",
    "    model.fit(x_poly, y_valid)\n",
    "    \n",
    "    y_pred_initial = model.predict(poly.transform(x.reshape(-1, 1)))\n",
    "    \n",
    "    residuals = y - y_pred_initial\n",
    "    std_dev = np.nanstd(residuals)\n",
    "    outliers = np.abs(residuals) > std_dev_threshold * std_dev\n",
    "    \n",
    "    if not np.any(outliers):\n",
    "        return df\n",
    "    \n",
    "    outlier_indices = np.where(outliers)[0]\n",
    "    non_outlier_indices = np.where(~outliers)[0]\n",
    "    \n",
    "    for i in range(len(outlier_indices)):\n",
    "        start = outlier_indices[i]\n",
    "        \n",
    "        end = start\n",
    "        while end + 1 in outlier_indices:\n",
    "            end += 1\n",
    "            i += 1\n",
    "        \n",
    "        upstream_index = non_outlier_indices[non_outlier_indices < start].max() if np.any(non_outlier_indices < start) else None\n",
    "        downstream_index = non_outlier_indices[non_outlier_indices > end].min() if np.any(non_outlier_indices > end) else None\n",
    "        \n",
    "        if upstream_index is None or downstream_index is None:\n",
    "            continue\n",
    "        \n",
    "        upstream_value = y[upstream_index]\n",
    "        downstream_value = y[downstream_index]\n",
    "        upstream_ds_order = x[upstream_index]\n",
    "        downstream_ds_order = x[downstream_index]\n",
    "        \n",
    "        for j in range(start, end + 1):\n",
    "            fraction = (x[j] - upstream_ds_order) / (downstream_ds_order - upstream_ds_order)\n",
    "            y[j] = upstream_value + fraction * (downstream_value - upstream_value)\n",
    "    \n",
    "    df[raster_name] = y\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_outliers_and_interpolate_all_rasters(df, degree=2, std_dev_threshold=2):\n",
    "    \"\"\"\n",
    "    Perform outlier detection and linear interpolation for all raster columns in the DataFrame.\n",
    "    \"\"\"\n",
    "    raster_columns = [col for col in df.columns if col != 'ds_order']\n",
    "    \n",
    "    for raster_name in raster_columns:\n",
    "        df = remove_outliers_and_interpolate(df, raster_name, degree, std_dev_threshold)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_wbmsed(root_dir, river_name, raster_dir, degree=2, std_dev_threshold=2):\n",
    "    \"\"\"\n",
    "    Wrapper function to load rasters, extract values for reaches, remove outliers using interpolation,\n",
    "    rename columns, and export the DataFrame as a CSV file in the root directory.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root directory for river shapefiles (also used for saving the CSV output).\n",
    "        river_name (str): Name of the river.\n",
    "        raster_dir (str): Directory containing the .asc raster files.\n",
    "        degree (int): Degree of the polynomial for curve fitting.\n",
    "        std_dev_threshold (float): Threshold for identifying outliers (in standard deviations).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with raster values, outliers removed, interpolated, and renamed columns.\n",
    "    \"\"\"\n",
    "    # Step 1: Load rasters\n",
    "    rasters = load_asc_rasters(raster_dir)\n",
    "    \n",
    "    # Step 2: Extract raster values for each reach\n",
    "    df = extract_reach_raster_values(root_dir, river_name, rasters)\n",
    "    \n",
    "    # Step 3: Remove outliers and interpolate for all raster columns\n",
    "    df_corrected = remove_outliers_and_interpolate_all_rasters(df, degree, std_dev_threshold)\n",
    "    \n",
    "    # Step 4: Rename columns (except 'ds_order')\n",
    "    for col in df_corrected.columns:\n",
    "        if col != 'ds_order':\n",
    "            # Extract everything between the second and third underscores\n",
    "            parts = col.split(\"_\")\n",
    "            if len(parts) >= 3:\n",
    "                metric = parts[2]  # Take the part after the second underscore\n",
    "            if len(parts) > 3:\n",
    "                # If there's a third underscore, ignore everything after it\n",
    "                metric = parts[2]  # Only use the second part\n",
    "            \n",
    "            # Determine the appropriate unit based on the metric\n",
    "            if \"Flux\" in metric:\n",
    "                unit = \"kg_s\"  # For flux metrics\n",
    "            elif \"Size\" in metric:\n",
    "                unit = \"m\"  # For particle size metrics\n",
    "            elif \"Discharge\" in metric:\n",
    "                unit = \"cms\" # For discharge\n",
    "            else:\n",
    "                unit = \"\"\n",
    "\n",
    "            # Rename the column with the new format: mean_<metric>_<unit>\n",
    "            new_col_name = f\"mean_{metric}_{unit}\".strip(\"_\")\n",
    "            df_corrected.rename(columns={col: new_col_name}, inplace=True)\n",
    "    \n",
    "    # Step 5: Export the DataFrame as a CSV file to the root_directory\n",
    "    csv_output_path = os.path.join(root_dir, f\"{river_name}_wbmsed.csv\")\n",
    "    df_corrected.to_csv(csv_output_path, index=False)\n",
    "    \n",
    "    print(f\"DataFrame exported to {csv_output_path}\")\n",
    "    \n",
    "    return df_corrected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc75cd6",
   "metadata": {},
   "source": [
    "## Enter input arguments and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0640f1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CRS found for D:\\Dissertation\\Data\\WBMsed\\RivMapperASCs\\Global_BedloadFlux_4p4p1+Dist_06min_aTS1990-2019.asc. Manually assigning EPSG:4326 (WGS 84).\n",
      "No CRS found for D:\\Dissertation\\Data\\WBMsed\\RivMapperASCs\\Global_Discharge_4p4p1+Dist_06min_aTS1990-2019.asc. Manually assigning EPSG:4326 (WGS 84).\n",
      "No CRS found for D:\\Dissertation\\Data\\WBMsed\\RivMapperASCs\\Global_ParticleSize_4p4p1+Dist_06min_aTS1990-2019.asc. Manually assigning EPSG:4326 (WGS 84).\n",
      "No CRS found for D:\\Dissertation\\Data\\WBMsed\\RivMapperASCs\\Global_SedimentFlux_4p4p1+Dist_06min_aTS1990-2019.asc. Manually assigning EPSG:4326 (WGS 84).\n",
      "No CRS found for D:\\Dissertation\\Data\\WBMsed\\RivMapperASCs\\Global_SuspendedBedFlux_4p4p1+Dist_06min_aTS1990-2019.asc. Manually assigning EPSG:4326 (WGS 84).\n",
      "No CRS found for D:\\Dissertation\\Data\\WBMsed\\RivMapperASCs\\Global_WashloadFlux_4p4p1+Dist_06min_aTS1990-2019.asc. Manually assigning EPSG:4326 (WGS 84).\n",
      "Reprojecting Beni shapefile from PROJCS[\"WGS_84_World_Mercator\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Mercator_2SP\"],PARAMETER[\"standard_parallel_1\",0],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"Meter\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]] to EPSG:4326.\n",
      "DataFrame exported to D:\\Dissertation\\Data\\RiverMapping\\Reaches\\Beni_wbmsed.csv\n"
     ]
    }
   ],
   "source": [
    "reach_directory = r\"D:\\Dissertation\\Data\\RiverMapping\\Reaches\"\n",
    "river_name = 'Beni'\n",
    "raster_directory = r\"D:\\Dissertation\\Data\\WBMsed\\RivMapperASCs\"\n",
    "\n",
    "df_final = extract_wbmsed(reach_directory, river_name, raster_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba504ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
