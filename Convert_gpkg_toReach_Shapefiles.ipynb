{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ab56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_river_folders(base_directory):\n",
    "    \"\"\"\n",
    "    Create folders for river monitoring stations in the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    base_directory (str): The path where folders should be created\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the base directory if it doesn't exist\n",
    "    if not os.path.exists(base_directory):\n",
    "        os.makedirs(base_directory)\n",
    "        print(f\"Created base directory: {base_directory}\")\n",
    "    \n",
    "    # Create each folder\n",
    "    created_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for folder_name in folder_names:\n",
    "        folder_path = os.path.join(base_directory, folder_name)\n",
    "        \n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "            created_count += 1\n",
    "            print(f\"Created: {folder_name}\")\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "            print(f\"Already exists: {folder_name}\")\n",
    "    \n",
    "    print(f\"\\n✓ Summary: {created_count} folders created, {skipped_count} already existed\")\n",
    "    print(f\"Total folders: {len(folder_names)}\")\n",
    "\n",
    "# Run it\n",
    "target_directory = r\"E:\\Dissertation\\Data\\RiverMapping\\Reaches\"\n",
    "create_river_folders(target_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70bdeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "def convert_gpkg_to_shapefile():\n",
    "    \"\"\"\n",
    "    Convert .gpkg files to shapefiles with specific attributes and reprojection.\n",
    "    Searches through all planform types and river names, converts each .gpkg to a shapefile\n",
    "    with a single 'ds_order' attribute set to 1, reprojects to EPSG:3395, and saves to\n",
    "    the corresponding reach folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define paths\n",
    "    source_base = r\"E:\\Dissertation\\Data\\Zhao_etal_2025\"\n",
    "    target_base = r\"E:\\Dissertation\\Data\\RiverMapping\\Reaches\"\n",
    "    \n",
    "    # Define planform types\n",
    "    planform_types = [\"B\", \"HSW\", \"LSW\", \"Me\"]\n",
    "    \n",
    "    # Counters for summary\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    not_found_count = 0\n",
    "    errors = []\n",
    "    \n",
    "    # Get list of all river folders from the target directory\n",
    "    river_folders = [f for f in os.listdir(target_base) \n",
    "                    if os.path.isdir(os.path.join(target_base, f))]\n",
    "    \n",
    "    print(f\"Found {len(river_folders)} river reach folders\")\n",
    "    print(f\"Searching through {len(planform_types)} planform types\\n\")\n",
    "    \n",
    "    # Process each river and planform combination\n",
    "    for river_name in river_folders:\n",
    "        found_gpkg = False\n",
    "        \n",
    "        for planform in planform_types:\n",
    "            # Construct source path\n",
    "            source_dir = os.path.join(source_base, planform, river_name)\n",
    "            source_file = os.path.join(source_dir, f\"{river_name}.gpkg\")\n",
    "            \n",
    "            # Check if the .gpkg file exists\n",
    "            if os.path.exists(source_file):\n",
    "                found_gpkg = True\n",
    "                \n",
    "                try:\n",
    "                    # Read the geopackage\n",
    "                    gdf = gpd.read_file(source_file)\n",
    "                    \n",
    "                    # Verify it has exactly one feature\n",
    "                    if len(gdf) != 1:\n",
    "                        print(f\"⚠ Warning: {river_name} ({planform}) has {len(gdf)} features, expected 1\")\n",
    "                    \n",
    "                    # Create new GeoDataFrame with only ds_order attribute\n",
    "                    gdf_new = gpd.GeoDataFrame(\n",
    "                        {'ds_order': [1]},\n",
    "                        geometry=gdf.geometry,\n",
    "                        crs=gdf.crs\n",
    "                    )\n",
    "                    \n",
    "                    # Reproject to EPSG:3395 (WGS 84 / World Mercator)\n",
    "                    gdf_reprojected = gdf_new.to_crs('EPSG:3395')\n",
    "                    \n",
    "                    # Construct output path\n",
    "                    output_dir = os.path.join(target_base, river_name)\n",
    "                    output_file = os.path.join(output_dir, f\"{river_name}.shp\")\n",
    "                    \n",
    "                    # Save as shapefile\n",
    "                    gdf_reprojected.to_file(output_file)\n",
    "                    \n",
    "                    success_count += 1\n",
    "                    print(f\"✓ Converted: {river_name} (from {planform})\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_count += 1\n",
    "                    error_msg = f\"{river_name} ({planform})\"\n",
    "                    errors.append(error_msg)\n",
    "                    print(f\"✗ Error: {river_name} ({planform}) - {str(e)}\")\n",
    "                    print(f\"   Skipping and continuing with remaining rivers...\")\n",
    "                \n",
    "                # Break after finding first matching planform (whether success or error)\n",
    "                break\n",
    "        \n",
    "        if not found_gpkg:\n",
    "            not_found_count += 1\n",
    "            print(f\"○ Not found: {river_name} (checked all planform types)\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"✓ Successfully converted: {success_count}\")\n",
    "    print(f\"○ GPKG files not found: {not_found_count}\")\n",
    "    print(f\"✗ Errors encountered: {error_count}\")\n",
    "    print(f\"Total river folders processed: {len(river_folders)}\")\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"FAILED RIVERS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for error in errors:\n",
    "            print(f\"  - {error}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting GPKG to Shapefile conversion...\\n\")\n",
    "    convert_gpkg_to_shapefile()\n",
    "    print(\"\\nConversion process complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefb7897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "target_base = r\"E:\\Dissertation\\Data\\RiverMapping\\Reaches\"\n",
    "\n",
    "print(f\"Does directory exist? {os.path.exists(target_base)}\")\n",
    "if os.path.exists(target_base):\n",
    "    folders = [f for f in os.listdir(target_base) if os.path.isdir(os.path.join(target_base, f))]\n",
    "    print(f\"Number of folders found: {len(folders)}\")\n",
    "    if folders:\n",
    "        print(f\"First few folders: {folders[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b81b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_folder_names_from_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Generate a list of folder names from a given directory.\n",
    "    \n",
    "    Parameters:\n",
    "    directory_path (str): Path to the directory containing folders\n",
    "    \n",
    "    Returns:\n",
    "    list: Sorted list of folder names\n",
    "    \"\"\"\n",
    "    # Get all items in the directory\n",
    "    all_items = os.listdir(directory_path)\n",
    "    \n",
    "    # Filter to only keep directories (not files)\n",
    "    folder_names = [item for item in all_items \n",
    "                   if os.path.isdir(os.path.join(directory_path, item))]\n",
    "    \n",
    "    # Sort alphabetically for consistency\n",
    "    folder_names.sort()\n",
    "    \n",
    "    return folder_names\n",
    "\n",
    "# Use it\n",
    "directory = r\"E:\\Dissertation\\Data\\Zhao_etal_2025\\B\"\n",
    "folder_names = get_folder_names_from_directory(directory)\n",
    "\n",
    "# Print as a Python list that you can copy-paste\n",
    "print(f\"Found {len(folder_names)} folders:\\n\")\n",
    "print(\"folder_names = [\")\n",
    "for i, name in enumerate(folder_names):\n",
    "    if i < len(folder_names) - 1:\n",
    "        print(f\"    '{name}',\")\n",
    "    else:\n",
    "        print(f\"    '{name}'\")\n",
    "print(\"]\")\n",
    "\n",
    "# Also print just the count and first few examples\n",
    "print(f\"\\nTotal: {len(folder_names)} folders\")\n",
    "if folder_names:\n",
    "    print(f\"First 5: {folder_names[:5]}\")\n",
    "    print(f\"Last 5: {folder_names[-5:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9808c430",
   "metadata": {},
   "source": [
    "## This folder creates folders with river names at the given directory\n",
    "It will optionally generate an additional subfolder, such as \"cleaned\" below the river_name folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5395c08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_unique_river_folders(base_directory, subfolder_name=None):\n",
    "    \"\"\"\n",
    "    Create folders for river monitoring stations, avoiding duplicates.\n",
    "    \n",
    "    Parameters:\n",
    "    base_directory (str): The path where folders should be created\n",
    "    subfolder_name (str, optional): If provided, creates a subfolder with this name\n",
    "                                    inside each river folder\n",
    "    \"\"\"\n",
    "    \n",
    "    # Raw list of folder names with duplicates\n",
    "    raw_folder_names = [\n",
    "        'Amazonas_Tamshiyacu', 'AmuDarya_Kerki', 'Amur_Khabarovsk',\n",
    "        'Amyl_Kachulka', 'Apalachicola_NearBlountstown', 'Araguaia_Aruana',\n",
    "        'Araguaia_LuizAlves', 'Araguaia_SaoFelixDoAraguaia', 'Beas_MandiPlain',\n",
    "        'Beni_Rurrenabaque', 'Benue_Ibi', 'Bhareli_NTRoadCrossing',\n",
    "        'Brahmaputra_Bahadurabad', 'Brahmaputra_Pandu', 'Brahmaputra_Yangcun',\n",
    "        'Chenab_Akhnoor', 'Cuiaba_PortoDoAlegre', 'Demini_PostoAjuricaba',\n",
    "        'Gandak_Dumariaghat', 'Gandak_Triveni', 'Ganges_Paksey',\n",
    "        'Guapore_Pimenteiras', 'HuangHe_Huayuankou', 'HuangHe_TanglaiQu',\n",
    "        'Ica_IpirangaVelho', 'Indus_Kotri', 'Irrawaddy_Katha',\n",
    "        'Irrawaddy_Sagaing', 'Itacuai_LadarioJusante', 'Jurua_EirunepeMontante',\n",
    "        'Jurua_SantosDumont', 'Katun_Srostki', 'Kokcha_Khojaghar',\n",
    "        'Krishna_Vijayawada', 'Lena_Tabaga', 'Liard_UpperCrossing',\n",
    "        'Logone_Bongor', 'Logone_Lai', 'Madeira_Humaita',\n",
    "        'Mamore_Guajara-Mirim', 'Mamore_PuertoSiles', 'Maranon_Borja',\n",
    "        'Maranon_SanRegis', 'Mortes_SantoAntonioDoLeverger', 'Napo_Bellavista',\n",
    "        'Napo_NvoRocafuerte', 'Naryn_UstKekirim', 'Ob_Prokhorkino',\n",
    "        'Panj_NizPyandzh', 'Parana_Corrientes', 'Peace_FifthMeridian',\n",
    "        'Pilcomayo_VillaMontes', 'Purus_Canutama', 'Purus_Labrea',\n",
    "        'Purus_SeringalDaCaridade', 'Purus_ValparaisoMontante', 'Red_Index',\n",
    "        'Rufiji_Stigler', 'Salinas_SanAugustin', 'SaoFrancisco_BomJesusDaLapa',\n",
    "        'SaoFrancisco_Morpara', 'SaptKosi_Baltara', 'SaptKosi_Chatara-Kothu',\n",
    "        'Selenga_Naushki', 'Solimoes_SantoAntonioDoIca', 'Solimoes_SaoPauloDeOlivenca',\n",
    "        'Solimoes_Tabatinga', 'Taku_NearTulsequa', 'Tanana_NearHardingLake',\n",
    "        'Tarauaca_Envira', 'Tista_AndersonBr', 'Tista_Kaunia',\n",
    "        'Tisza_Vylok', 'Trinity_Romayor', 'Ucayali_Atalaya',\n",
    "        'Ucayali_Pucallpa', 'Ucayali_Requena', 'Ural_Kushum',\n",
    "        'White_DevallsBluff', 'White_Petersburg', 'Yukon_NearStevensVillage',\n",
    "        'Zambezi_Sesheke', 'Beni', 'Beni', 'Beni', 'Beni', 'Beni',\n",
    "        'Beni', 'Beni', 'Beni', 'Beni', 'Beni', 'Bermejo', 'Bermejo',\n",
    "        'Bermejo', 'Bermejo', 'Bermejo', 'Bermejo', 'Bermejo', 'Bermejo',\n",
    "        'Bermejo', 'Bermejo', 'Bermejo', 'Bermejo', 'Bermejo', 'Bermejo',\n",
    "        'Bermejo', 'Bermejo', 'Bermejo', 'Bermejo', 'Bermejo', 'Bermejo',\n",
    "        'Koyukuk_Huslia', 'Yukon_Beaver', 'Aladan_VerkhoyanskiyPerevoz',\n",
    "        'Amazonas_Jatuarana', 'Amur_Komsomolsk', 'Benue_Umaisha',\n",
    "        'BolshayaKet_Rodyonovka', 'Brahmaputra_Pasighat', 'Chari_Bousso',\n",
    "        'Chari_Guelengdeng', 'Chari_Ndjamena', 'Chari_Sahr', 'Fraser_Hope',\n",
    "        'Gandak_Devghat', 'Helmand_Kajaki', 'Helmand_Malakhan', 'Indus_Attock',\n",
    "        'Irtysh_Bobrovsky', 'Irtysh_Hanti-Mansisk', 'Irtysh_Pavlodar',\n",
    "        'Irtysh_Semiyarskoje', 'Jutai_PortoSeguro', 'Kamchatka_Kozyrevsk',\n",
    "        'Kan_Kansk', 'MadreDeDios_CachuelaEsperanza', 'Magdalena_Calamar',\n",
    "        'Magdalena_PuertoBerrio', 'Manas_Mathanguri', 'Mbam_Goura',\n",
    "        'Mekong_Kratie', 'Niger_Tossaye', 'Ob_Barnaul', 'Ob_Kolpashevo',\n",
    "        'Ob_Mogochin', 'Ob_Phominskoje', 'Orinoco_CiudadBolivar',\n",
    "        'Orinoco_Musinacio', 'Paraguay_Asuncion', 'Paraguay_PortoMurtinho',\n",
    "        'Parana_Chapeton', 'Porcupine_NearFortYukon', 'Sangha_Ouesso',\n",
    "        'Solimoes_Itapeua', 'Solimoes_Manacapuru', 'SonghuaJiang_Haerbin',\n",
    "        'Vilyuy_KhatyrykKhoma', 'Yangtze_Datong', 'Yellowstone_NearSidney',\n",
    "        'Yukon_Eagle', 'Zambezi_LukuluMission', 'Zambezi_Matundo-Cais'\n",
    "    ]\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    folder_names = []\n",
    "    duplicates_removed = []\n",
    "    \n",
    "    for name in raw_folder_names:\n",
    "        if name not in seen:\n",
    "            seen.add(name)\n",
    "            folder_names.append(name)\n",
    "        else:\n",
    "            duplicates_removed.append(name)\n",
    "    \n",
    "    # Report on duplicates\n",
    "    if duplicates_removed:\n",
    "        from collections import Counter\n",
    "        duplicate_counts = Counter(raw_folder_names)\n",
    "        print(\"Duplicate rivers found and removed:\")\n",
    "        for name, count in duplicate_counts.items():\n",
    "            if count > 1:\n",
    "                print(f\"  - {name}: appeared {count} times, keeping 1\")\n",
    "        print()\n",
    "    \n",
    "    # Create the base directory if it doesn't exist\n",
    "    if not os.path.exists(base_directory):\n",
    "        os.makedirs(base_directory)\n",
    "        print(f\"Created base directory: {base_directory}\\n\")\n",
    "    \n",
    "    # Create each folder\n",
    "    created_count = 0\n",
    "    skipped_count = 0\n",
    "    subfolders_created = 0\n",
    "    subfolders_skipped = 0\n",
    "    \n",
    "    for folder_name in folder_names:\n",
    "        folder_path = os.path.join(base_directory, folder_name)\n",
    "        \n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "            created_count += 1\n",
    "            print(f\"Created: {folder_name}\")\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "            print(f\"Already exists: {folder_name}\")\n",
    "        \n",
    "        # Create subfolder if specified\n",
    "        if subfolder_name:\n",
    "            subfolder_path = os.path.join(folder_path, subfolder_name)\n",
    "            if not os.path.exists(subfolder_path):\n",
    "                os.makedirs(subfolder_path)\n",
    "                subfolders_created += 1\n",
    "            else:\n",
    "                subfolders_skipped += 1\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Original list: {len(raw_folder_names)} entries\")\n",
    "    print(f\"Duplicates removed: {len(duplicates_removed)}\")\n",
    "    print(f\"Unique folders: {len(folder_names)}\")\n",
    "    print(f\"✓ Folders created: {created_count}\")\n",
    "    print(f\"○ Already existed: {skipped_count}\")\n",
    "    \n",
    "    if subfolder_name:\n",
    "        print(f\"\\nSubfolder '{subfolder_name}':\")\n",
    "        print(f\"✓ Created: {subfolders_created}\")\n",
    "        print(f\"○ Already existed: {subfolders_skipped}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify your target directory here\n",
    "    target_directory = r\"E:\\Dissertation\\Data\\RiverMapping\\RiverMasks\"  # Change this to your desired path\n",
    "    \n",
    "    # Optional: specify a subfolder name to create inside each river folder\n",
    "    # Set to None if you don't want subfolders\n",
    "    subfolder = \"Cleaned\"  # Example: \"masks\" or \"data\" or None\n",
    "    \n",
    "    create_unique_river_folders(target_directory, subfolder_name=subfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baa5445",
   "metadata": {},
   "source": [
    "This moves the Greenberg et al 2024 masks from his folder structure to the new folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3ac0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def move_greenberg_masks():\n",
    "    \"\"\"\n",
    "    Move .tif mask files from Greenberg et al. directory structure to RiverMapping directory.\n",
    "    Searches through all planform types and river names, finds mask folders, and moves\n",
    "    all .tif files to the new directory structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define paths\n",
    "    source_base = r\"E:\\Dissertation\\Data\\Greenberg_etal_2024\\RiverData\"\n",
    "    target_base = r\"E:\\Dissertation\\Data\\RiverMapping\\RiverMasks\"\n",
    "    \n",
    "    # Define planform types\n",
    "    planform_types = [\"B\", \"HSW\", \"LSW\", \"Me\"]\n",
    "    \n",
    "    # Counters for summary\n",
    "    rivers_processed = 0\n",
    "    files_moved = 0\n",
    "    rivers_not_found = 0\n",
    "    rivers_with_multiple_folders = 0\n",
    "    errors = []\n",
    "    rivers_with_masks = []\n",
    "    rivers_without_masks = []\n",
    "    rivers_with_ambiguous_structure = []\n",
    "    \n",
    "    # Get list of all potential river names by scanning all planform directories\n",
    "    all_river_names = set()\n",
    "    for planform in planform_types:\n",
    "        planform_dir = os.path.join(source_base, planform)\n",
    "        if os.path.exists(planform_dir):\n",
    "            river_dirs = [d for d in os.listdir(planform_dir) \n",
    "                         if os.path.isdir(os.path.join(planform_dir, d))]\n",
    "            all_river_names.update(river_dirs)\n",
    "    \n",
    "    all_river_names = sorted(all_river_names)\n",
    "    print(f\"Found {len(all_river_names)} unique river names across all planform types\")\n",
    "    print(f\"Searching through {len(planform_types)} planform types\\n\")\n",
    "    \n",
    "    # Process each river\n",
    "    for river_name in all_river_names:\n",
    "        found_masks = False\n",
    "        \n",
    "        # Search through planform types for this river\n",
    "        for planform in planform_types:\n",
    "            # Construct source path to river directory\n",
    "            river_dir = os.path.join(source_base, planform, river_name)\n",
    "            \n",
    "            if not os.path.exists(river_dir):\n",
    "                continue\n",
    "            \n",
    "            # Check what's inside the river directory\n",
    "            items_in_river_dir = [d for d in os.listdir(river_dir) \n",
    "                                 if os.path.isdir(os.path.join(river_dir, d))]\n",
    "            \n",
    "            # Look for potential search locations\n",
    "            waterlevel_folders = [f for f in items_in_river_dir if f.startswith('WaterLevel')]\n",
    "            has_mask_direct = 'mask' in items_in_river_dir\n",
    "            has_river_subfolder = river_name in items_in_river_dir\n",
    "            \n",
    "            # Determine the correct path to mask folder\n",
    "            mask_dir = None\n",
    "            all_waterlevel_folders = []\n",
    "            \n",
    "            # Collect all WaterLevel folders from both levels\n",
    "            if waterlevel_folders:\n",
    "                all_waterlevel_folders.extend(waterlevel_folders)\n",
    "            \n",
    "            # Also check inside river_name subfolder if it exists\n",
    "            if has_river_subfolder:\n",
    "                river_subfolder = os.path.join(river_dir, river_name)\n",
    "                subfolders = [d for d in os.listdir(river_subfolder) \n",
    "                             if os.path.isdir(os.path.join(river_subfolder, d))]\n",
    "                subfolder_waterlevels = [f for f in subfolders if f.startswith('WaterLevel')]\n",
    "                all_waterlevel_folders.extend(subfolder_waterlevels)\n",
    "            \n",
    "            # Case 1: Multiple WaterLevel folders across both levels - ambiguous\n",
    "            if len(all_waterlevel_folders) > 1:\n",
    "                found_masks = True  # Mark as found to avoid \"not found\" message\n",
    "                rivers_with_ambiguous_structure.append(river_name)\n",
    "                rivers_with_multiple_folders += 1\n",
    "                print(f\"⚠ Warning: {river_name} ({planform}) has multiple WaterLevel folders: {all_waterlevel_folders}\")\n",
    "                print(f\"   Skipping due to ambiguous structure...\")\n",
    "                break\n",
    "            \n",
    "            # Case 2: WaterLevel folder directly in river_dir\n",
    "            elif len(waterlevel_folders) == 1:\n",
    "                potential_mask_dir = os.path.join(river_dir, waterlevel_folders[0], \"mask\")\n",
    "                if os.path.exists(potential_mask_dir) and os.path.isdir(potential_mask_dir):\n",
    "                    mask_dir = potential_mask_dir\n",
    "            \n",
    "            # Case 3: WaterLevel folder inside river_name subfolder\n",
    "            elif has_river_subfolder:\n",
    "                river_subfolder = os.path.join(river_dir, river_name)\n",
    "                subfolders = [d for d in os.listdir(river_subfolder) \n",
    "                             if os.path.isdir(os.path.join(river_subfolder, d))]\n",
    "                subfolder_waterlevels = [f for f in subfolders if f.startswith('WaterLevel')]\n",
    "                \n",
    "                if len(subfolder_waterlevels) == 1:\n",
    "                    potential_mask_dir = os.path.join(river_subfolder, subfolder_waterlevels[0], \"mask\")\n",
    "                    if os.path.exists(potential_mask_dir) and os.path.isdir(potential_mask_dir):\n",
    "                        mask_dir = potential_mask_dir\n",
    "                elif 'mask' in subfolders:\n",
    "                    # No WaterLevel, but mask exists directly in river_name subfolder\n",
    "                    mask_dir = os.path.join(river_subfolder, \"mask\")\n",
    "            \n",
    "            # Case 4: mask folder directly in river_dir (no river_name subfolder, no WaterLevel)\n",
    "            elif has_mask_direct:\n",
    "                mask_dir = os.path.join(river_dir, \"mask\")\n",
    "            \n",
    "            # If we found a mask directory, process it\n",
    "            if mask_dir:\n",
    "                # Get all .tif files in the mask directory\n",
    "                tif_files = [f for f in os.listdir(mask_dir) if f.lower().endswith('.tif')]\n",
    "                \n",
    "                if tif_files:\n",
    "                    found_masks = True\n",
    "                    \n",
    "                    try:\n",
    "                        # Create target directory\n",
    "                        target_dir = os.path.join(target_base, river_name, \"Cleaned\")\n",
    "                        os.makedirs(target_dir, exist_ok=True)\n",
    "                        \n",
    "                        # Move each .tif file\n",
    "                        files_moved_for_river = 0\n",
    "                        for tif_file in tif_files:\n",
    "                            source_file = os.path.join(mask_dir, tif_file)\n",
    "                            target_file = os.path.join(target_dir, tif_file)\n",
    "                            \n",
    "                            shutil.move(source_file, target_file)\n",
    "                            files_moved_for_river += 1\n",
    "                        \n",
    "                        files_moved += files_moved_for_river\n",
    "                        rivers_processed += 1\n",
    "                        rivers_with_masks.append(river_name)\n",
    "                        print(f\"✓ Moved {files_moved_for_river} file(s): {river_name} (from {planform})\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        error_msg = f\"{river_name} ({planform})\"\n",
    "                        errors.append(error_msg)\n",
    "                        print(f\"✗ Error: {river_name} ({planform}) - {str(e)}\")\n",
    "                        print(f\"   Skipping and continuing with remaining rivers...\")\n",
    "                    \n",
    "                    # Break after finding first matching planform with masks\n",
    "                    break\n",
    "        \n",
    "        if not found_masks:\n",
    "            rivers_without_masks.append(river_name)\n",
    "            rivers_not_found += 1\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"✓ Rivers with masks processed: {rivers_processed}\")\n",
    "    print(f\"✓ Total .tif files moved: {files_moved}\")\n",
    "    print(f\"○ Rivers without mask folders: {rivers_not_found}\")\n",
    "    print(f\"⚠ Rivers with ambiguous structure (skipped): {rivers_with_multiple_folders}\")\n",
    "    print(f\"✗ Errors encountered: {len(errors)}\")\n",
    "    print(f\"Total unique rivers scanned: {len(all_river_names)}\")\n",
    "    \n",
    "    if rivers_with_ambiguous_structure:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"RIVERS WITH AMBIGUOUS STRUCTURE (Multiple WaterLevel Folders)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for river in rivers_with_ambiguous_structure:\n",
    "            print(f\"  - {river}\")\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"FAILED RIVERS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for error in errors:\n",
    "            print(f\"  - {error}\")\n",
    "    \n",
    "    # Optional: Print rivers without masks (useful for debugging)\n",
    "    if rivers_without_masks and len(rivers_without_masks) <= 20:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"RIVERS WITHOUT MASK FOLDERS (showing up to 20)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for river in rivers_without_masks[:20]:\n",
    "            print(f\"  - {river}\")\n",
    "        if len(rivers_without_masks) > 20:\n",
    "            print(f\"  ... and {len(rivers_without_masks) - 20} more\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting mask file migration...\\n\")\n",
    "    move_greenberg_masks()\n",
    "    print(\"\\nMask migration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16394dc0",
   "metadata": {},
   "source": [
    "This moves the Zhao et al 2025 masks from his folder structure to the new folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47af1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_zhao_masks():\n",
    "    \"\"\"\n",
    "    Move .tif mask files from Zhao et al. directory structure to RiverMapping directory.\n",
    "    Searches through all planform types and river names, finds PreparedImagery_annual folders,\n",
    "    and moves all .tif files to the new directory structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define paths\n",
    "    source_base = r\"E:\\Dissertation\\Data\\Zhao_etal_2025\"\n",
    "    target_base = r\"E:\\Dissertation\\Data\\RiverMapping\\RiverMasks\"\n",
    "    \n",
    "    # Define planform types\n",
    "    planform_types = [\"B\", \"HSW\", \"LSW\", \"Me\"]\n",
    "    \n",
    "    # Counters for summary\n",
    "    rivers_processed = 0\n",
    "    files_moved = 0\n",
    "    rivers_not_found = 0\n",
    "    errors = []\n",
    "    rivers_with_masks = []\n",
    "    rivers_without_masks = []\n",
    "    \n",
    "    # Get list of all potential river names by scanning all planform directories\n",
    "    all_river_names = set()\n",
    "    for planform in planform_types:\n",
    "        planform_dir = os.path.join(source_base, planform)\n",
    "        if os.path.exists(planform_dir):\n",
    "            river_dirs = [d for d in os.listdir(planform_dir) \n",
    "                         if os.path.isdir(os.path.join(planform_dir, d))]\n",
    "            all_river_names.update(river_dirs)\n",
    "    \n",
    "    all_river_names = sorted(all_river_names)\n",
    "    print(f\"Found {len(all_river_names)} unique river names across all planform types\")\n",
    "    print(f\"Searching through {len(planform_types)} planform types\\n\")\n",
    "    \n",
    "    # Process each river\n",
    "    for river_name in all_river_names:\n",
    "        found_masks = False\n",
    "        \n",
    "        # Search through planform types for this river\n",
    "        for planform in planform_types:\n",
    "            # Construct source path: planform/river_name/PreparedImagery_annual/river_name\n",
    "            river_dir = os.path.join(source_base, planform, river_name)\n",
    "            imagery_dir = os.path.join(river_dir, \"PreparedImagery_annual\")\n",
    "            mask_dir = os.path.join(imagery_dir, river_name)\n",
    "            \n",
    "            # Check if mask directory exists\n",
    "            if os.path.exists(mask_dir) and os.path.isdir(mask_dir):\n",
    "                # Get all .tif files in the directory\n",
    "                tif_files = [f for f in os.listdir(mask_dir) if f.lower().endswith('.tif')]\n",
    "                \n",
    "                if tif_files:\n",
    "                    found_masks = True\n",
    "                    \n",
    "                    try:\n",
    "                        # Create target directory\n",
    "                        target_dir = os.path.join(target_base, river_name, \"Cleaned\")\n",
    "                        os.makedirs(target_dir, exist_ok=True)\n",
    "                        \n",
    "                        # Move each .tif file\n",
    "                        files_moved_for_river = 0\n",
    "                        for tif_file in tif_files:\n",
    "                            source_file = os.path.join(mask_dir, tif_file)\n",
    "                            target_file = os.path.join(target_dir, tif_file)\n",
    "                            \n",
    "                            shutil.move(source_file, target_file)\n",
    "                            files_moved_for_river += 1\n",
    "                        \n",
    "                        files_moved += files_moved_for_river\n",
    "                        rivers_processed += 1\n",
    "                        rivers_with_masks.append(river_name)\n",
    "                        print(f\"✓ Moved {files_moved_for_river} file(s): {river_name} (from {planform})\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        error_msg = f\"{river_name} ({planform})\"\n",
    "                        errors.append(error_msg)\n",
    "                        print(f\"✗ Error: {river_name} ({planform}) - {str(e)}\")\n",
    "                        print(f\"   Skipping and continuing with remaining rivers...\")\n",
    "                    \n",
    "                    # Break after finding first matching planform with masks\n",
    "                    break\n",
    "        \n",
    "        if not found_masks:\n",
    "            rivers_without_masks.append(river_name)\n",
    "            rivers_not_found += 1\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"✓ Rivers with masks processed: {rivers_processed}\")\n",
    "    print(f\"✓ Total .tif files moved: {files_moved}\")\n",
    "    print(f\"○ Rivers without mask folders: {rivers_not_found}\")\n",
    "    print(f\"✗ Errors encountered: {len(errors)}\")\n",
    "    print(f\"Total unique rivers scanned: {len(all_river_names)}\")\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"FAILED RIVERS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for error in errors:\n",
    "            print(f\"  - {error}\")\n",
    "    \n",
    "    # Optional: Print rivers without masks (useful for debugging)\n",
    "    if rivers_without_masks and len(rivers_without_masks) <= 20:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"RIVERS WITHOUT MASK FOLDERS (showing up to 20)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for river in rivers_without_masks[:20]:\n",
    "            print(f\"  - {river}\")\n",
    "        if len(rivers_without_masks) > 20:\n",
    "            print(f\"  ... and {len(rivers_without_masks) - 20} more\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Greenberg mask file migration...\\n\")\n",
    "    move_greenberg_masks()\n",
    "    print(\"\\nGreenberg mask migration complete!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\\nStarting Zhao mask file migration...\\n\")\n",
    "    move_zhao_masks()\n",
    "    print(\"\\nZhao mask migration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b732b31",
   "metadata": {},
   "source": [
    "Preliminary QC of river masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c61ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from datetime import datetime\n",
    "\n",
    "def analyze_mask_statistics(mask_dir):\n",
    "    \"\"\"\n",
    "    Analyze all .tif masks in a directory and compute statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mask_dir : str\n",
    "        Path to directory containing .tif mask files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with statistics for each mask\n",
    "    \"\"\"\n",
    "    tif_files = sorted([f for f in os.listdir(mask_dir) if f.lower().endswith('.tif')])\n",
    "    \n",
    "    if not tif_files:\n",
    "        return None\n",
    "    \n",
    "    stats_list = []\n",
    "    \n",
    "    for tif_file in tif_files:\n",
    "        file_path = os.path.join(mask_dir, tif_file)\n",
    "        \n",
    "        try:\n",
    "            # Suppress the NotGeoreferencedWarning\n",
    "            import warnings\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n",
    "                with rasterio.open(file_path) as src:\n",
    "                    data = src.read(1)  # Read first band\n",
    "            \n",
    "            # Get unique values to check what's actually in the data\n",
    "            unique_vals = np.unique(data)\n",
    "            \n",
    "            # Count pixels\n",
    "            total_pixels = data.size\n",
    "            wet_pixels = np.sum(data == 1)\n",
    "            dry_pixels = np.sum(data == 0)\n",
    "            \n",
    "            # Check for non-binary values (anything other than 0 or 1)\n",
    "            # Use a tolerance for floating point comparison\n",
    "            binary_mask = (np.abs(data - 0) < 0.01) | (np.abs(data - 1) < 0.01)\n",
    "            other_pixels = np.sum(~binary_mask)\n",
    "            \n",
    "            # Calculate percentages\n",
    "            pct_wet = (wet_pixels / total_pixels) * 100 if total_pixels > 0 else 0\n",
    "            pct_dry = (dry_pixels / total_pixels) * 100 if total_pixels > 0 else 0\n",
    "            \n",
    "            stats_list.append({\n",
    "                'filename': tif_file,\n",
    "                'total_pixels': total_pixels,\n",
    "                'wet_pixels': wet_pixels,\n",
    "                'dry_pixels': dry_pixels,\n",
    "                'other_pixels': other_pixels,\n",
    "                'pct_wet': pct_wet,\n",
    "                'pct_dry': pct_dry,\n",
    "                'unique_values': str(unique_vals.tolist())\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading {tif_file}: {e}\")\n",
    "            stats_list.append({\n",
    "                'filename': tif_file,\n",
    "                'total_pixels': 0,\n",
    "                'wet_pixels': 0,\n",
    "                'dry_pixels': 0,\n",
    "                'other_pixels': 0,\n",
    "                'pct_wet': 0,\n",
    "                'pct_dry': 0,\n",
    "                'unique_values': '',\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(stats_list)\n",
    "\n",
    "def flag_problematic_masks(df, z_threshold=3):\n",
    "    \"\"\"\n",
    "    Flag masks that deviate significantly from the mean or have other issues.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with mask statistics\n",
    "    z_threshold : float\n",
    "        Z-score threshold for flagging outliers\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with added flag columns\n",
    "    \"\"\"\n",
    "    # Calculate z-scores for wet pixels\n",
    "    mean_wet = df['wet_pixels'].mean()\n",
    "    std_wet = df['wet_pixels'].std()\n",
    "    \n",
    "    if std_wet > 0:\n",
    "        df['z_score'] = (df['wet_pixels'] - mean_wet) / std_wet\n",
    "    else:\n",
    "        df['z_score'] = 0\n",
    "    \n",
    "    # Flag different types of problems\n",
    "    df['flag_all_dry'] = df['wet_pixels'] == 0\n",
    "    df['flag_all_wet'] = df['pct_wet'] > 99.9\n",
    "    df['flag_extreme_outlier'] = np.abs(df['z_score']) > z_threshold\n",
    "    df['flag_non_binary'] = df['other_pixels'] > 0\n",
    "    df['flag_tiny_wet'] = (df['pct_wet'] < 0.1) & (df['pct_wet'] > 0)\n",
    "    \n",
    "    # Overall flag\n",
    "    df['flagged'] = (df['flag_all_dry'] | df['flag_all_wet'] | \n",
    "                     df['flag_extreme_outlier'] | df['flag_non_binary'] | \n",
    "                     df['flag_tiny_wet'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_contact_sheet(mask_dir, df, output_path, max_images=40):\n",
    "    \"\"\"\n",
    "    Create a contact sheet showing all masks with flagged ones highlighted.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mask_dir : str\n",
    "        Path to directory containing masks\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with mask statistics and flags\n",
    "    output_path : str\n",
    "        Path to save the contact sheet image\n",
    "    max_images : int\n",
    "        Maximum number of images to show\n",
    "    \"\"\"\n",
    "    tif_files = df['filename'].tolist()[:max_images]\n",
    "    n_images = len(tif_files)\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    n_cols = min(8, n_images)\n",
    "    n_rows = int(np.ceil(n_images / n_cols))\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, n_rows * 2.5))\n",
    "    \n",
    "    for idx, tif_file in enumerate(tif_files):\n",
    "        file_path = os.path.join(mask_dir, tif_file)\n",
    "        \n",
    "        try:\n",
    "            # Suppress the NotGeoreferencedWarning\n",
    "            import warnings\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n",
    "                with rasterio.open(file_path) as src:\n",
    "                    data = src.read(1)\n",
    "            \n",
    "            ax = plt.subplot(n_rows, n_cols, idx + 1)\n",
    "            ax.imshow(data, cmap='Blues', vmin=0, vmax=1)\n",
    "            \n",
    "            # Highlight flagged images with red border\n",
    "            row = df[df['filename'] == tif_file].iloc[0]\n",
    "            if row['flagged']:\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_edgecolor('red')\n",
    "                    spine.set_linewidth(3)\n",
    "            \n",
    "            # Add title with basic info\n",
    "            title = f\"{tif_file[:15]}...\\n\"\n",
    "            title += f\"Wet: {row['pct_wet']:.1f}%\"\n",
    "            if row['flagged']:\n",
    "                title += \"\\n⚠ FLAGGED\"\n",
    "            \n",
    "            ax.set_title(title, fontsize=8, color='red' if row['flagged'] else 'black')\n",
    "            ax.axis('off')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error visualizing {tif_file}: {e}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_time_series_plot(df, output_path, river_name):\n",
    "    \"\"\"\n",
    "    Create a time series plot of wet pixels over time.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with mask statistics\n",
    "    output_path : str\n",
    "        Path to save the plot\n",
    "    river_name : str\n",
    "        Name of the river for the title\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    x = np.arange(len(df))\n",
    "    \n",
    "    # Plot all points\n",
    "    ax.plot(x, df['wet_pixels'], 'o-', color='steelblue', markersize=4, label='Normal')\n",
    "    \n",
    "    # Highlight flagged points\n",
    "    flagged_df = df[df['flagged']]\n",
    "    if not flagged_df.empty:\n",
    "        flagged_indices = flagged_df.index\n",
    "        ax.plot(flagged_indices, flagged_df['wet_pixels'], 'ro', \n",
    "                markersize=8, label='Flagged', zorder=5)\n",
    "    \n",
    "    # Add mean line\n",
    "    mean_wet = df['wet_pixels'].mean()\n",
    "    ax.axhline(mean_wet, color='green', linestyle='--', alpha=0.7, label=f'Mean: {mean_wet:.0f}')\n",
    "    \n",
    "    # Add ±3 std lines\n",
    "    std_wet = df['wet_pixels'].std()\n",
    "    ax.axhline(mean_wet + 3*std_wet, color='red', linestyle=':', alpha=0.5, label='±3 σ')\n",
    "    ax.axhline(mean_wet - 3*std_wet, color='red', linestyle=':', alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Mask Index', fontsize=12)\n",
    "    ax.set_ylabel('Wet Pixels', fontsize=12)\n",
    "    ax.set_title(f'{river_name} - Wet Pixel Count Over Time', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def screen_all_rivers(base_dir, output_dir, z_threshold=3):\n",
    "    \"\"\"\n",
    "    Screen all river mask directories and generate reports.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_dir : str\n",
    "        Base directory containing river folders\n",
    "    output_dir : str\n",
    "        Directory to save QC reports\n",
    "    z_threshold : float\n",
    "        Z-score threshold for flagging outliers\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all river directories\n",
    "    river_dirs = [d for d in os.listdir(base_dir) \n",
    "                  if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    river_dirs = sorted(river_dirs)\n",
    "    \n",
    "    print(f\"Found {len(river_dirs)} river directories\\n\")\n",
    "    \n",
    "    # Track overall QC status\n",
    "    qc_summary = []\n",
    "    \n",
    "    for river_name in river_dirs:\n",
    "        print(f\"Processing {river_name}...\")\n",
    "        \n",
    "        # Path to masks\n",
    "        mask_dir = os.path.join(base_dir, river_name, \"Cleaned\")\n",
    "        \n",
    "        if not os.path.exists(mask_dir):\n",
    "            print(f\"  ⚠ No 'Cleaned' folder found, skipping...\")\n",
    "            qc_summary.append({\n",
    "                'river_name': river_name,\n",
    "                'status': 'No masks found',\n",
    "                'total_masks': 0,\n",
    "                'flagged_masks': 0,\n",
    "                'qc_complete': False\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Analyze masks\n",
    "        df = analyze_mask_statistics(mask_dir)\n",
    "        \n",
    "        if df is None or len(df) == 0:\n",
    "            print(f\"  ⚠ No .tif files found, skipping...\")\n",
    "            qc_summary.append({\n",
    "                'river_name': river_name,\n",
    "                'status': 'No .tif files',\n",
    "                'total_masks': 0,\n",
    "                'flagged_masks': 0,\n",
    "                'qc_complete': False\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Flag problematic masks\n",
    "        df = flag_problematic_masks(df, z_threshold)\n",
    "        \n",
    "        # Create river-specific output directory\n",
    "        river_output_dir = os.path.join(output_dir, river_name)\n",
    "        os.makedirs(river_output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save statistics CSV\n",
    "        csv_path = os.path.join(river_output_dir, f\"{river_name}_statistics.csv\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        # Create contact sheet\n",
    "        contact_sheet_path = os.path.join(river_output_dir, f\"{river_name}_contact_sheet.png\")\n",
    "        create_contact_sheet(mask_dir, df, contact_sheet_path)\n",
    "        \n",
    "        # Create time series plot\n",
    "        timeseries_path = os.path.join(river_output_dir, f\"{river_name}_timeseries.png\")\n",
    "        create_time_series_plot(df, timeseries_path, river_name)\n",
    "        \n",
    "        # Summary stats\n",
    "        total_masks = len(df)\n",
    "        flagged_count = df['flagged'].sum()\n",
    "        \n",
    "        print(f\"  ✓ Processed {total_masks} masks, {flagged_count} flagged\")\n",
    "        \n",
    "        qc_summary.append({\n",
    "            'river_name': river_name,\n",
    "            'status': 'Processed',\n",
    "            'total_masks': total_masks,\n",
    "            'flagged_masks': flagged_count,\n",
    "            'pct_flagged': (flagged_count / total_masks * 100) if total_masks > 0 else 0,\n",
    "            'qc_complete': False,\n",
    "            'date_screened': datetime.now().strftime('%Y-%m-%d'),\n",
    "            'notes': ''\n",
    "        })\n",
    "    \n",
    "    # Save overall QC tracking file\n",
    "    qc_df = pd.DataFrame(qc_summary)\n",
    "    qc_tracking_path = os.path.join(output_dir, 'QC_tracking.csv')\n",
    "    qc_df.to_csv(qc_tracking_path, index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SCREENING COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total rivers screened: {len(river_dirs)}\")\n",
    "    print(f\"QC tracking file saved to: {qc_tracking_path}\")\n",
    "    print(f\"Individual river reports saved to: {output_dir}\")\n",
    "    print(f\"\\nRivers with flagged masks:\")\n",
    "    for _, row in qc_df[qc_df['flagged_masks'] > 0].iterrows():\n",
    "        print(f\"  - {row['river_name']}: {row['flagged_masks']}/{row['total_masks']} masks flagged\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your paths here\n",
    "    base_directory = r\"E:\\Dissertation\\Data\\RiverMapping\\RiverMasks\"\n",
    "    output_directory = r\"E:\\Dissertation\\Data\\RiverMapping\\MaskQC\"\n",
    "    \n",
    "    # Run the screening\n",
    "    screen_all_rivers(base_directory, output_directory, z_threshold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6dc62d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
