{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a48ab391",
   "metadata": {},
   "source": [
    "# Calculate river planform (sinuosity, channel count index, channel form index)\n",
    "\n",
    "The following code is directed to a given local path containing 2-D water mask rasters. The code takes the water mask, and start by creating a \"skeleton\" of the mask. It then dilates the tips of the skeleton to improve connection of the channel network, reskeletonizes, and reduces the skeleton to only the identifiable river channels. From the final skeletion, channel links and nodes are created. The links are filtered according to criteria, and a shortest path line or \"main channel\" is extracted, along with a simplified main channel which acts as a valley center line, enabling sinuosity calculations. Finally, cross sections of the river are created and channel count index is calculated across the cross-sections. With sinuosity and channel-count index, the chanel form index can be calculated. These river metrics are provided and exported to a .csv. The processed skeleton, nodes, channel links, main channel, valley center-line, and channel-belt cross-sections are output as shapefiles, and the network for each reach/year is plotted and compiled in a PDF for the user' to QA/QC.\n",
    "\n",
    "Channel form index is calculated as outlined in:\n",
    "Galeazzi, C.P., Almeida, R.P., do Prado, A.H., 2021. Linking rivers to the rock record: Channel patterns and paleocurrent circular variance. Geology 49, 1402â€“1407. https://doi.org/10.1130/G49121.1\n",
    "\n",
    "Inspiration for river channel network analysis taken from rivgraph: https://github.com/VeinsOfTheEarth/RivGraph\n",
    "Schwenk, J., Hariharan, J., 2021. RivGraph: Automatic extraction and analysis of river and delta channel network topology. Journal of Open Source Software 6, 2952. https://doi.org/10.21105/joss.02952\n",
    "\n",
    "Author: James (Huck) Rees; PhD Student, UCSB Geography\n",
    "\n",
    "Date: January 14, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b19a039",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "facbeca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from glob import glob\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from fractions import Fraction\n",
    "\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from rasterio.transform import xy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from skimage.morphology import skeletonize, label\n",
    "from skimage.measure import regionprops\n",
    "from skimage import io, img_as_bool\n",
    "from skimage.feature import corner_harris, corner_peaks\n",
    "\n",
    "from shapefile import Reader, Writer\n",
    "from shapely.geometry import LineString, Point, MultiLineString, MultiPoint\n",
    "from shapely.ops import split, linemerge, snap, nearest_points, unary_union\n",
    "from shapely.strtree import STRtree\n",
    "\n",
    "from scipy.ndimage import label as scipy_label, find_objects\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "from rtree import index as rtree_index\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84ad93d",
   "metadata": {},
   "source": [
    "## Initialize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ef65ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load raster data\n",
    "def load_raster(file_path):\n",
    "    \"\"\"\n",
    "    Loads a raster file and returns the data of the first band along with its metadata.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the raster file.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - data (numpy.ndarray): The data of the first band of the raster.\n",
    "        - metadata (dict): The metadata of the raster file.\n",
    "    \"\"\"\n",
    "    with rasterio.open(file_path) as dataset:\n",
    "        data = dataset.read(1)  # Read the first band\n",
    "        metadata = dataset.meta\n",
    "    return data, metadata\n",
    "\n",
    "# Function to save a raster file\n",
    "def save_raster(output_path, data, metadata):\n",
    "    \"\"\"\n",
    "    Saves a raster file with the given data and metadata.\n",
    "\n",
    "    Parameters:\n",
    "    output_path (str): The path to save the output raster file.\n",
    "    data (numpy.ndarray): The data to be written to the raster file.\n",
    "    metadata (dict): The metadata of the raster file, including CRS and transform information.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    with rasterio.open(\n",
    "        output_path, \n",
    "        'w', \n",
    "        driver='GTiff', \n",
    "        height=data.shape[0], \n",
    "        width=data.shape[1], \n",
    "        count=1, \n",
    "        dtype='uint8', \n",
    "        crs=metadata['crs'], \n",
    "        transform=metadata['transform']\n",
    "    ) as dst:\n",
    "        dst.write(data.astype('uint8'), 1)\n",
    "\n",
    "def eliminate_small_islands(water_mask, min_size=10):\n",
    "    \"\"\"\n",
    "    Eliminate small \"islands\" of water and non-water regions in a binary water mask array \n",
    "    based on a minimum size threshold.\n",
    "\n",
    "    Parameters:\n",
    "    water_mask (numpy.ndarray): A 2D binary array where:\n",
    "                                - `1` represents water.\n",
    "                                - `0` represents no-water.\n",
    "    min_size (int, optional): The minimum number of pixels a region must have to be retained.\n",
    "                              Regions smaller than this size will be removed. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The cleaned water mask array with small islands of water and non-water removed.\n",
    "\n",
    "    Workflow:\n",
    "    1. Label connected components in the inverse of the water mask (non-water regions).\n",
    "    2. Identify and remove non-water regions smaller than the `min_size` threshold by \n",
    "       converting them to water (value `1`).\n",
    "    3. Label connected components in the original water mask (water regions).\n",
    "    4. Identify and remove water regions smaller than the `min_size` threshold by \n",
    "       converting them to no-water (value `0`).\n",
    "    \"\"\"\n",
    "    # Step 1: Label connected components in the inverse water mask (non-water regions)\n",
    "    labeled_array, num_features = scipy_label(1 - water_mask)\n",
    "    \n",
    "    # Step 2: Remove non-water regions smaller than `min_size`\n",
    "    for i in range(1, num_features + 1):\n",
    "        blob = labeled_array == i\n",
    "        if np.sum(blob) <= min_size:\n",
    "            water_mask[blob] = 1  # Convert small no-water regions to water (1)\n",
    "\n",
    "    # Step 3: Label connected components in the original water mask (water regions)\n",
    "    labeled_array, num_features = scipy_label(water_mask)\n",
    "    \n",
    "    # Step 4: Remove water regions smaller than `min_size`\n",
    "    for i in range(1, num_features + 1):\n",
    "        blob = labeled_array == i\n",
    "        if np.sum(blob) <= min_size:\n",
    "            water_mask[blob] = 0  # Convert small water regions to no-water (0)\n",
    "    \n",
    "    return water_mask\n",
    "\n",
    "# Function to perform conditional dilation\n",
    "def conditional_dilation(image, radius=5):\n",
    "    \"\"\"\n",
    "    Performs a conditional dilation on a binary image. Pixels with a value of 1 that have \n",
    "    two or fewer neighbors with the same value will cause a dilation within a given radius.\n",
    "\n",
    "    Parameters:\n",
    "    image (numpy.ndarray): The input binary image (2D array) to be processed.\n",
    "    radius (int, optional): The radius for the dilation operation. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The dilated image.\n",
    "    \"\"\"\n",
    "    dilated_image = np.copy(image)\n",
    "    for row in range(1, image.shape[0] - 1):\n",
    "        for col in range(1, image.shape[1] - 1):\n",
    "            if image[row, col] == 1:\n",
    "                neighbors = image[row-1:row+2, col-1:col+2]\n",
    "                if np.sum(neighbors) <= 2:  # Include the pixel itself in the count\n",
    "                    dilated_image[max(0, row-radius):min(row+radius+1, image.shape[0]), \n",
    "                                  max(0, col-radius):min(col+radius+1, image.shape[1])] = 1\n",
    "    return dilated_image\n",
    "\n",
    "# Function to keep only the largest connected component\n",
    "def keep_largest_component(image):\n",
    "    \"\"\"\n",
    "    Identifies and retains the largest connected component in a binary image. All other components are removed.\n",
    "\n",
    "    Parameters:\n",
    "    image (numpy.ndarray): The input binary image (2D array).\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A binary image containing only the largest connected component.\n",
    "    \"\"\"\n",
    "    labeled_image, num_features = label(image, connectivity=2, return_num=True)\n",
    "    if num_features == 0:\n",
    "        return image\n",
    "    regions = regionprops(labeled_image)\n",
    "    largest_region = max(regions, key=lambda r: r.area)\n",
    "    largest_component = (labeled_image == largest_region.label)\n",
    "    return largest_component\n",
    "\n",
    "# Function to create links shapefile\n",
    "def create_links(image, metadata):\n",
    "    \"\"\"\n",
    "    Identifies and creates links between adjacent pixels in a binary image. Links are represented as LineStrings.\n",
    "\n",
    "    Parameters:\n",
    "    image (numpy.ndarray): The input binary image (2D array) to be processed.\n",
    "    metadata (dict): The metadata of the raster file, including transform information.\n",
    "\n",
    "    Returns:\n",
    "    geopandas.GeoDataFrame: A GeoDataFrame containing the links as LineStrings.\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    transform = metadata['transform']\n",
    "    link_id = 1\n",
    "\n",
    "    # Iterate over each pixel in the image\n",
    "    for row in range(image.shape[0]):\n",
    "        for col in range(image.shape[1]):\n",
    "            if image[row, col] == 1:  # Check if the pixel is part of a segment\n",
    "                # Identify neighboring pixels that are also part of the segment\n",
    "                neighbors = [\n",
    "                    (row + dr, col + dc) \n",
    "                    for dr, dc in [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)] \n",
    "                    if 0 <= row + dr < image.shape[0] and 0 <= col + dc < image.shape[1] and image[row + dr, col + dc] == 1\n",
    "                ]\n",
    "                # Create LineString for each neighbor\n",
    "                for nr, nc in neighbors:\n",
    "                    x1, y1 = xy(transform, row, col)  # Convert pixel coordinates to spatial coordinates\n",
    "                    x2, y2 = xy(transform, nr, nc)\n",
    "                    line = LineString([(x1, y1), (x2, y2)])\n",
    "                    links.append((link_id, line))  # Append link to the list\n",
    "                    link_id += 1\n",
    "\n",
    "    # Remove duplicate links by sorting the coordinates of each LineString\n",
    "    unique_links = []\n",
    "    seen = set()\n",
    "    for link in links:\n",
    "        coords = tuple(sorted(link[1].coords))\n",
    "        if coords not in seen:\n",
    "            seen.add(coords)\n",
    "            unique_links.append(link)\n",
    "\n",
    "    # Create a GeoDataFrame from the unique links\n",
    "    gdf = gpd.GeoDataFrame(unique_links, columns=['id', 'geometry'])\n",
    "    \n",
    "    # Set the coordinate reference system (CRS)\n",
    "    gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "    return gdf\n",
    "\n",
    "# Function to filter links\n",
    "def filter_links(gdf):\n",
    "    \"\"\"\n",
    "    Filters out diagonal links from a GeoDataFrame of line segments, retaining only those\n",
    "    that are not part of an intersection with horizontal and vertical links.\n",
    "\n",
    "    Parameters:\n",
    "    gdf (geopandas.GeoDataFrame): The input GeoDataFrame containing line segments.\n",
    "\n",
    "    Returns:\n",
    "    geopandas.GeoDataFrame: A filtered GeoDataFrame with certain diagonal links removed.\n",
    "    \"\"\"\n",
    "    # Function to categorize the line segments\n",
    "    def categorize_line(row):\n",
    "        if row['start_point'][1] == row['end_point'][1]:\n",
    "            return 'horizontal'\n",
    "        elif row['start_point'][0] == row['end_point'][0]:\n",
    "            return 'vertical'\n",
    "        else:\n",
    "            return 'diagonal'\n",
    "    \n",
    "    # Function to extract start and end coordinates of each line segment\n",
    "    def get_coordinates(geometry):\n",
    "        start_point = geometry.coords[0]\n",
    "        end_point = geometry.coords[1]\n",
    "        return start_point, end_point\n",
    "    \n",
    "    # Apply the function to get coordinates and categorize each segment\n",
    "    gdf[['start_point', 'end_point']] = gdf.apply(lambda row: get_coordinates(row.geometry), axis=1, result_type='expand')\n",
    "    gdf['category'] = gdf.apply(categorize_line, axis=1)\n",
    "    \n",
    "    # Initialize spatial indexes for horizontal and vertical links\n",
    "    idx_horizontal = rtree_index.Index()\n",
    "    idx_vertical = rtree_index.Index()\n",
    "    \n",
    "    for idx, row in gdf.iterrows():\n",
    "        if row['category'] == 'horizontal':\n",
    "            idx_horizontal.insert(idx, row['geometry'].bounds)\n",
    "        elif row['category'] == 'vertical':\n",
    "            idx_vertical.insert(idx, row['geometry'].bounds)\n",
    "    \n",
    "    diagonals_to_remove = set()\n",
    "    \n",
    "    # Loop through each diagonal link\n",
    "    for index, diag_row in gdf[gdf['category'] == 'diagonal'].iterrows():\n",
    "        diag_start = diag_row['start_point']\n",
    "        diag_end = diag_row['end_point']\n",
    "        diag_bounds = diag_row['geometry'].bounds\n",
    "        x_coords = {diag_start[0], diag_end[0]}\n",
    "        y_coords = {diag_start[1], diag_end[1]}\n",
    "        hor = ver = False\n",
    "        \n",
    "        # Find horizontal links intersecting with the diagonal link using spatial index\n",
    "        for hor_idx in idx_horizontal.intersection(diag_bounds):\n",
    "            hor_row = gdf.loc[hor_idx]\n",
    "            hor_start = hor_row['start_point']\n",
    "            hor_end = hor_row['end_point']\n",
    "            if (hor_start[1] in y_coords or hor_end[1] in y_coords) and (hor_start[0] in x_coords and hor_end[0] in x_coords):\n",
    "                hor = True\n",
    "                break\n",
    "        \n",
    "        # Find vertical links intersecting with the diagonal link using spatial index\n",
    "        for ver_idx in idx_vertical.intersection(diag_bounds):\n",
    "            ver_row = gdf.loc[ver_idx]\n",
    "            ver_start = ver_row['start_point']\n",
    "            ver_end = ver_row['end_point']\n",
    "            if (ver_start[0] in x_coords or ver_end[0] in x_coords) and (ver_start[1] in y_coords and ver_end[1] in y_coords):\n",
    "                ver = True\n",
    "                break\n",
    "        \n",
    "        # Mark the diagonal for removal if it satisfies both conditions\n",
    "        if hor and ver:\n",
    "            diagonals_to_remove.add(index)\n",
    "    \n",
    "    # Drop the identified diagonal links\n",
    "    filtered_links = gdf.drop(index=diagonals_to_remove)\n",
    "    \n",
    "    # Drop the unnecessary columns before returning\n",
    "    filtered_links = filtered_links.drop(columns=['start_point', 'end_point', 'category'])\n",
    "    \n",
    "    return filtered_links\n",
    "\n",
    "def remove_degree_2_nodes(G):\n",
    "    \"\"\"\n",
    "    Remove degree-2 nodes from a graph and merge their adjacent edges.\n",
    "\n",
    "    This function simplifies a graph by removing nodes with exactly two neighbors (degree 2),\n",
    "    merging the two edges connected to the node into a single edge, and maintaining the \n",
    "    overall topology of the graph.\n",
    "\n",
    "    Parameters:\n",
    "    G (networkx.Graph or networkx.MultiGraph): The input graph. If it is not a MultiGraph, \n",
    "                                               it will be converted to a MultiGraph.\n",
    "\n",
    "    Returns:\n",
    "    networkx.MultiGraph: The simplified graph with degree-2 nodes removed and their edges merged.\n",
    "\n",
    "    Workflow:\n",
    "    1. Identify all nodes with a degree of 2.\n",
    "    2. For each degree-2 node:\n",
    "        - Retrieve its two neighbors.\n",
    "        - Merge the edges connecting the node to its neighbors into a single edge.\n",
    "        - Remove the degree-2 node from the graph.\n",
    "    3. Return the simplified graph.\n",
    "\n",
    "    Notes:\n",
    "    - Assumes that edges have a 'geometry' attribute containing their geometry (e.g., a LineString).\n",
    "    - Uses `linemerge` to combine the geometries of two edges into a single geometry.\n",
    "    \"\"\"\n",
    "    # Ensure the graph is a MultiGraph\n",
    "    if not isinstance(G, nx.MultiGraph):\n",
    "        G = nx.MultiGraph(G)\n",
    "\n",
    "    # Identify all nodes with a degree of 2\n",
    "    degree_2_nodes = [node for node, degree in dict(G.degree()).items() if degree == 2]\n",
    "\n",
    "    # Simplify the graph by merging edges of degree-2 nodes\n",
    "    for node in degree_2_nodes:\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        if len(neighbors) == 2:  # Ensure the node has exactly two neighbors\n",
    "            u, v = neighbors\n",
    "            \n",
    "            # Retrieve the keys for the edges connecting the node to its neighbors\n",
    "            key_uv = list(G[u][node])[0]\n",
    "            key_vu = list(G[v][node])[0]\n",
    "            \n",
    "            # Merge the geometries of the two edges\n",
    "            merged_line = linemerge([G.edges[node, u, key_uv]['geometry'], \n",
    "                                     G.edges[node, v, key_vu]['geometry']])\n",
    "            \n",
    "            # Add a new edge connecting the neighbors with the merged geometry\n",
    "            G.add_edge(u, v, geometry=merged_line)\n",
    "            \n",
    "            # Remove the degree-2 node from the graph\n",
    "            G.remove_node(node)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def geodataframe_to_graph(filtered_links):\n",
    "    \"\"\"\n",
    "    Convert a GeoDataFrame of line geometries into a MultiGraph representation.\n",
    "\n",
    "    Parameters:\n",
    "    filtered_links (gpd.GeoDataFrame): A GeoDataFrame containing line geometries.\n",
    "                                       Each row represents a link with a `geometry` column\n",
    "                                       containing `LineString` objects.\n",
    "\n",
    "    Returns:\n",
    "    networkx.MultiGraph: A MultiGraph where:\n",
    "                         - Nodes represent the start and end points of the lines.\n",
    "                         - Edges represent the line geometries with associated attributes:\n",
    "                           - `index`: The row index of the line in the GeoDataFrame.\n",
    "                           - `geometry`: The `LineString` geometry of the line.\n",
    "    \"\"\"\n",
    "    # Initialize an empty MultiGraph\n",
    "    G = nx.MultiGraph()\n",
    "    \n",
    "    # Iterate through each row in the GeoDataFrame\n",
    "    for idx, row in filtered_links.iterrows():\n",
    "        line = row.geometry  # Extract the LineString geometry\n",
    "        start, end = line.coords[0], line.coords[-1]  # Get the start and end points of the line\n",
    "        \n",
    "        # Add an edge to the graph with attributes\n",
    "        G.add_edge(start, end, index=idx, geometry=line)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def graph_to_merged_geodataframes(G):\n",
    "    \"\"\"\n",
    "    Convert a graph into two GeoDataFrames: one for nodes and one for merged edges.\n",
    "\n",
    "    This function processes a graph by extracting its nodes and merging connected edge geometries.\n",
    "    The resulting GeoDataFrames can be used for spatial analysis or visualization.\n",
    "\n",
    "    Parameters:\n",
    "    G (networkx.Graph or networkx.MultiGraph): A graph where:\n",
    "                                               - Nodes are represented as coordinate tuples (x, y).\n",
    "                                               - Edges have a `geometry` attribute representing\n",
    "                                                 their spatial extent (e.g., `LineString`).\n",
    "\n",
    "    Returns:\n",
    "    tuple:\n",
    "        - gpd.GeoDataFrame: A GeoDataFrame containing the graph nodes as `Point` geometries.\n",
    "        - gpd.GeoDataFrame: A GeoDataFrame containing the merged edge geometries as `LineString` or\n",
    "                            `MultiLineString` objects.\n",
    "\n",
    "    Workflow:\n",
    "    1. Convert graph nodes into `Point` geometries.\n",
    "    2. For each connected component of the graph:\n",
    "       - Extract edge geometries.\n",
    "       - Merge the geometries into a single `LineString` or `MultiLineString` using `unary_union`.\n",
    "       - Handle cases where the merged result is a `MultiLineString` by breaking it into individual lines.\n",
    "    3. Create GeoDataFrames for nodes and merged edges.\n",
    "\n",
    "    Notes:\n",
    "    - Assumes edge geometries are provided as `LineString` objects under the `geometry` attribute.\n",
    "    \"\"\"\n",
    "    # Step 1: Convert graph nodes into Point geometries\n",
    "    nodes = [Point(x, y) for x, y in G.nodes]\n",
    "    \n",
    "    # Step 2: Merge edge geometries for each connected component\n",
    "    merged_lines = []\n",
    "    for component in nx.connected_components(G):\n",
    "        subgraph = G.subgraph(component)  # Extract subgraph for the connected component\n",
    "        lines = [data['geometry'] for u, v, data in subgraph.edges(data=True)]  # Collect edge geometries\n",
    "        merged_line = unary_union(lines)  # Merge all geometries into one\n",
    "        \n",
    "        # Handle MultiLineString cases by separating into individual lines\n",
    "        if merged_line.geom_type == 'MultiLineString':\n",
    "            for line in merged_line.geoms:\n",
    "                merged_lines.append(line)\n",
    "        else:\n",
    "            merged_lines.append(merged_line)\n",
    "    \n",
    "    # Step 3: Create GeoDataFrames for nodes and edges\n",
    "    nodes_gdf = gpd.GeoDataFrame(geometry=nodes)\n",
    "    edges_gdf = gpd.GeoDataFrame(geometry=merged_lines)\n",
    "    \n",
    "    return nodes_gdf, edges_gdf\n",
    "\n",
    "# Function to find furthest endpoints\n",
    "def find_furthest_endpoints(gdf_points):\n",
    "    \"\"\"\n",
    "    Finds the two furthest nodes in the geodataframe, which may be of type 'endpoint' or 'junction'.\n",
    "\n",
    "    Parameters:\n",
    "    gdf_points (geopandas.GeoDataFrame): The geodataframe of points (nodes).\n",
    "\n",
    "    Returns:\n",
    "    geopandas.GeoDataFrame: A GeoDataFrame containing the two furthest points.\n",
    "    \"\"\"\n",
    "    if len(gdf_points) < 2:\n",
    "        raise ValueError(\"Not enough points to find the furthest pair.\")\n",
    "    \n",
    "    max_distance = 0\n",
    "    furthest_pair = None\n",
    "    for (idx1, point1), (idx2, point2) in combinations(gdf_points.iterrows(), 2):\n",
    "        distance = point1.geometry.distance(point2.geometry)\n",
    "        if distance > max_distance:\n",
    "            max_distance = distance\n",
    "            furthest_pair = (point1, point2)\n",
    "    \n",
    "    furthest_geometries = [furthest_pair[0].geometry, furthest_pair[1].geometry]\n",
    "    start_end_pts = gpd.GeoDataFrame(geometry=furthest_geometries, crs=gdf_points.crs)\n",
    "    return start_end_pts\n",
    "\n",
    "def remove_spurs(merged_gdf, start_end_pts):\n",
    "    start_point = start_end_pts.geometry.iloc[0]\n",
    "    end_point = start_end_pts.geometry.iloc[1]\n",
    "    \n",
    "    G = nx.MultiGraph()\n",
    "    \n",
    "    for idx, row in merged_gdf.iterrows():\n",
    "        line = row.geometry\n",
    "        start, end = line.coords[0], line.coords[-1]\n",
    "        G.add_edge(start, end, index=idx, geometry=line)\n",
    "    \n",
    "    dead_end_segments = []\n",
    "    for node in G.nodes:\n",
    "        if G.degree(node) == 1 and Point(node) not in [start_point, end_point]:\n",
    "            neighbors = list(G.neighbors(node))\n",
    "            if neighbors:\n",
    "                neighbor = neighbors[0]\n",
    "                edge_data = G.get_edge_data(node, neighbor)\n",
    "                for key, data in edge_data.items():\n",
    "                    dead_end_segments.append(data['index'])\n",
    "    \n",
    "    pruned_links = merged_gdf.drop(dead_end_segments)\n",
    "    \n",
    "    return pruned_links\n",
    "\n",
    "def prune_network(edges, start_end_pts):\n",
    "    \"\"\"\n",
    "    Prunes spurs from the network repeatedly until the number of edges remains constant.\n",
    "\n",
    "    Parameters:\n",
    "    edges (geopandas.GeoDataFrame): The GeoDataFrame of edges (river segments).\n",
    "    start_end_pts (geopandas.GeoDataFrame): The GeoDataFrame containing the two furthest points.\n",
    "\n",
    "    Returns:\n",
    "    geopandas.GeoDataFrame: A pruned GeoDataFrame with all spurs removed.\n",
    "    \"\"\"\n",
    "    previous_edge_count = -1  # Initialize with an impossible count\n",
    "    current_edge_count = len(edges)\n",
    "\n",
    "    while previous_edge_count != current_edge_count:\n",
    "        previous_edge_count = current_edge_count\n",
    "        \n",
    "        # Remove spurs\n",
    "        edges = remove_spurs(edges, start_end_pts)\n",
    "        \n",
    "        # Convert to graph\n",
    "        G = geodataframe_to_graph(edges)\n",
    "        \n",
    "        # Remove degree-2 nodes and merge edges\n",
    "        G = remove_degree_2_nodes(G)\n",
    "        \n",
    "        # Convert back to GeoDataFrame\n",
    "        _, edges = graph_to_merged_geodataframes(G)\n",
    "        \n",
    "        # Update edge count after merging\n",
    "        current_edge_count = len(edges)\n",
    "    \n",
    "    return edges\n",
    "\n",
    "# Function to find shortest path\n",
    "def find_shortest_path(start_end_pts, filtered_links):\n",
    "    \"\"\"\n",
    "    Finds the shortest path between two points in a network of filtered links.\n",
    "\n",
    "    Parameters:\n",
    "    start_end_pts (geopandas.GeoDataFrame): The GeoDataFrame containing the start and end points.\n",
    "    filtered_links (geopandas.GeoDataFrame): The GeoDataFrame containing the network of links (line segments).\n",
    "\n",
    "    Returns:\n",
    "    geopandas.GeoDataFrame: A GeoDataFrame containing the shortest path as a LineString.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    for idx, row in filtered_links.iterrows():\n",
    "        line = row.geometry\n",
    "        for i in range(len(line.coords) - 1):\n",
    "            start = Point(line.coords[i])\n",
    "            end = Point(line.coords[i + 1])\n",
    "            distance = start.distance(end)\n",
    "            G.add_edge(tuple(start.coords[0]), tuple(end.coords[0]), weight=distance)\n",
    "    \n",
    "    start_point = tuple(start_end_pts.geometry.iloc[0].coords[0])\n",
    "    end_point = tuple(start_end_pts.geometry.iloc[1].coords[0])\n",
    "    shortest_path = nx.shortest_path(G, source=start_point, target=end_point, weight='weight')\n",
    "    shortest_path_coords = [Point(coord) for coord in shortest_path]\n",
    "    shortest_path_line = LineString(shortest_path_coords)\n",
    "    shortest_path_length = shortest_path_line.length\n",
    "    shortest_path_gdf = gpd.GeoDataFrame({'geometry': [shortest_path_line]}, crs=filtered_links.crs)\n",
    "    return shortest_path_gdf\n",
    "\n",
    "# Function to classify channels\n",
    "def classify_channels(edges, shortest_path):\n",
    "    main_channel_line = shortest_path.geometry.iloc[0]\n",
    "\n",
    "    # Creating 'chnl_cat' column to classify channels\n",
    "    edges['chnl_cat'] = edges.apply(\n",
    "        lambda row: 'main_channel' if row.geometry.within(main_channel_line) else 'other', axis=1\n",
    "    )\n",
    "    \n",
    "    # Assigning unique 'chnl_id' to each segment\n",
    "    edges['chnl_id'] = None\n",
    "    edges.loc[edges['chnl_cat'] == 'main_channel', 'chnl_id'] = 1\n",
    "    \n",
    "    # Assign unique ids for 'other' channels\n",
    "    other_idx = edges[edges['chnl_cat'] == 'other'].index\n",
    "    edges.loc[other_idx, 'chnl_id'] = range(2, 2 + len(other_idx))\n",
    "    \n",
    "    return edges\n",
    "\n",
    "# Function to simplify shortest path\n",
    "def simplify_shortest_path(shortest_path, num_vertices=10):\n",
    "    \"\"\"\n",
    "    Simplifies the shortest path to a specified number of vertices.\n",
    "\n",
    "    Parameters:\n",
    "    shortest_path (geopandas.GeoDataFrame): The GeoDataFrame containing the shortest path as a LineString.\n",
    "    num_vertices (int, optional): The number of vertices for the simplified path. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    geopandas.GeoDataFrame: A GeoDataFrame containing the simplified shortest path as a LineString.\n",
    "    \"\"\"\n",
    "    original_line = shortest_path.geometry.iloc[0]\n",
    "    simplified_coords = [\n",
    "        original_line.interpolate(i / (num_vertices - 1), normalized=True).coords[0] \n",
    "        for i in range(num_vertices)\n",
    "    ]\n",
    "    simplified_line = LineString(simplified_coords)\n",
    "    simplified_path_gdf = gpd.GeoDataFrame({'geometry': [simplified_line]}, crs=shortest_path.crs)\n",
    "    return simplified_path_gdf\n",
    "\n",
    "# Function to create perpendicular lines\n",
    "def create_perpendicular_lines(simplified_path, num_lines=10, fraction_length=1/5):\n",
    "    \"\"\"\n",
    "    Creates perpendicular lines along the simplified path at equal intervals.\n",
    "\n",
    "    Parameters:\n",
    "    simplified_path (geopandas.GeoDataFrame): A GeoDataFrame containing the simplified path as a LineString.\n",
    "    num_lines (int): Number of perpendicular lines to create.\n",
    "    fraction_length (float): Fraction of the total path length for the length of each perpendicular line.\n",
    "\n",
    "    Returns:\n",
    "    geopandas.GeoDataFrame: A GeoDataFrame containing the perpendicular lines.\n",
    "    \"\"\"\n",
    "    # Extract the LineString from the GeoDataFrame\n",
    "    line = simplified_path.geometry.iloc[0]\n",
    "    line_length = line.length\n",
    "    \n",
    "    # Calculate spacing between perpendicular lines and half the length of each perpendicular line\n",
    "    spacing = line_length / num_lines\n",
    "    half_length = (line_length * fraction_length) / 2\n",
    "    \n",
    "    # Generate points at equal intervals along the line\n",
    "    points = [line.interpolate(i * spacing, normalized=False) for i in range(num_lines)]\n",
    "    \n",
    "    perpendicular_lines = []\n",
    "    \n",
    "    coords = list(line.coords)\n",
    "    \n",
    "    for idx, point in enumerate(points):\n",
    "        # Find the segment that the point falls on\n",
    "        segment = None\n",
    "        for i in range(len(coords) - 1):\n",
    "            segment_line = LineString([coords[i], coords[i+1]])\n",
    "            if segment_line.project(point) < segment_line.length:\n",
    "                segment = segment_line\n",
    "                break\n",
    "        \n",
    "        if segment is None:\n",
    "            print(f\"No segment found for point {idx}: {point}\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate the perpendicular direction to the segment\n",
    "        dx = segment.coords[1][0] - segment.coords[0][0]\n",
    "        dy = segment.coords[1][1] - segment.coords[0][1]\n",
    "        length = np.sqrt(dx**2 + dy**2)\n",
    "        perpendicular_direction = (-dy / length, dx / length)\n",
    "        \n",
    "        # Calculate the start and end points of the perpendicular line\n",
    "        start_point = Point(point.x + half_length * perpendicular_direction[0],\n",
    "                            point.y + half_length * perpendicular_direction[1])\n",
    "        end_point = Point(point.x - half_length * perpendicular_direction[0],\n",
    "                          point.y - half_length * perpendicular_direction[1])\n",
    "        \n",
    "        # Create the perpendicular line and add it to the list\n",
    "        perpendicular_line = LineString([start_point, end_point])\n",
    "        perpendicular_lines.append(perpendicular_line)\n",
    "    \n",
    "    # Create a GeoDataFrame from the perpendicular lines\n",
    "    channel_belt_cross_sections = gpd.GeoDataFrame({'geometry': perpendicular_lines}, crs=simplified_path.crs)\n",
    "    \n",
    "    return channel_belt_cross_sections\n",
    "\n",
    "# Function to calculate channel count index\n",
    "def calc_channel_count_index(filtered_links, cross_sections):\n",
    "    \"\"\"\n",
    "    Calculates the Channel Count Index (CCI) for a network of links intersecting with cross sections.\n",
    "\n",
    "    Parameters:\n",
    "    filtered_links (geopandas.GeoDataFrame): The GeoDataFrame containing the network of links (line segments) with a 'chnl_id' classification.\n",
    "    cross_sections (geopandas.GeoDataFrame): The GeoDataFrame containing the cross sections.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - cci (float): The Channel Count Index.\n",
    "        - cross_sections (geopandas.GeoDataFrame): The cross sections GeoDataFrame with an additional 'channel_count' column.\n",
    "    \"\"\"\n",
    "    channel_counts = []\n",
    "    \n",
    "    for idx, cross_section in cross_sections.iterrows():\n",
    "        cross_section_geom = cross_section.geometry\n",
    "        \n",
    "        # Find the chnl_ids of segments that intersect the cross section\n",
    "        intersecting_segments = filtered_links[filtered_links.intersects(cross_section_geom)]\n",
    "        unique_chnl_ids = intersecting_segments['chnl_id'].unique()\n",
    "        \n",
    "        # Count the number of unique chnl_ids intersected by this cross section\n",
    "        channel_count = len(unique_chnl_ids)\n",
    "        channel_counts.append(channel_count)\n",
    "    \n",
    "    cross_sections['channel_count'] = channel_counts\n",
    "    \n",
    "    # Calculate the Channel Count Index (CCI)\n",
    "    cci = sum(channel_counts) / len(channel_counts)\n",
    "    \n",
    "    print(f\"Channel Count Index (CCI): {cci}\")\n",
    "    \n",
    "    return cci, cross_sections\n",
    "\n",
    "# Function to calculate sinuosity\n",
    "def calc_sinuosity(shortest_path, simplified_path):\n",
    "    \"\"\"\n",
    "    Calculates the sinuosity of a path by comparing the lengths of the shortest path and the simplified path.\n",
    "\n",
    "    Parameters:\n",
    "    shortest_path (geopandas.GeoDataFrame): The GeoDataFrame containing the shortest path as a LineString.\n",
    "    simplified_path (geopandas.GeoDataFrame): The GeoDataFrame containing the simplified path as a LineString.\n",
    "\n",
    "    Returns:\n",
    "    float: The sinuosity value, which is the ratio of the shortest path length to the simplified path length.\n",
    "    \"\"\"\n",
    "    shortest_path_line = shortest_path.geometry.iloc[0]\n",
    "    simplified_path_line = simplified_path.geometry.iloc[0]\n",
    "    shortest_path_length = shortest_path_line.length\n",
    "    simplified_path_length = simplified_path_line.length\n",
    "    sinuosity = shortest_path_length / simplified_path_length\n",
    "    print(f\"Sinuosity: {sinuosity}\")\n",
    "    return sinuosity\n",
    "\n",
    "# Function to calculate channel form index\n",
    "def calculate_channel_form_index(sinuosity, cci):\n",
    "    \"\"\"\n",
    "    Calculates the Channel Form Index (CFI) based on sinuosity and Channel Count Index (CCI).\n",
    "\n",
    "    Parameters:\n",
    "    sinuosity (float): The sinuosity of the channel.\n",
    "    cci (float): The Channel Count Index.\n",
    "\n",
    "    Returns:\n",
    "    float: The Channel Form Index (CFI).\n",
    "    \"\"\"\n",
    "    cfi = sinuosity / cci\n",
    "    print(f\"Channel Form Index (CFI): {cfi}\")\n",
    "    return cfi\n",
    "\n",
    "# Main function to process network\n",
    "def process_network_folder(river, \n",
    "                           radius,\n",
    "                           min_size = 10,\n",
    "                           year_range=\"All\", \n",
    "                           reach_range=\"All\", \n",
    "                           num_lines=10, \n",
    "                           num_vertices=10, \n",
    "                           fraction_length=1/5, \n",
    "                           root_input=\"C:/Users/huckr/Desktop/UCSB/Dissertation/Data/RiverMapping/RiverMasks\", \n",
    "                           root_output=\"C:/Users/huckr/Desktop/UCSB/Dissertation/Data/RiverMapping/Channels\"):\n",
    "    \"\"\"\n",
    "    Processes a folder containing water mask rasters to extract river channel networks and calculate metrics.\n",
    "    Also generates a PDF with plots of classified channels, cross-sections, and other elements.\n",
    "\n",
    "    Parameters:\n",
    "    river (str): Name of the river.\n",
    "    radius (int): Radius for conditional dilation.\n",
    "    year_range (tuple or str): Year range for processing (default is \"All\").\n",
    "    reach_range (tuple or str): Reach range for processing (default is \"All\").\n",
    "    num_lines (int): Number of perpendicular lines (cross-sections) (default is 10).\n",
    "    num_vertices (int): Number of vertices for simplifying the shortest path (default is 10).\n",
    "    fraction_length (float): Fraction length for creating cross-sections (default is 1/5).\n",
    "    root_input (str): Root input directory (default is the specified path).\n",
    "    root_output (str): Root output directory (default is the specified path).\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    input_folder = os.path.join(root_input, river)\n",
    "    os.makedirs(input_folder, exist_ok=True)\n",
    "    output_folder_base = os.path.join(root_output, river)\n",
    "    os.makedirs(output_folder_base, exist_ok=True)\n",
    "    \n",
    "    def parse_range(input_range, default_start, default_end, range_name, pattern):\n",
    "        \"\"\"\n",
    "        Parses and validates a range input for years or reaches.\n",
    "\n",
    "        Parameters:\n",
    "        input_range (str, int, tuple, None): The range to parse.\n",
    "        default_start (int): Default start value if input_range is 'All' or None.\n",
    "        default_end (int): Default end value if input_range is 'All' or None.\n",
    "        range_name (str): Name of the range for error messages.\n",
    "        pattern (str): Regex pattern for validating string representations of ranges.\n",
    "\n",
    "        Returns:\n",
    "        tuple[int, int]: Parsed start and end of the range.\n",
    "        \"\"\"\n",
    "        if input_range in [\"All\", None]:\n",
    "            return default_start, default_end\n",
    "        elif isinstance(input_range, int):\n",
    "            return input_range, input_range\n",
    "        elif isinstance(input_range, str):\n",
    "            if re.match(pattern, input_range):  # Match the pattern\n",
    "                try:\n",
    "                    # Convert the string to a tuple of integers\n",
    "                    input_range = ast.literal_eval(input_range)\n",
    "                    if isinstance(input_range, tuple) and len(input_range) == 2 and all(isinstance(i, int) for i in input_range):\n",
    "                        return input_range\n",
    "                    else:\n",
    "                        raise ValueError(f\"{range_name} string must represent a tuple of two integers.\")\n",
    "                except (ValueError, SyntaxError):\n",
    "                    raise ValueError(f\"Invalid {range_name} format: {input_range}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid string format for {range_name}: {input_range}\")\n",
    "        elif isinstance(input_range, tuple) and len(input_range) == 2 and all(isinstance(i, int) for i in input_range):\n",
    "            return input_range\n",
    "        else:\n",
    "            raise ValueError(f\"{range_name} must be 'All', an int, or a tuple (start, end).\")\n",
    "\n",
    "    # Define patterns for validating string inputs\n",
    "    year_pattern = r'^\\(\\d{4}, \\d{4}\\)$'  # (YYYY, YYYY)\n",
    "    reach_pattern = r'^\\(\\d{1,4}, \\d{1,4}\\)$'  # (XX, YY) with 1 to 4 digits\n",
    "\n",
    "    # Parse year_range and reach_range using the refactored function\n",
    "    year_start, year_end = parse_range(year_range, 1984, 2025, \"year_range\", year_pattern)\n",
    "    reach_start, reach_end = parse_range(reach_range, 1, 9999, \"reach_range\", reach_pattern)\n",
    "    \n",
    "    # Initialize a dictionary to store metrics\n",
    "    metrics = {}\n",
    "\n",
    "    # Create a PDF file to store the plots\n",
    "    pdf_path = os.path.join(output_folder_base, f'{river}_report.pdf')\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        # Title page with summary information\n",
    "        fig, ax = plt.subplots(figsize=(8.5, 11))\n",
    "        ax.axis('off')\n",
    "        summary_text = (f\"River Name: {river}\\n\"\n",
    "                        f\"Year Range: {year_start} - {year_end}\\n\"\n",
    "                        f\"Reach Range: {reach_start} - {reach_end}\\n\"\n",
    "                        f\"Radius for Conditional Dilation: {radius}\\n\"\n",
    "                        f\"Minimum Size for Islands: {min_size}\\n\"\n",
    "                        f\"Number of Perpendicular Lines: {num_lines}\\n\"\n",
    "                        f\"Number of Vertices for Simplification: {num_vertices}\\n\"\n",
    "                        f\"Fraction Length for Cross-Sections: {fraction_length}\\n\")\n",
    "        ax.text(0.5, 0.5, summary_text, ha='center', va='center', fontsize=12)\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Process each reach folder\n",
    "        for reach_folder in glob(os.path.join(input_folder, 'reach_*')):\n",
    "            reach_folder_name = os.path.basename(reach_folder)\n",
    "            match_reach = re.match(r\"reach_(\\d+)\", reach_folder_name)\n",
    "            if match_reach:\n",
    "                reach = int(match_reach.group(1))\n",
    "                if reach_start <= reach <= reach_end:\n",
    "                    processed_folder = os.path.join(reach_folder, 'Processed')\n",
    "                    for file_path in glob(os.path.join(processed_folder, '*.tif')):\n",
    "                        file_name = os.path.basename(file_path)\n",
    "                        match_year = re.match(rf\"{river}_reach_{reach}_(\\d{{4}})_.*\\.tif\", file_name)\n",
    "                        if match_year:\n",
    "                            year = int(match_year.group(1))\n",
    "                            if year_start <= year <= year_end:\n",
    "                                output_folder = os.path.join(output_folder_base, f\"reach_{reach}\", str(year))\n",
    "                                os.makedirs(output_folder, exist_ok=True)\n",
    "                                \n",
    "                                try:\n",
    "                                    water_mask, metadata = load_raster(file_path)\n",
    "                                    cleaned_water_mask = eliminate_small_islands(water_mask, min_size=10)\n",
    "                                    skeleton = skeletonize(cleaned_water_mask > 0)\n",
    "                                    dilated_skeleton = conditional_dilation(skeleton, radius)\n",
    "                                    reskeletonized = skeletonize(dilated_skeleton > 0)\n",
    "                                    largest_component = keep_largest_component(reskeletonized)\n",
    "                                    \n",
    "                                    largest_component_output_path = os.path.join(output_folder, 'largest_component.tif')\n",
    "                                    save_raster(largest_component_output_path, largest_component, metadata)\n",
    "\n",
    "                                    initial_links = create_links(largest_component, metadata)\n",
    "                                    filtered_links = filter_links(initial_links)\n",
    "                                    \n",
    "                                    chan_graph1 = geodataframe_to_graph(filtered_links)\n",
    "\n",
    "                                    chan_graph2 = remove_degree_2_nodes(chan_graph1)\n",
    "\n",
    "                                    nodes, edges = graph_to_merged_geodataframes(chan_graph2)\n",
    "                                    start_end_pts = find_furthest_endpoints(nodes)\n",
    "                                    pruned_edges = prune_network(edges, start_end_pts)\n",
    "\n",
    "                                    shortest_path_gdf = find_shortest_path(start_end_pts, pruned_edges)\n",
    "                                    classified_links = classify_channels(pruned_edges, shortest_path_gdf)\n",
    "                                    valley_center_line = simplify_shortest_path(shortest_path_gdf, num_vertices)\n",
    "                                    channel_belt_cross_sections = create_perpendicular_lines(valley_center_line, num_lines, fraction_length)\n",
    "                                    \n",
    "                                    sinuosity_value = calc_sinuosity(shortest_path_gdf, valley_center_line)\n",
    "                                    cci, updated_cross_sections = calc_channel_count_index(classified_links, channel_belt_cross_sections)\n",
    "                                    cfi_value = calculate_channel_form_index(sinuosity_value, cci)\n",
    "                                    \n",
    "                                    classified_links.to_file(os.path.join(output_folder, 'channel_links.shp'))\n",
    "                                    channel_belt_cross_sections.to_file(os.path.join(output_folder, 'channel_belt_cross_sections.shp'))\n",
    "                                    nodes.to_file(os.path.join(output_folder, 'nodes.shp'))\n",
    "                                    shortest_path_gdf.to_file(os.path.join(output_folder, 'main_channel.shp'))\n",
    "                                    valley_center_line.to_file(os.path.join(output_folder, 'valley_center_line.shp'))\n",
    "                                    \n",
    "                                    # Store metrics\n",
    "                                    reach_key = f\"reach_{reach}\"\n",
    "                                    if reach_key not in metrics:\n",
    "                                        metrics[reach_key] = {}\n",
    "                                    metrics[reach_key][year] = {\n",
    "                                        'Sinuosity': sinuosity_value,\n",
    "                                        'CCI': cci,\n",
    "                                        'CFI': cfi_value\n",
    "                                    }\n",
    "\n",
    "                                    # Generate a plot for the PDF\n",
    "                                    fig, ax = plt.subplots(figsize=(8.5, 11))\n",
    "                                    ax.set_title(f\"Reach {reach}, Year {year}\")\n",
    "                                    \n",
    "                                    # Plot the cleaned water mask at the bottom\n",
    "                                    show(cleaned_water_mask, transform=metadata['transform'], ax=ax, cmap='gray')\n",
    "                                    \n",
    "                                    # Plot classified channels and cross-sections\n",
    "                                    classified_links.plot(ax=ax, color='#39FF14', linewidth=1)\n",
    "                                    channel_belt_cross_sections.plot(ax=ax, color='orange', linewidth=1)\n",
    "                                    \n",
    "                                    # Plot the main channel on top\n",
    "                                    shortest_path_gdf.plot(ax=ax, color='red', linewidth=2)\n",
    "                                    \n",
    "                                    # Add channel counts at the end of each cross-section\n",
    "                                    for idx, row in updated_cross_sections.iterrows():\n",
    "                                        x, y = row.geometry.centroid.x, row.geometry.centroid.y\n",
    "                                        ax.text(x, y, str(row['channel_count']), fontsize=8, ha='center', va='center', color='black', bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "                                    # Display metrics in the bottom right corner\n",
    "                                    ax.text(0.95, 0.05, f\"Sinuosity: {sinuosity_value:.2f}\\nCCI: {cci:.2f}\\nCFI: {cfi_value:.2f}\",\n",
    "                                            ha='right', va='bottom', transform=ax.transAxes, fontsize=10,\n",
    "                                            bbox=dict(facecolor='white', alpha=0.5))\n",
    "                                    \n",
    "                                    # Save the plot to the PDF\n",
    "                                    pdf.savefig(fig)\n",
    "                                    plt.close(fig)\n",
    "\n",
    "                                except Exception as e:\n",
    "                                    logging.error(f\"Error processing file {file_path}: {e}\")\n",
    "                                    continue\n",
    "\n",
    "        # Save metrics to an Excel workbook\n",
    "        metrics_output_path = os.path.join(output_folder_base, f'{river}_metrics.xlsx')\n",
    "        with pd.ExcelWriter(metrics_output_path) as writer:\n",
    "            for reach, reach_metrics in metrics.items():\n",
    "                df = pd.DataFrame.from_dict(reach_metrics, orient='index')\n",
    "                df.to_excel(writer, sheet_name=reach)\n",
    "                \n",
    "def main(input_directory):\n",
    "    \"\"\"\n",
    "    Main function to process rivers based on a CSV file of input variables.\n",
    "    \n",
    "    Args:\n",
    "        input_directory (str): The directory where the input .csv file resides.\n",
    "    \n",
    "    The .csv file should contain the following columns:\n",
    "        - river_name\n",
    "        - radius\n",
    "        - min_blob_size\n",
    "        - year_range\n",
    "        - reach_range\n",
    "        - num_xcs (num_lines)\n",
    "        - num_vertices\n",
    "        - fraction_length\n",
    "        - root_input\n",
    "        - root_output\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the CSV into a pandas DataFrame\n",
    "    river_data = pd.read_csv(input_directory)\n",
    "    \n",
    "    # Iterate through each row (each river) and run the process_network_folder() function\n",
    "    for index, row in river_data.iterrows():\n",
    "        river_name = row['river_name']\n",
    "        working_directory = row['working_directory']\n",
    "        radius = row['dilation_radius']\n",
    "        min_blob_size = row['min_blob_size']\n",
    "        year_range = row['year_range'] \n",
    "        reach_range = row['reach_range'] \n",
    "        num_lines = row['num_xcs']\n",
    "        num_vertices = row['num_vertices']\n",
    "        fraction_length = float(Fraction(row['fraction_length']))\n",
    "        root_input = os.path.join(working_directory, \"RiverMapping\", \"RiverMasks\")\n",
    "        os.makedirs(root_input, exist_ok=True)\n",
    "        root_output = os.path.join(working_directory, \"RiverMapping\", \"Channels\")\n",
    "        os.makedirs(root_output, exist_ok=True)\n",
    "        print(f\"Processing river: {river_name}\")\n",
    "        \n",
    "        # Call the existing function with inputs from the current row\n",
    "        process_network_folder(\n",
    "            river=river_name,\n",
    "            radius=radius,\n",
    "            min_size=min_blob_size,\n",
    "            year_range=year_range,\n",
    "            reach_range=reach_range,\n",
    "            num_lines=num_lines,\n",
    "            num_vertices=num_vertices,\n",
    "            fraction_length=fraction_length,\n",
    "            root_input=root_input,\n",
    "            root_output=root_output\n",
    "        )\n",
    "        \n",
    "    print(\"All rivers processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe48584",
   "metadata": {},
   "source": [
    "## Execute code for a river, a reach, or specific years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df9d3ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing river: Yukon_Beaver\n",
      "Sinuosity: 1.3020967249979178\n",
      "Channel Count Index (CCI): 4.46\n",
      "Channel Form Index (CFI): 0.29194993834034033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huckr\\AppData\\Local\\Temp\\ipykernel_14344\\578621774.py:838: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  channel_belt_cross_sections.to_file(os.path.join(output_folder, 'channel_belt_cross_sections.shp'))\n",
      "WARNING:fiona._env:Normalized/laundered field name: 'channel_count' to 'channel_co'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDissertation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGeyman_river_datasheet.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m main(csv_path)\n",
      "Cell \u001b[1;32mIn[2], line 933\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(input_directory)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing river: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mriver_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;66;03m# Call the existing function with inputs from the current row\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m     process_network_folder(\n\u001b[0;32m    934\u001b[0m         river\u001b[38;5;241m=\u001b[39mriver_name,\n\u001b[0;32m    935\u001b[0m         radius\u001b[38;5;241m=\u001b[39mradius,\n\u001b[0;32m    936\u001b[0m         min_size\u001b[38;5;241m=\u001b[39mmin_blob_size,\n\u001b[0;32m    937\u001b[0m         year_range\u001b[38;5;241m=\u001b[39myear_range,\n\u001b[0;32m    938\u001b[0m         reach_range\u001b[38;5;241m=\u001b[39mreach_range,\n\u001b[0;32m    939\u001b[0m         num_lines\u001b[38;5;241m=\u001b[39mnum_lines,\n\u001b[0;32m    940\u001b[0m         num_vertices\u001b[38;5;241m=\u001b[39mnum_vertices,\n\u001b[0;32m    941\u001b[0m         fraction_length\u001b[38;5;241m=\u001b[39mfraction_length,\n\u001b[0;32m    942\u001b[0m         root_input\u001b[38;5;241m=\u001b[39mroot_input,\n\u001b[0;32m    943\u001b[0m         root_output\u001b[38;5;241m=\u001b[39mroot_output\n\u001b[0;32m    944\u001b[0m     )\n\u001b[0;32m    946\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll rivers processed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 808\u001b[0m, in \u001b[0;36mprocess_network_folder\u001b[1;34m(river, radius, min_size, year_range, reach_range, num_lines, num_vertices, fraction_length, root_input, root_output)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    807\u001b[0m     water_mask, metadata \u001b[38;5;241m=\u001b[39m load_raster(file_path)\n\u001b[1;32m--> 808\u001b[0m     cleaned_water_mask \u001b[38;5;241m=\u001b[39m eliminate_small_islands(water_mask, min_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m    809\u001b[0m     skeleton \u001b[38;5;241m=\u001b[39m skeletonize(cleaned_water_mask \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    810\u001b[0m     dilated_skeleton \u001b[38;5;241m=\u001b[39m conditional_dilation(skeleton, radius)\n",
      "Cell \u001b[1;32mIn[2], line -1\u001b[0m, in \u001b[0;36meliminate_small_islands\u001b[1;34m(water_mask, min_size)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "csv_path = r\"D:\\Dissertation\\Data\\Geyman_river_datasheet.csv\"\n",
    "main(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233aa871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
